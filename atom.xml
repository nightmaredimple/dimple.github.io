<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>见渊の博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://huangpiao.tech/"/>
  <updated>2020-04-08T07:00:00.000Z</updated>
  <id>https://huangpiao.tech/</id>
  
  <author>
    <name>黄飘</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CVPR2020 | 多目标跟踪（车辆）与检测框架 RetinaTrack</title>
    <link href="https://huangpiao.tech/2020/04/08/CVPR2020%20%20%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%EF%BC%88%E8%BD%A6%E8%BE%86%EF%BC%89%E4%B8%8E%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%20RetinaTrack/"/>
    <id>https://huangpiao.tech/2020/04/08/CVPR2020  多目标跟踪（车辆）与检测框架 RetinaTrack/</id>
    <published>2020-04-08T07:00:00.000Z</published>
    <updated>2020-04-08T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>今天经群友提醒，发现漏掉了一篇CVPR2020的MOT论文，同样是基于检测和跟踪一体的框架，只不过它是以车辆跟踪为背景而写的。这里我们也凑个整，Tracktor++（就叫它FrcnnTrack吧，哈哈）、CenterTrack、FairMOT、JDE(YoloTrack。。。) ，以及这次的RetinaTrack开始神仙打架。不过也为MOT领域担忧，在public赛道上基于检测的跟踪框架把baseline刷得太高了，有点不利于后续发展呀。</p></blockquote><a id="more"></a><h2 id="RetinaTrack"><a href="#RetinaTrack" class="headerlink" title="RetinaTrack"></a>RetinaTrack</h2><blockquote><p>论文题目：RetinaTrack: Online Single Stage Joint Detection and Tracking</p><p>作者团队：谷歌</p><p>备注：Waymo 39.12MOTA,14FPS</p></blockquote><p>虽然RetinaTrack也是同之前的联合检测和跟踪的算法一样的框架（感兴趣的可以在我放在参考文献中的链接里面去看看），从名字也知道是基于RetinaNet的，但是论文中是以<strong>自动驾驶</strong>为背景进行介绍的，没有在MOT Challenge赛道比拼，倒是跟Tracktor++进行了比较。</p><p>首先我们回顾一下RetinaNet的结构：</p><p><img src="https://pic1.zhimg.com/v2-f3ef4c6e92c2f6d9e2782e7e32361042_1200x500.jpg" alt="RetinaNet(Focal Loss)"></p><p>整体来看，我们可以讲其归纳为三个特点：<strong>FPN、focal loss、回归和分类的两个分支</strong>（在我之前介绍目标检测中的特征冲突中提到了）。然后我们看看RetinaTrack的架构：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408131611030.png" alt="image-20200408131611030"></p><p>直接从图上看的话我们可以得到的信息是，RetinaNet在分类和回归的分支上分别预测了k个anchor下的分类和回归信息。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408132015081.png" alt="image-20200408132015081"></p><p>而RetinaTrack与JDE和FairMOT一样，都增加了一个256维的特征信息embeddings分支：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408132429065.png" alt="image-20200408132429065"></p><p>我们都知道，在MOT场景中需要解决严重遮挡问题，这个问题对于检测的影响也很大，比如：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408133204547.png" alt="image-20200408133204547"></p><p>上图中两辆车的中心重合，二者的检测框如果都是基于同一个anchor点进行预测的，则很难得到具有分辨力的embeddings。另外，我们之前的博客讨论过，reid和目标检测在特征方面的需求不同，以行人检索为例，目标检测中分类要求同类目标特征一致，而ReID则是要求在保证类内距离尽可能小的同时，确保类间距离大，但是这里的类间指的是不同身份的人，但是对于目标检测而言都是人。所以这里将ReID和分类的共享特征减少是最好的选择，作者这里实际上隐含着用了三种方式改进这一点：</p><ul><li><strong>通过将分类、回归和特征提取设为三个分支任务，除了FPN之前的部分，三者的特征共享部分含有m1个3x3卷积；</strong></li><li><strong>对于每层特征图上每个特征点的k个anchor，全部预测分类、回归和特征，增加区分度。；</strong></li><li><strong>对于检测任务，分类和回归分支都包含m2个3x3卷积，而embedding分支则为m3个1x1卷积。</strong></li></ul><p>对于训练部分，不同于JDE和FairMOT采用的identification模式，RetinaTrack采用的是verification模式，采用基于batch-hard的triplet loss进行训练，其中margin为0.1。</p><p>以上任务是在一堆TPU上训练的，基于Momentum SGD算法，每个batch还有128个clips，每个clip含两个相隔8帧的样本（对于10Hz的Waymo数据集而言就是相隔0.8s），图像输入是1024x1024，并采用bfloat16式的混合精度训练模式。其中去除embeddings分支的部分是在COCO数据集上预训练的，然后采用warmup和余弦退火学习策略训练。</p><p>实验效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408140308308.png" alt="image-20200408140308308"></p><p>由消融实验可知，anchor类型数量越多效果越好，其中RetinaNet部分是直接通过IOU进行数据关联的。紧接着作者又做了几组对比实验：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200408141113151.png" alt="image-20200408141113151"></p><ul><li>对比MOT Challenge中表现良好的Tracktor++算法，RetinaTrack效果更好；</li><li>基于IOU，不采用triplet loss（这是直接做成identification了？），或者将特征分支单独利用resnet50训练这两种方法都不如RetinaTrack。</li><li>在Waymo v1.1数据集上MOTA可达44.92，mAP可达45.70，推理速度为70ms</li></ul><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1] RetinaTrack: Online Single Stage Joint Detection and Tracking</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/125395219">https://zhuanlan.zhihu.com/p/125395219</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/126558285">https://zhuanlan.zhihu.com/p/126558285</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/126359766">https://zhuanlan.zhihu.com/p/126359766</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;今天经群友提醒，发现漏掉了一篇CVPR2020的MOT论文，同样是基于检测和跟踪一体的框架，只不过它是以车辆跟踪为背景而写的。这里我们也凑个整，Tracktor++（就叫它FrcnnTrack吧，哈哈）、CenterTrack、FairMOT、JDE(YoloTrack。。。) ，以及这次的RetinaTrack开始神仙打架。不过也为MOT领域担忧，在public赛道上基于检测的跟踪框架把baseline刷得太高了，有点不利于后续发展呀。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="MOT" scheme="https://huangpiao.tech/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>MOT开源实时新SOTA |A Simple Baseline for Multi-Object Tracking</title>
    <link href="https://huangpiao.tech/2020/04/07/MOT%E5%BC%80%E6%BA%90%E5%AE%9E%E6%97%B6%E6%96%B0SOTA%20A%20Simple%20Baseline%20for%20Multi-Object%20Tracking/"/>
    <id>https://huangpiao.tech/2020/04/07/MOT开源实时新SOTA A Simple Baseline for Multi-Object Tracking/</id>
    <published>2020-04-07T05:00:00.000Z</published>
    <updated>2020-04-07T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>今天又开源了一篇MOT的新SOTA，也是实时的，也是CenterNet为底层的，估计是看到CenterTrack开源了。emmm….看来我近期看的几篇都在今年某顶会扎堆了，噗。这里我还是把这篇文章给介绍一下吧，有意思的是其中的大部分论点我都在之前的博客([2]、[3])说过了。</p></blockquote><a id="more"></a><h2 id="FairMOT"><a href="#FairMOT" class="headerlink" title="FairMOT"></a>FairMOT</h2><blockquote><p>论文题目：A Simple Baseline for Multi-Object Tracking</p><p>作者团队：华科&amp;微软亚研院</p><p>备注：MOT15~20(private)：59.0、68.7、67.5、58.7 MOTA</p><p>代码链接：<a href="https://github.com/ifzhang/FairMOT">https://github.com/ifzhang/FairMOT</a></p></blockquote><p>这篇论文的立意是两部分，<strong>一个是类似于CenterTrack的基于CenterNet的联合检测和跟踪的框架，一个是类似于JDE，但是却又不同的，探讨了检测框架与ReID特征任务的集成问题。</strong></p><p>作者称这类框架为one-shot MOT框架，论文一开始作者讨论了检测框架和ReID任务的关系：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407123132578.png" alt="image-20200407123132578"></p><p>作者的意思是anchor-based的检测框架中存在anchor和特征的不对齐问题，所以这方面不如anchor-free框架，emmm…指出的问题的确是对的，不过详细的讨论建议各位看看我之前对这个问题的详细讨论[3] [4]。作者因为这个问题而选择了anchor-free算法——CenterNet，不过其用法并不是类似于CenterTrack[2]中采取的类似于D&amp;T的孪生联合方式，而是采用的Tracktor++的方式。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407123702842.png" alt="image-20200407123702842"></p><p>我们知道原始的anchor-free框架的大多数backbone都是采用了骨骼关键点中的hourglass结构：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407123941552.png" alt="image-20200407123941552"></p><p>后面我会单独开一个Re-ID和MOT的专题，这里呢作者就谈到了Re-ID网络中典型的多尺度问题，所以就提出要将hourglass结构改成上图中的多尺度融合的形式。最后通过两个分支完成了检测和Re-ID任务的集成，那么接下来的部分就是如何训练。</p><p>在训练部分呢，同样地，考虑到正负样本不均衡问题，作者采用了focal loss的形式：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407124542103.png" alt="image-20200407124542103"></p><p>其中M(x,y)表示的是heatmap在(x,y)处存在目标的概率，而对于box size和offset则采用L1 loss：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407124835568.png" alt="image-20200407124835568"></p><p>最后对于Re-ID分支而言，作者采用了identification式的分类框架，这里面的L就是不同的ID的one-hot表示，p就是网络预测的分类置信度。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407124853141.png" alt="image-20200407124853141"></p><p>在实验部分，作者先是通过实验证明anchor-free的框架比anchor-based框架更适合reid：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407125133467.png" alt="image-20200407125133467"></p><p>紧接着论证了多尺度融合框架对于Re-ID的影响：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407125254015.png" alt="image-20200407125254015"></p><p>的确，从特征空间来讲，各ID的特征距离更大了。而对于Re-ID的特征维度，作者通过实验表明128维即可，这里我就不细说了。最后放一下结果，下面都是private赛道的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200407125523425.png" alt="image-20200407125523425"></p><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1] A Simple Baseline for Multi-Object Tracking</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/125395219">https://zhuanlan.zhihu.com/p/125395219</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/114700229">https://zhuanlan.zhihu.com/p/114700229</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/126359766">https://zhuanlan.zhihu.com/p/126359766</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;今天又开源了一篇MOT的新SOTA，也是实时的，也是CenterNet为底层的，估计是看到CenterTrack开源了。emmm….看来我近期看的几篇都在今年某顶会扎堆了，噗。这里我还是把这篇文章给介绍一下吧，有意思的是其中的大部分论点我都在之前的博客([2]、[3])说过了。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="MOT" scheme="https://huangpiao.tech/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2020 商汤|再谈目标检测中的分类和定位冲突问题</title>
    <link href="https://huangpiao.tech/2020/04/06/CVPR2020%20%E5%95%86%E6%B1%A4%E5%86%8D%E8%B0%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E5%92%8C%E5%AE%9A%E4%BD%8D%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"/>
    <id>https://huangpiao.tech/2020/04/06/CVPR2020 商汤再谈目标检测中的分类和定位冲突问题/</id>
    <published>2020-04-06T15:00:00.000Z</published>
    <updated>2020-04-06T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>前段时间我在专栏里详细分析了目标检测中的特征冲突与不对齐问题，今天无意间又看到了商汤在CVPR2020的一篇相关论文，分析角度跟我之前所说的类似，但是解决方案增加了一些技巧，论文中提到其对于各类backbone都有~3%mAP的提升，该算法也用到了OpenImage比赛中，是对Decoupling Head框架的详细分析。这里我们一起来分析下论文内容。</p></blockquote><a id="more"></a><h2 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h2><p>前段时间我写了两篇相关的博客《谈谈CNN中的位置和尺度问题》[2]和《目标检测中的特征冲突与不对齐问题》[3]。其中其第一篇里面我详细分析了CNN中平移和尺度不变性和相等性问题，并介绍了padding对于位置估计的影响。在第二篇中我们从回归和分类任务对于平移/尺度不变性和相等性要求的冲突展开了一系列讨论，并结合anchor和特征的不对齐介绍了two-stage和one-stage检测算法的相关解决方案。因此强烈建议在阅读本篇内容之前先看看上两篇。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406205630499.png" alt="image-20200406205630499"></p><p>论文一开始就给出了一副直观的示意图，图中展示的是分类和定位任务对于特征空间的敏感性对比，其中分类任务只对特定区域敏感，而定位任务对于目标整体，也可以说是目标边界都很敏感，这一点就衬托出二者对于特征的要求不同。之前的博客里我们所介绍的解决方案是尽可能减少定位和分类分支的特征共享部分。比如原始的Double Head的处理方式是在ROI Pooling之后进行解耦：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406210501137.png" alt="image-20200406210501137"></p><p>而商汤之前<strong>《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》</strong>一文中所提出的Decoupling Head算法则是更为复杂的方式，我之前的博客中好像说漏了一部分，而关于这一点的详细介绍可以在这篇文章中得到解答。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200320132308113.png" alt="image-20200320132308113"></p><h2 id="二、-Task-aware-spatial-disentanglement-learning"><a href="#二、-Task-aware-spatial-disentanglement-learning" class="headerlink" title="二、 Task-aware spatial disentanglement learning"></a>二、 Task-aware spatial disentanglement learning</h2><p>论文所提出的算法名为TSD，我们重点要关注的是图中的三个部分，我们先看Spatial disentanglement和TSD部分：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406211223491.png" alt="image-20200406211223491"></p><p>首先作者定义了新的损失函数形式：</p><p>$$ \begin{array}{l} L = L_{cls}^D\left( {H_1^D\left( {{F_l},{{\hat P}_c}} \right),y} \right) + L_{loc}^D\left( {H_2^D\left( {{F_l},{{\hat P}_r}} \right),B} \right)\\ \left\{ \begin{array}{l} H_1^D\left( \cdot \right) = \left\{ {{f_c}\left( \cdot \right),C\left( \cdot \right)} \right\}\\ H_2^D\left( \cdot \right) = \left\{ {{f_r}\left( \cdot \right),R\left( \cdot \right)} \right\} \end{array} \right. \end{array} $$<br>跟传统的方式不同，作者想通过特征变换的方式让两个任务执行得更好，关于这一点可以在[3]中的Guided Anchoring和AlignDet等算法中看到类似的思路，<strong>即基于预测的anchor框offset，通过deform conv的方式重新提取特征进行分类</strong>。这里作者<strong>令分类任务的特征变换形式为point-wise，令回归任务特征变换的形式为proposal-wise</strong>，也就是说作者让网络特征通过两种变换形式变成了任务驱动型的特征输入。</p><p>下面我们来看看两种变换具体是怎么执行的。</p><p>$$ \left\{ \begin{array}{l} {{\hat P}_c} = {\tau _c}\left( {P,\Delta C} \right),\Delta C = \gamma {F_c}\left( {F;{\theta _c}} \right) \cdot \left( {w,h} \right)\\ {{\hat P}_r} = {\tau _r}\left( {P,\Delta R} \right),\Delta R = \gamma {F_r}\left( {F;{\theta _r}} \right) \cdot \left( {w,h} \right) \end{array} \right. $$<br>对于分类部分，这一部分我们比较熟悉，作者通过第一阶段的回归结果，预测了256x256x(kxkx2)大小的偏移量，其中kxk指的是ROI Pooling之后的特征图大小。通过这个offset，基于deformable conv的方式对特征重采样，重采样的特征用于分类，之前已经讨论过了。</p><p>我们再看回归部分，作者通过第一阶段的回归结果，在此基础上预测了框特征的偏移量，即256x256x2，不同的是，这个偏移量是proposal-wise的，也就是说<strong>框的整体平移</strong>，通过平移进行特征采样。</p><p>综上，TSD部分的创新在于对回归和分类都进行了特征变换，不同的是回归采用特征平移，而分类则采用我们熟悉的模式。</p><h2 id="三、-Progressive-constraint"><a href="#三、-Progressive-constraint" class="headerlink" title="三、 Progressive constraint"></a>三、 Progressive constraint</h2><p>如果我们观察了Decoupling Head框架，会发现除了回归和分类两个分支，作者还保留了原有的ROI-Pooling主干。关于这一点，作者在这篇文章也解释了，就是上文中的Sibling head+PC部分。这一部分呢主要还是为了提升TSD性能而提出来的，我们可以总结为<strong>一致性约束，即令TSD和传统ROI Pooling主干结果保持一致。</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406223228849.png" alt="image-20200406223228849"></p><h2 id="四-、实验结果"><a href="#四-、实验结果" class="headerlink" title="四 、实验结果"></a>四 、实验结果</h2><p>这里我简单罗列部分实验结果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406223341787.png" alt="image-20200406223341787"></p><p>这里作者主要想表明TSD+PC的策略对于不同的backbone都有提升，这里我再给出COCO的结果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406223604607.png" alt="image-20200406223604607"></p><p>可见本文的效果的确不错。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>综上呢，其实我们可以看到，绝大多数的内容都在之前的博客中讲到了，不同的地方只有回归部分特征的平移策略，以及模型的ensemble策略。这篇博客的主要目的是对之前内容的一点补充，以上这些主要讲的还是特征层面的应对策略，对于我们之前所提及的关于NMS过程中分类置信度的“不可信”问题并没有进行说明。</p><p>而之前有知友在评论里面提到了ICCV2019的Guassian YOLOv3，其代码链接为：<a href="https://github.com/jwchoi384/Gaussian_YOLOv3">https://github.com/jwchoi384/Gaussian_YOLOv3</a></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200406/image-20200406224549413.png" alt="image-20200406224549413"></p><p>这个算法通过将回归任务改进为高斯分布参数的预测，从侧面预测了回归框的可靠性，进而利用这部分参数修正回归框和分类置信度，不失为一种改进策略：</p><p>$$ prob = {P_{object}} \times {P_{clas{s_i}}} \times \left( {1 - \frac{{{\delta _x} + {\delta _y} + {\delta _w}{\rm{ + }}{\delta _h}}}{4}} \right) $$<br>不过这方面的研究还没结束，期待以后有更简洁的特征层面的解决策略~</p><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1] Revisiting the Sibling Head in Object Detector</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/113443895">https://zhuanlan.zhihu.com/p/113443895</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/114700229">https://zhuanlan.zhihu.com/p/114700229</a></p><p>[4]1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation.</p><p>[5]Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomo</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;前段时间我在专栏里详细分析了目标检测中的特征冲突与不对齐问题，今天无意间又看到了商汤在CVPR2020的一篇相关论文，分析角度跟我之前所说的类似，但是解决方案增加了一些技巧，论文中提到其对于各类backbone都有~3%mAP的提升，该算法也用到了OpenImage比赛中，是对Decoupling Head框架的详细分析。这里我们一起来分析下论文内容。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>联合检测和跟踪的MOT算法解析（含MOT17 No.1等多个榜前算法!）</title>
    <link href="https://huangpiao.tech/2020/04/03/%E8%81%94%E5%90%88%E6%A3%80%E6%B5%8B%E5%92%8C%E8%B7%9F%E8%B8%AA%E7%9A%84MOT%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"/>
    <id>https://huangpiao.tech/2020/04/03/联合检测和跟踪的MOT算法解析/</id>
    <published>2020-04-03T09:35:00.000Z</published>
    <updated>2020-04-03T09:35:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>最近一年里，随着Tracktor++这类集成检测和多目标跟踪算法框架的出现，涌现了很多相关的多目标跟踪算法变种，基本都位列MOT Challeng榜单前列，包括刚刚开源的榜首CenterTrack。这里我就对集成检测和跟踪的框架进行分析，相关MOT和数据关联的基础知识可以去我的<a href="https://zhuanlan.zhihu.com/c_1212750151892365312">专栏</a>查看，后期我也会针对基于深度学习的数据关联、ReID2MOT和SOT2MOT等进行专题介绍。</p></blockquote><a id="more"></a><h2 id="1-Detect-to-Track-and-Track-to-Detect（D-amp-T"><a href="#1-Detect-to-Track-and-Track-to-Detect（D-amp-T" class="headerlink" title="1.Detect to Track and Track to Detect（D&amp;T)"></a>1.Detect to Track and Track to Detect（D&amp;T)</h2><blockquote><p>作者：Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman</p><p>备注信息：ICCV2017</p><p>论文链接：<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Feichtenhofer_Detect_to_Track_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Feichtenhofer_Detect_to_Track_ICCV_2017_paper.pdf</a></p><p>代码链接：<a href="https://github.com/feichtenhofer/Detect-Track">https://github.com/feichtenhofer/Detect-Track</a></p></blockquote><p>当前的多目标跟踪算法主流是基于检测的框架，即Detection based Tracking(DBT)，所以检测的质量对于跟踪的性能影响是很大的。那么在MOT Challenge上也分别设置了两种赛道，一种是采用官方提供的几种公共检测器的结果，即public赛道，一种是允许参赛者使用自己的检测器，即private赛道。</p><p>这篇D&amp;T就属于private类跟踪框架，并初步将检测与跟踪框架进行了结合：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403134756623.png" alt="image-20200403134756623"></p><p>从图中可以清晰看到，作者通过改进版的R-FCN检测网络实现了主线的检测任务，然后基于两阶段目标检测的特点，将第一阶段所获得的多尺度特征图进行交互。这种方式借鉴了单目标跟踪中经典的Siamese网络框架，不同之处在于原本的Siamese网络做的是1:1的相关滤波，而D&amp;T框架做的是n:n的相关滤波。其中两个分支中所包含的目标数量也是不定的，那么为什么作者要用R-FCN网络呢，可以发现，R-FCN的网络结构起到了很好的作用，正是因为其独特的position-sensitive ROI Pooling模块：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/9090908-7736545774930dc2.webp" alt="img"></p><p>不同于传统两阶段目标检测框架利用全连接网络分支预测分类和回归的情况，R-FCN通过全卷积的方式将分类得分转化到特征图通道上，使得特征图保持了一定的平移不变性（这个可以看我之前的<a href="https://zhuanlan.zhihu.com/p/113443895">博客</a>），有利于跟踪任务的相关滤波。那么这里D&amp;T在传统目标检测的分类和回归任务上，增加了一个跟踪分支，作者巧妙地将跟踪任务转化成了预测相邻两帧各目标位置相对偏移量的回归任务。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403140822424.png" alt="image-20200403140822424"></p><p>当然，跟踪分支只考虑与gt的IOU&gt;0.5的预测框，并且目标要同时出现在这两帧。多任务损失函数如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403141030323.png" alt="image-20200403141030323"></p><p>最后我们谈一下最重要的一点，如何做ROI Tracking，即在不丢失相对位置关系的前提下，执行多个区域的相关滤波：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403135326044.png" alt="image-20200403135326044"></p><p>提到相关滤波，我们可能容易想到单目标跟踪中的CF类传统方法，比如KCF（详细原理可以看我的<a href="https://zhuanlan.zhihu.com/p/56405827">解析</a>）。KCF算法中就是通过循环移位的方式，利用相关滤波估计目标在图像中的位置变化。但是这种方式并不适合多目标的相关滤波，我们基于相邻两帧变化幅度不大的假设，更希望的是每块局部区域单独做类似于循环移位之类的操作。对此，作者借鉴了FlowNet的Corr操作，因为光流任务也是估计相邻帧像素的偏移量，所以用在这里很合适。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/20160116203948666" alt="20160116203948666"></p><p>Corr的公式是：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/20160116204023529" alt="这里写图片描述"></p><p>可以看到，这里的滤波不是对卷积核的，而是将两幅特征图的多个kxk的区域分别做相关滤波，从而保持了相对位置。</p><p>最后对于多目标跟踪的部分，作者对于两个目标的连接代价设置如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403142449157.png" alt="image-20200403142449157"></p><p>其中p表示的相邻两帧的检测置信度，最后一项指的是相邻两帧的目标框与预测到的位置的IOU&gt;0.5时为1，否则为0。至此我们就可以得到跟踪预测位置和代价矩阵了，后面就是常规的多目标跟踪算法操作了。</p><h2 id="2-Real-time-multiple-people-tracking-with-deeply-learned-candidate-selection-and-person-re-identification（MOTDT"><a href="#2-Real-time-multiple-people-tracking-with-deeply-learned-candidate-selection-and-person-re-identification（MOTDT" class="headerlink" title="2.Real-time multiple people tracking with deeply learned candidate selection and person re-identification（MOTDT)"></a>2.Real-time multiple people tracking with deeply learned candidate selection and person re-identification（MOTDT)</h2><blockquote><p>作者：Long Chen, Haizhou Ai, Zijie Zhuang, Chong Shang</p><p>备注信息：ICME2018</p><p>论文链接：<a href="https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification">https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification</a></p><p>代码链接：<a href="https://github.com/longcw/MOTDT">https://github.com/longcw/MOTDT</a></p></blockquote><p>这篇论文表面看上基于R-FCN检测框架的private多目标跟踪算法，不过与上一篇不同的是，<strong>作者只利用R-FCN对观测框进行进一步的前景/背景分类，即用于目标框的分类过滤，而且MOTDT将检测和跟踪框架分离了</strong>。作者的框架也是由现在多目标跟踪算法的通用模块组成的，即检测、外观模型和运动模型。这里我们就只关注他的算法流程：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403143247609.png" alt="image-20200403143247609"></p><p>从算法流程可以清晰地看到，MOTDT的流程是：</p><ul><li><p>利用Kalman Filter完成目标的运动估计；</p></li><li><p>将观测框和跟踪框合并，并做NMS操作，其中每个目标框的置信度得到了修正：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403143519741.png" alt="image-20200403143519741"></p><p>这里面L表示的长度，通过上面两个公式，作者将检测置信度和跟踪轨迹置信度结合在一起了。</p></li><li><p>提取ReID特征，先基于ReID相似度进行匹配，再对剩余的利用IOU进行关联。</p></li></ul><p>MOTDT这个算法框架很经典，对于后续的一些多目标跟踪算法也起到了启发作用。</p><h2 id="3-Tracking-without-bells-and-whistles-Tracktor"><a href="#3-Tracking-without-bells-and-whistles-Tracktor" class="headerlink" title="3.Tracking without bells and whistles(Tracktor++)"></a>3.Tracking without bells and whistles(Tracktor++)</h2><blockquote><p>作者：Philipp Bergmann，Tim Meinhardt，Laura Leal-Taixe</p><p>备注信息：ICCV2019，MOT15~17: 46.6, 56.2. 56.3 MOTA(public）</p><p>论文链接：<a href="https://arxiv.org/pdf/1903.05625.pdf">https://arxiv.org/pdf/1903.05625.pdf</a></p><p>代码链接：<a href="https://github.com/phil-bergmann/tracking_wo_bnw">https://github.com/phil-bergmann/tracking_wo_bnw</a></p></blockquote><p>Tracktor++算法是去年出现的一类全新的联合检测和跟踪的框架，这类框架与MOTDT框架最大的不同在于，检测部分不仅仅用于前景和背景的进一步分类，还利用回归对目标进行了进一步修正，因此关于这类框架属于public还是private得争论也存在，这里我们就不做过多的讨论了。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403144435797.png" alt="image-20200403144435797"></p><p>只要熟悉两阶段目标检测算法的应该都能理解这个算法，<strong>其核心在于利用跟踪框和观测框代替原有的RPN模块，从而得到真正的观测框，最后利用数据关联实现跟踪框和观测框的匹配。</strong>流程图如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403145105871.png" alt="image-20200403145105871"></p><p>有了检测模块的加持，自然对于检测质量进行了增强，所以效果也得到了大幅提升：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403145233804.png" alt="image-20200403145233804"></p><p>可以看到，DPM、FRCNN和SDP三种检测器输入下的性能差距不大，然而DPM检测器的性能是很差的，所以Tracktor++这类算法对于平衡检测输入的效果提升很大。</p><h2 id="4-Multiple-Object-Tracking-by-Flowing-and-Fusing-FFT"><a href="#4-Multiple-Object-Tracking-by-Flowing-and-Fusing-FFT" class="headerlink" title="4.Multiple Object Tracking by Flowing and Fusing(FFT)"></a>4.Multiple Object Tracking by Flowing and Fusing(FFT)</h2><blockquote><p>作者：Jimuyang Zhang, Sanping Zhou, Xin Chang, Fangbin Wan, Jinjun Wang, Yang Wu, Dong Huang</p><p>备注信息：MOT15~17: 46.3, 56.5. 56.5 MOTA(public）</p><p>论文链接：<a href="https://arxiv.org/abs/2001.11180">https://arxiv.org/abs/2001.11180</a></p></blockquote><p>这篇文章也是基于Tracktor++的模式，做了很直接的一步操作，即直接增加一个光流预测分支，将Tracktor++中的<code>跟踪框+观测框</code>变成了<code>光流预测框+观测框</code></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403145545096.png" alt="image-20200403145545096"></p><p>不过好处在于光流网络和Faster RCNN可以联合训练，在训练的时候RPN保留，不过从论文来看光流部分好像是固定权重的，其效果相对来说的确更好了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403150200732.png" alt="image-20200403150200732"></p><h2 id="5-Towards-Real-Time-Multi-Object-Tracking-JDE"><a href="#5-Towards-Real-Time-Multi-Object-Tracking-JDE" class="headerlink" title="5.Towards Real-Time Multi-Object Tracking(JDE)"></a>5.Towards Real-Time Multi-Object Tracking(JDE)</h2><blockquote><p>作者：Zhongdao Wang，Liang Zheng，Yixuan Liu，Shengjin Wang</p><p>备注信息：MOT16 74.8 MOTA(private), 22FPS!!</p><p>论文链接：<a href="https://arxiv.org/pdf/1909.12605v1.pdf">https://arxiv.org/pdf/1909.12605v1.pdf</a></p><p>代码链接：<a href="https://github.com/Zhongdao/Towards-Realtime-MOT">https://github.com/Zhongdao/Towards-Realtime-MOT</a></p></blockquote><p>JDE这篇跟这次的主题不是很相符，但是考虑到这也是近期比较热门的实时多目标跟踪算法，我们也一起讲。<strong>它的框架出发点是为了增加特征的复用性，基于检测算法（作者采用的是YOLOv3），在原本的分类和回归分支上增加了一个表观特征提取的分支。</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403150502509.png" alt="image-20200403150502509"></p><p>文中作者重点介绍了多任务网络框架的训练方式，首先分析了三种Loss：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403151745337.png" alt="image-20200403151745337"></p><p>对于triplet loss，这个在表观模型的metric learning任务中很常见，作者采用了batch hard模式，并提出了triplet loss的上界，推导很简单，关键在于多的那个<code>1</code>。为了更好地跟交叉熵损失函数进行比较，作者将上界进行了平滑。那么区别就在于<code>g</code>，g表示的正负样本的权重。在交叉熵损失函数中，所有的负样本都会参与计算，然而在triplet loss中，负样本是采样出来的，所以：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403152549909.png" alt="image-20200403152549909"></p><p>作者通过实验也论证了上面的结论，所以在metric learning中作者采用了交叉熵损失函数。最后关于各个任务的损失函数的权重，作者提出了一种自适应平衡的加权方式：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403152909142.png" alt="image-20200403152909142"></p><p>其中的s是一种度量不同任务下个体损失的不确定性因子，详细的原理可参见CVPR2018的《 Multi-task learning using uncertainty to weigh losses for scene geometry and semantics》关于方差不确定性对于多任务权重的影响分析。</p><p>效果和速度都很诱人~</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403150659198.png" alt="image-20200403150659198"></p><h2 id="6-Refinements-in-Motion-and-Appearance-for-Online-Multi-Object-Tracking-MIFT"><a href="#6-Refinements-in-Motion-and-Appearance-for-Online-Multi-Object-Tracking-MIFT" class="headerlink" title="6.Refinements in Motion and Appearance for Online Multi-Object Tracking(MIFT)"></a>6.Refinements in Motion and Appearance for Online Multi-Object Tracking(MIFT)</h2><blockquote><p>作者：Piao Huang, Shoudong Han, Jun Zhao, Donghaisheng Liu, HongweiWang, En Yu, and Alex ChiChung Kot</p><p>备注信息：MOT15~17: 60.1, 60.4, 48.1 MOTA(public）</p><p>论文链接：<a href="https://arxiv.org/abs/2003.07177">https://arxiv.org/abs/2003.07177</a></p><p>代码链接：<a href="https://github.com/nightmaredimple/libmot">https://github.com/nightmaredimple/libmot</a></p></blockquote><p>这篇也是我们团队基于Tracktor++框架做的一个框架，主要关注的是运动模型、表观模型和数据关联部分的改进，由于某些原因，我这里不能细讲。代码会慢慢开源，暂时没有完全开源。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403153552817.png" alt="image-20200403153552817"></p><p>其中对于运动模型部分，我们将Kalman和ECC模型集成在一起，而不是将Kalman和ECC模型独立执行，实验证明融合的版本比分开的提升了1.4 MOTA。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403153753222.png" alt="image-20200403153753222"></p><p>对于表观模型，我们考虑到特征对齐的因素，做了一点小改进，结合可视度预测设计了多任务的表观模型：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403154002112.png" alt="image-20200403154002112"></p><p>并在观测框和跟踪轨迹特征比对的时候，考虑了跟踪轨迹历史信息，来进行自适应加权：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403154058675.png" alt="image-20200403154058675"></p><p>通过上面的分析，我们可以知道的是，数据关联部分的特征相似度计算，不仅要进行n:m的Kalman更新过程（为了求马氏距离），还要进行m:(nxk)的表观特征比对，这个过程很耗时。所以我们利用3-D integral image快速将空间区域分配，使得特征相似度计算过程的复杂度降至O(m+n)。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403154343987.png" alt="image-20200403154343987"></p><p>方法很巧妙，就是将每个观测框利用one-hot编码映射到特征图，这种方式比基于iou的要快很多：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403154527814.png" alt="image-20200403154527814"></p><p>我后期又做了一些实验，效果比论文中的更好一些，MOT15~17: 48.1、60.4、60.1 MOTA(public）</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403154835300.png" alt="image-20200403154835300"></p><h2 id="7-Tracking-Objects-as-Points-CenterTrack"><a href="#7-Tracking-Objects-as-Points-CenterTrack" class="headerlink" title="7.Tracking Objects as Points(CenterTrack)"></a>7.Tracking Objects as Points(CenterTrack)</h2><blockquote><p>作者：Xingyi Zhou(<strong>CenterNet的作者</strong>), Vladlen Koltun, and Philipp Krähenbühl</p><p>备注信息：同时实现了2D/3D多目标跟踪，包含人和车辆，MOT17：61.4(public）、67.3(private) MOTA, 22FPS!!!</p><p>KITTI：89.4MOTA</p><p>论文链接：<a href="http://arxiv.org/abs/2004.01177">http://arxiv.org/abs/2004.01177</a></p><p>代码链接：<a href="https://github.com/xingyizhou/CenterTrack">https://github.com/xingyizhou/CenterTrack</a></p></blockquote><p>CenterTrack是CenterNet作者基于Tracktor++这类跟踪机制，通过将Faster RCNN换成CenterNet实现的一种多目标跟踪框架，因此跟踪框也就变成了跟踪中心点。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403155115318.png" alt="image-20200403155115318"></p><p>通过上图我们可以大致分析出算法框架，除了对相邻两帧利用CenterNet进行检测之外，还利用了上文中提到的D&amp;T框架的策略，预测同时存在于两帧中目标的相对位移，由此进行跟踪预测。对于提供的观测框，作者通过将这些观测框的中心点映射到一张单通道的heatmap上，然后利用高斯模糊的方式将点的附近区域也考虑进去。</p><p><strong>因此CenterTrack相对于CenterNet的不同之处在于，输入维度增加了（两幅3维图像和一张观测位置heatmap），输出变成了两张图像的目标中心位置、大小和相对偏移。</strong></p><p>对于测试环节的数据关联部分，作者直接通过中心点的距离来判断是否匹配，是一种贪婪的方式，并非匈牙利算法那种全局的数据关联优化。在训练过程中，作者并非只用相邻帧进行训练，允许跨3帧。</p><p>CenterTrack在MOT、KITTI和nuScenes等数据集上的2D/3D多行人/车辆跟踪任务上均取得了SOTA的成绩。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403160241964.png" alt="image-20200403160241964"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200403/image-20200403160309217.png" alt="image-20200403160309217"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Feichtenhofer C, Pinz A, Zisserman A. Detect to track and track to detect[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2017. 3038-3046.<br>[2] Chen L, Ai H, Zhuang Z, et al. Real-time multiple people tracking with deeply learned candidate selection and person re-identification[C]. in: 2018 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2018. 1-6.<br>[3] Bergmann P, Meinhardt T, Leal-Taixe L. Tracking without bells and whistles[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 941-951.<br>[4] Multiple Object Tracking by Flowing and Fusing<br>[5] Towards Real-Time Multi-Object Tracking<br>[6] Refinements in Motion and Appearance for Online Multi-Object Tracking<br>[7] Tracking Objects as Points</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;最近一年里，随着Tracktor++这类集成检测和多目标跟踪算法框架的出现，涌现了很多相关的多目标跟踪算法变种，基本都位列MOT Challeng榜单前列，包括刚刚开源的榜首CenterTrack。这里我就对集成检测和跟踪的框架进行分析，相关MOT和数据关联的基础知识可以去我的&lt;a href=&quot;https://zhuanlan.zhihu.com/c_1212750151892365312&quot;&gt;专栏&lt;/a&gt;查看，后期我也会针对基于深度学习的数据关联、ReID2MOT和SOT2MOT等进行专题介绍。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="MOT" scheme="https://huangpiao.tech/tags/MOT/"/>
    
      <category term="Detection" scheme="https://huangpiao.tech/tags/Detection/"/>
    
  </entry>
  
  <entry>
    <title>多目标跟踪中的相机运动模型</title>
    <link href="https://huangpiao.tech/2020/03/27/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%A8%A1%E5%9E%8B/"/>
    <id>https://huangpiao.tech/2020/03/27/多目标跟踪中的相机运动模型/</id>
    <published>2020-03-27T09:10:00.000Z</published>
    <updated>2020-03-27T09:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>之前的博客中我介绍了Kalman滤波器，这个算法被广泛用于多目标跟踪任务中的行人运动模型。然而实际场景中存在有很多相机运动，仅仅依赖行人运动模型是不够的。这次我主要介绍下相机运动模型，以对极几何和ECC为主。完整的代码和示例我都放在了<a href="https://github.com/nightmaredimple/libmot">github</a>。</p></blockquote><a id="more"></a><h2 id="1-多目标跟踪中的相机运动"><a href="#1-多目标跟踪中的相机运动" class="headerlink" title="1 多目标跟踪中的相机运动"></a>1 多目标跟踪中的相机运动</h2><p>在多目标跟踪场景中往往存在有复杂的运动模式，这些模式除了行人这类非刚性运动，还有相机这类刚性运动。以MOT Challenge数据集为例，其中就存在大量相机运动场景，甚至超过了静态相机场景数。比如MOT17-13号视频中车载相机在车辆转弯时对于两个运动速度较慢行人的视角：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327161756167.png" alt="image-20200327161756167" style="zoom:50%"></p><p>我们从示意图可以看到，由于车辆转弯速度很快，上一帧的行人位置映射到下一帧就变成了另一个的位置。因此相机运动对于多目标跟踪的影响很大，尤其是仅依赖运动信息的模型，相机的运动会严重干扰运动模型。</p><h2 id="2对极几何"><a href="#2对极几何" class="headerlink" title="2对极几何"></a>2对极几何</h2><h3 id="2-1-对极几何模型"><a href="#2-1-对极几何模型" class="headerlink" title="2.1 对极几何模型"></a>2.1 对极几何模型</h3><p>关于相机运动方面的知识，我在之前介绍<a href="https://zhuanlan.zhihu.com/p/111759578">单目深度估计</a>中的无监督模型时介绍过，即将变化差异不剧烈的两帧画面近似看作不同相机视角下同一场景的画面，也就是对极几何，这一点可以看看《计算机视觉中的多视几何》中关于相机几何方面的知识：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/20161126220621480" alt="img"></p><p>不过这里我需要先解释一下一些概念，以方便后续模型的讲解：</p><ol><li>基线[baseline]：直线CC’为基线。</li><li>对极平面束[epipolar pencil]：以基线为轴的平面束。</li><li>对极平面[epipolar plane]：任何包含基线的平面都称为对极平面。</li><li>对极点[epipole]：摄像机的基线与每幅图像的交点。比如，上图中的点x和x’。</li><li>对极线[epipolar line]：对极平面与图像的交线。</li><li>5点共面：点x，x’，摄像机中心C、C’，空间点X是5点共面的。</li><li>极线约束：两极线上点的对应关系。</li></ol><p>接下来，我们首先看一篇ACM MM2019的论文TNT[1]，这是一篇研究端到端运动信息和表观信息结合框架的论文：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327173044302.png" alt="image-20200327173044302"></p><p>不过这里我们要讲的是其提出来的相机运动模型：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327173339469.png" alt="image-20200327173339469"></p><p>我们可以看到，作者将行人运动和相机运动结合了，其中目标函数的第一部分是利用了对极几何中本质矩阵F的性质，相关的理论推导可以看下图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327173257802.png" alt="image-20200327173257802"></p><p>其中x表示的目标框的四个顶点的坐标信息，第二部分中作者则是假设两帧中的同一目标的形状近似不变。因此我们只需要求得本质矩阵F，即可根据上一帧目标框信息，利用最小二乘法求得下一帧目标框信息。关于本质矩阵F的求解，作者提到基于SURF特征点提取和Ransac采样进行估计。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327174308906.png" alt="image-20200327174308906"></p><p>不过作者也没有给出详细的实现过程，我这里试着做一下理论推导。首先由于作者在目标函数中要求了目标框形状的一致性，那么我们不妨直接把下一帧目标框的形状信息看做已知的。其次，我们先假设本质矩阵F已经被估计出来了，这个矩阵是3x3的形状，那么为了推导方便，我这里做一个假设：</p><blockquote><p>对于第t帧的任意一个目标框的每一个节点$x$，这里由于是三维的几何信息，所以添加一个z轴坐标，令$x^TF$为一个已知的三维向量，那么一个目标框就存在四个这样的三维向量，不妨看作一个4x3的矩阵M</p></blockquote><p>那么就可以将目标函数展开，这里面的(w,h)为已知信息，(x,y)为下一帧目标框的左上角坐标：</p><p>$$ \begin{array}{l} \left\{ \begin{array}{l} {M_{11}}x + {M_{12}}y + {M_{13}} = 0\\ {M_{21}}\left( {x + w} \right) + {M_{22}}y + {M_{23}} = 0\\ {M_{31}}x + {M_{32}}\left( {y + h} \right) + {M_{33}} = 0\\ {M_{41}}\left( {x + w} \right) + {M_{42}}\left( {y + h} \right) + {M_{43}} = 0 \end{array} \right.\\ \Rightarrow \left[ {\begin{array}{*{20}{c}} {{M_{11}}}&{{M_{12}}}\\ {{M_{21}}}&{{M_{22}}}\\ {{M_{31}}}&{{M_{32}}}\\ {{M_{41}}}&{{M_{42}}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ y \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{M_{13}}}\\ {{M_{23}}{\rm{ + }}w{M_{21}}}\\ {{M_{33}}{\rm{ + }}h{M_{32}}}\\ {{M_{43}}{\rm{ + }}w{M_{41}} + h{M_{42}}} \end{array}} \right] \end{array} $$<br>很明显这就是一个典型的Ax=b问题，后面的问题就迎刃而解了。</p><h3 id="2-2-实验分析"><a href="#2-2-实验分析" class="headerlink" title="2.2 实验分析"></a>2.2 实验分析</h3><p>为了保证效率，我这里采用ORB特征提取策略，然后采用brute force的匹配策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Epipolar</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_method = <span class="string">'orb'</span>, match_method = <span class="string">'brute force'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 metric = cv2.NORM_HAMMING, n_points = <span class="number">50</span>, nfeatures = <span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 scaleFactor = <span class="number">1.2</span>, nlevels = <span class="number">8</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Using Epipolar Geometry to Estimate Camara Motion</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        feature_method : str</span></span><br><span class="line"><span class="string">            the method of feature extraction, the default is ORB, more methods will be added in the future</span></span><br><span class="line"><span class="string">        match_method : str</span></span><br><span class="line"><span class="string">            the method of feature matching, the default is brute force, more methods will be added in the future</span></span><br><span class="line"><span class="string">        metric: metrics in cv2</span></span><br><span class="line"><span class="string">            distance metric for feature matching</span></span><br><span class="line"><span class="string">        n_points: int</span></span><br><span class="line"><span class="string">            numbers of matched points to be considered</span></span><br><span class="line"><span class="string">        nfeatures: int</span></span><br><span class="line"><span class="string">            numbers of features to be extract</span></span><br><span class="line"><span class="string">        scaleFactor: float</span></span><br><span class="line"><span class="string">            scale factor for orb</span></span><br><span class="line"><span class="string">        nlevels: float</span></span><br><span class="line"><span class="string">            levels for orb</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.metric = metric</span><br><span class="line">        <span class="keyword">if</span> feature_method == <span class="string">'orb'</span>:</span><br><span class="line">            self.feature_extractor = cv2.ORB_create(nfeatures = nfeatures,</span><br><span class="line">                                                    scaleFactor = scaleFactor, nlevels = nlevels)</span><br><span class="line">        <span class="keyword">if</span> match_method == <span class="string">'brute force'</span>:</span><br><span class="line">            self.matcher = cv2.BFMatcher(metric, crossCheck=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.n_points = n_points</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FeatureExtract</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        <span class="string">"""Detect and Compute the input image's keypoints and descriptors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        img : ndarray of opencv</span></span><br><span class="line"><span class="string">            An HxW(x3) matrix of img</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        keypoints : List of cv2.KeyPoint</span></span><br><span class="line"><span class="string">            using keypoint.pt can see (x,y)</span></span><br><span class="line"><span class="string">        descriptors: List of descriptors[keypoints, features]</span></span><br><span class="line"><span class="string">            keypoints: keypoints which a descriptor cannot be computed are removed</span></span><br><span class="line"><span class="string">            features: An Nx32 ndarray of unit8 when using "orb" method</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> img.ndim == <span class="number">3</span>:</span><br><span class="line">            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the keypoints with ORB</span></span><br><span class="line">        keypoints = self.feature_extractor.detect(img, <span class="keyword">None</span>)</span><br><span class="line">        <span class="comment"># compute the descriptors with ORB</span></span><br><span class="line">        keypoints, descriptors = self.feature_extractor.compute(img, keypoints)</span><br><span class="line">        <span class="keyword">return</span> keypoints, descriptors</span><br></pre></td></tr></table></figure><p>那么对于本质矩阵的估计和最小二乘法的应用，都可以直接利用已有的工具箱opencv和numpy搞定：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetFundamentalMat</span><span class="params">(self, keypoints1, descriptors1, keypoints2, descriptors2)</span>:</span></span><br><span class="line">    <span class="string">"""Estimate FunfamentalMatrix using BF matcher and ransac</span></span><br><span class="line"><span class="string">        [p2;1]^T K^(-T) E K^(-1) [p1;1] = 0, T means transpose, K means the intrinsic matrix of camera</span></span><br><span class="line"><span class="string">        F = K^(-T) E K^(-1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    keypoints : List of cv2.KeyPoint</span></span><br><span class="line"><span class="string">        using keypoint.pt can see (x,y)</span></span><br><span class="line"><span class="string">    descriptor : ndarray</span></span><br><span class="line"><span class="string">        An Nx32 matrix of descriptors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    F: ndarray</span></span><br><span class="line"><span class="string">        A 3x3 Matrix of Fundamental Matrix</span></span><br><span class="line"><span class="string">    mask: ndarray</span></span><br><span class="line"><span class="string">        A Nx1 Matrix of those inline points</span></span><br><span class="line"><span class="string">    pts1: List of cv2.KeyPoint</span></span><br><span class="line"><span class="string">        keypoints matched</span></span><br><span class="line"><span class="string">    pts2: List of cv2.KeyPoint</span></span><br><span class="line"><span class="string">        keypoints matched</span></span><br><span class="line"><span class="string">    matches : List of matches</span></span><br><span class="line"><span class="string">        distance - distance of two points,</span></span><br><span class="line"><span class="string">        queryIdx - query image's descriptor id, default is the second image</span></span><br><span class="line"><span class="string">        trainIdx - train image's descriptor id, default is the second image</span></span><br><span class="line"><span class="string">        imageIdx - train image's id, default is 0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># matching points</span></span><br><span class="line">    matches = self.matcher.match(descriptors1, descriptors2)</span><br><span class="line">    matches = sorted(matches, key=<span class="keyword">lambda</span> x: x.distance)</span><br><span class="line"></span><br><span class="line">    pts1 = []</span><br><span class="line">    pts2 = []</span><br><span class="line">    <span class="keyword">for</span> i, match <span class="keyword">in</span> enumerate(matches):</span><br><span class="line">        <span class="keyword">if</span> i &gt;= self.n_points:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        pts1.append(keypoints1[match.queryIdx].pt)</span><br><span class="line">        pts2.append(keypoints2[match.trainIdx].pt)</span><br><span class="line"></span><br><span class="line">    pts1 = np.int32(pts1)</span><br><span class="line">    pts2 = np.int32(pts2)</span><br><span class="line">    matches = matches[:self.n_points]</span><br><span class="line"></span><br><span class="line">    <span class="comment">## Estimate Fundamental Matrix by ransac, distance_threshold = 1, confidence_threshold = 0.99</span></span><br><span class="line">    F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC, <span class="number">1</span>, <span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> F, mask, pts1, pts2, matches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EstimateBox</span><span class="params">(self, boxes, F)</span>:</span></span><br><span class="line">    <span class="string">"""Estimate box in target image by Fundamental Matrix</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    boxes : array like</span></span><br><span class="line"><span class="string">        A Nx4 matrix of boxes in source images (x,y,w,h)</span></span><br><span class="line"><span class="string">    F : ndarray</span></span><br><span class="line"><span class="string">        A 3x3 Fundamental Matrix</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    aligned_boxes: ndarray</span></span><br><span class="line"><span class="string">        A Nx4 matrix of boxes in source images (x,y,w,h)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Method</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">        L = ||Bi^T F Ai||2 + ||(A2-A0)+(B2-B0)||2</span></span><br><span class="line"><span class="string">        A is the four corner of box in source image</span></span><br><span class="line"><span class="string">        B is the four corner of aligned box in target image</span></span><br><span class="line"><span class="string">        A0,B0:top left corner of box, [x;y;1]</span></span><br><span class="line"><span class="string">        A1,B1:top right corner of box</span></span><br><span class="line"><span class="string">        A2,B2:bottom left corner of box</span></span><br><span class="line"><span class="string">        A3,B3:bottom right corner of box</span></span><br><span class="line"><span class="string">        the height and width of boxes and aligned boxes are assumed to be same</span></span><br><span class="line"><span class="string">        we can use greedy strategy: make M = A^T F^T</span></span><br><span class="line"><span class="string">        then:</span></span><br><span class="line"><span class="string">            M11   x1   +   M12  y1   + M13 = 0</span></span><br><span class="line"><span class="string">            M21 (x1+w) +   M22  y1   + M23 = 0</span></span><br><span class="line"><span class="string">            M31   x1   +   M32 y1+h  + M33 = 0</span></span><br><span class="line"><span class="string">            M41 (x1+w) +  M42 (y1+h) + M43 = 0</span></span><br><span class="line"><span class="string">        =&gt;</span></span><br><span class="line"><span class="string">            M[:2][x;y] + M[:3]+[0;M21w;M32h;M41w+M42h] = 0 -&gt;Ax = b</span></span><br><span class="line"><span class="string">            x = (pseudo inverse of A )b</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    boxes = np.asarray(boxes)</span><br><span class="line">    <span class="keyword">if</span> boxes.ndim == <span class="number">1</span>:</span><br><span class="line">        boxes = boxes[np.newaxis, :]</span><br><span class="line">    aligned_boxes = np.zeros(boxes.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> enumerate(boxes):</span><br><span class="line">        w = bbox[<span class="number">2</span>]</span><br><span class="line">        h = bbox[<span class="number">3</span>]</span><br><span class="line">        AT = np.array([[bbox[<span class="number">0</span>]   , bbox[<span class="number">1</span>]    , <span class="number">1</span>],</span><br><span class="line">                      [bbox[<span class="number">0</span>] + w, bbox[<span class="number">1</span>]    , <span class="number">1</span>],</span><br><span class="line">                      [bbox[<span class="number">0</span>]    , bbox[<span class="number">1</span>] + h, <span class="number">1</span>],</span><br><span class="line">                      [bbox[<span class="number">0</span>] + w, bbox[<span class="number">1</span>] + h, <span class="number">1</span>]])</span><br><span class="line">        M = AT @ F.T</span><br><span class="line">        b = -M[:, <span class="number">2</span>] - np.array([<span class="number">0</span>, M[<span class="number">1</span>][<span class="number">0</span>]*w, M[<span class="number">2</span>][<span class="number">1</span>]*h, M[<span class="number">3</span>][<span class="number">0</span>]*w+M[<span class="number">3</span>][<span class="number">1</span>]*h])</span><br><span class="line">        aligned_tl = np.linalg.pinv(M[:,:<span class="number">2</span>]) @ b</span><br><span class="line"></span><br><span class="line">        aligned_boxes[i, <span class="number">0</span>] = aligned_tl[<span class="number">0</span>]</span><br><span class="line">        aligned_boxes[i, <span class="number">1</span>] = aligned_tl[<span class="number">1</span>]</span><br><span class="line">        aligned_boxes[i, <span class="number">2</span>] = w</span><br><span class="line">        aligned_boxes[i, <span class="number">3</span>] = h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> aligned_boxes.astype(np.int32)</span><br></pre></td></tr></table></figure><p>具体效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327180100020.png" alt="image-20200327180100020"></p><p>上面极线的法线也正好是车载相机的方向所在，可以看到第一章的示例问题被很大缓解了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327180141990.png" alt="image-20200327180141990"></p><h2 id="3ECC"><a href="#3ECC" class="headerlink" title="3ECC"></a>3ECC</h2><h3 id="3-1原理介绍"><a href="#3-1原理介绍" class="headerlink" title="3.1原理介绍"></a>3.1原理介绍</h3><p>第二章所介绍的对极几何方法，由于我们只是根据二维信息对三维信息的估计，所以也会存在误差。这一张我们也讲一个简单有效的方案，那就是“仿射变换”。当然，并不是我们所理解的那种仿射变换，具体细节我将慢慢介绍。</p><p>第一次看到ECC算法，我是在ICCV2019的Tracktor++[3]中，不过作者只是一笔带过，没有提及如何实现。ECC算法全名是增强相关系数算法[2]，来自于PAMI2008的一篇论文，这个算法适用于图像配准任务的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327180718283.png" alt="image-20200327180718283"></p><p>也就是对于两张内容差异小，但是存在光照、尺度、颜色、平移等变换影响的图像，将二者对齐。ECC算法本质是一个目标函数：</p><p>$$ ECC = {\left\| {\frac{{{x_i}}}{{\left\| {{x_i}} \right\|}} - \frac{{{y_i}}}{{\left\| {{y_i}} \right\|}}} \right\|^2},y = warp(x) $$<br>当然这只是一个原始形式，在求解过程中有所调整，我就不细讲这里的理论了。可以注意到的是y=warp(x)这个函数，所以这个算法假设两帧图像之间存在某种变换，不一定是仿射变换，可能有以下几种：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327181213719.png" alt="image-20200327181213719"></p><p>其中最后一种透视变换的矩阵形式是：</p><p>$$ \left[ {\begin{array}{*{20}{c}} {x'}\\ {y'}\\ {z'} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{a_{11}}}&{{a_{12}}}&{{t_x}}\\ {{a_{21}}}&{{a_{22}}}&{{t_y}}\\ {{a_{23}}}&{{a_{24}}}&{{t_z}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ y\\ z \end{array}} \right] $$<br>前三种变换则不考虑最后一行信息，即2x3的矩阵形式。</p><h3 id="3-2实验分析"><a href="#3-2实验分析" class="headerlink" title="3.2实验分析"></a>3.2实验分析</h3><p>opencv中正好提供了ECC相关的功能函数，这里我们只需要再次封装，以方便多目标跟踪。可以知道的是ECC算法的核心在于变换矩阵的求解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ECC</span><span class="params">(src, dst, warp_mode = cv2.MOTION_EUCLIDEAN, eps = <span class="number">1e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_iter = <span class="number">100</span>, scale = None, align = False)</span>:</span></span><br><span class="line">    <span class="string">"""Compute the warp matrix from src to dst.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    src : ndarray</span></span><br><span class="line"><span class="string">        An NxM matrix of source img(BGR or Gray), it must be the same format as dst.</span></span><br><span class="line"><span class="string">    dst : ndarray</span></span><br><span class="line"><span class="string">        An NxM matrix of target img(BGR or Gray).</span></span><br><span class="line"><span class="string">    warp_mode: flags of opencv</span></span><br><span class="line"><span class="string">        translation: cv2.MOTION_TRANSLATION</span></span><br><span class="line"><span class="string">        rotated and shifted: cv2.MOTION_EUCLIDEAN</span></span><br><span class="line"><span class="string">        affine(shift,rotated,shear): cv2.MOTION_AFFINE</span></span><br><span class="line"><span class="string">        homography(3d): cv2.MOTION_HOMOGRAPHY</span></span><br><span class="line"><span class="string">    eps: float</span></span><br><span class="line"><span class="string">        the threshold of the increment in the correlation coefficient between two iterations</span></span><br><span class="line"><span class="string">    max_iter: int</span></span><br><span class="line"><span class="string">        the number of iterations.</span></span><br><span class="line"><span class="string">    scale: float or [int, int]</span></span><br><span class="line"><span class="string">        scale_ratio: float</span></span><br><span class="line"><span class="string">        scale_size: [W, H]</span></span><br><span class="line"><span class="string">    align: bool</span></span><br><span class="line"><span class="string">        whether to warp affine or perspective transforms to the source image</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    warp matrix : ndarray</span></span><br><span class="line"><span class="string">        Returns the warp matrix from src to dst.</span></span><br><span class="line"><span class="string">        if motion model is homography, the warp matrix will be 3x3, otherwise 2x3</span></span><br><span class="line"><span class="string">    src_aligned: ndarray</span></span><br><span class="line"><span class="string">        aligned source image of gray</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> src.shape == dst.shape, <span class="string">"the source image must be the same format to the target image!"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># BGR2GRAY</span></span><br><span class="line">    <span class="keyword">if</span> src.ndim == <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># Convert images to grayscale</span></span><br><span class="line">        src = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make the imgs smaller to speed up</span></span><br><span class="line">    <span class="keyword">if</span> scale <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> isinstance(scale, float) <span class="keyword">or</span> isinstance(scale, int):</span><br><span class="line">            <span class="keyword">if</span> scale != <span class="number">1</span>:</span><br><span class="line">                src_r = cv2.resize(src, (<span class="number">0</span>, <span class="number">0</span>), fx = scale, fy = scale,interpolation =  cv2.INTER_LINEAR)</span><br><span class="line">                dst_r = cv2.resize(dst, (<span class="number">0</span>, <span class="number">0</span>), fx = scale, fy = scale,interpolation =  cv2.INTER_LINEAR)</span><br><span class="line">                scale = [scale, scale]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                src_r, dst_r = src, dst</span><br><span class="line">                scale = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> scale[<span class="number">0</span>] != src.shape[<span class="number">1</span>] <span class="keyword">and</span> scale[<span class="number">1</span>] != src.shape[<span class="number">0</span>]:</span><br><span class="line">                src_r = cv2.resize(src, (scale[<span class="number">0</span>], scale[<span class="number">1</span>]), interpolation = cv2.INTER_LINEAR)</span><br><span class="line">                dst_r = cv2.resize(dst, (scale[<span class="number">0</span>], scale[<span class="number">1</span>]), interpolation=cv2.INTER_LINEAR)</span><br><span class="line">                scale = [scale[<span class="number">0</span>] / src.shape[<span class="number">1</span>], scale[<span class="number">1</span>] / src.shape[<span class="number">0</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                src_r, dst_r = src, dst</span><br><span class="line">                scale = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        src_r, dst_r = src, dst</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define 2x3 or 3x3 matrices and initialize the matrix to identity</span></span><br><span class="line">    <span class="keyword">if</span> warp_mode == cv2.MOTION_HOMOGRAPHY :</span><br><span class="line">        warp_matrix = np.eye(<span class="number">3</span>, <span class="number">3</span>, dtype=np.float32)</span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        warp_matrix = np.eye(<span class="number">2</span>, <span class="number">3</span>, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define termination criteria</span></span><br><span class="line">    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, max_iter, eps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the ECC algorithm. The results are stored in warp_matrix.</span></span><br><span class="line">    (cc, warp_matrix) = cv2.findTransformECC (src_r, dst_r, warp_matrix, warp_mode, criteria, <span class="keyword">None</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> scale <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        warp_matrix[<span class="number">0</span>, <span class="number">2</span>] = warp_matrix[<span class="number">0</span>, <span class="number">2</span>] / scale[<span class="number">0</span>]</span><br><span class="line">        warp_matrix[<span class="number">1</span>, <span class="number">2</span>] = warp_matrix[<span class="number">1</span>, <span class="number">2</span>] / scale[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> align:</span><br><span class="line">        sz = src.shape</span><br><span class="line">        <span class="keyword">if</span> warp_mode == cv2.MOTION_HOMOGRAPHY:</span><br><span class="line">            <span class="comment"># Use warpPerspective for Homography</span></span><br><span class="line">            src_aligned = cv2.warpPerspective(src, warp_matrix, (sz[<span class="number">1</span>],sz[<span class="number">0</span>]), flags=cv2.INTER_LINEAR)</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="comment"># Use warpAffine for Translation, Euclidean and Affine</span></span><br><span class="line">            src_aligned = cv2.warpAffine(src, warp_matrix, (sz[<span class="number">1</span>],sz[<span class="number">0</span>]), flags=cv2.INTER_LINEAR)</span><br><span class="line">        <span class="keyword">return</span> warp_matrix, src_aligned</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> warp_matrix, <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>这里面我添加了一个技巧，由于ECC算法针对的是两幅图，所以图像的尺寸对于算法求解速度的影响很大。因此这里我根据变换矩阵的形式，设计了一种可以根据尺度放缩自动调节的简易算法。效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327182407942.png" alt="image-20200327182407942"></p><p>效果也很好，值得一提的是，ECC算法只需要大约几毫秒的时间，但是由于它的求解效率跟变换的难度相关，所以间隔越久越慢，而对极几何的方法效率比较稳定，不过就很慢了。</p><h2 id="4-其他近似方案"><a href="#4-其他近似方案" class="headerlink" title="4 其他近似方案"></a>4 其他近似方案</h2><h3 id="4-1光流"><a href="#4-1光流" class="headerlink" title="4.1光流"></a>4.1光流</h3><p>上面我介绍的都是近两年关于相机运动的针对性解决方案，那么实际上在有一些算法模型中，如果场景变化不剧烈，并不特别需要用到运动模型。比如基于光流法的多目标跟踪算法，这里众所周知的就是ICCV2015的NOMT[5]算法。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327183710335.png" alt="image-20200327183710335"></p><p>作者用的是一种简化版的快速光流法，那么更形象的可以看今年刚出的一篇论文《Multiple Object Tracking by Flowing and Fusing》，具体我就不说了，就是简单的在Tracktor++框架上加了一个光流预测分支：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327183820956.png" alt="image-20200327183820956"></p><p>可以看到的是，光流也是在捕捉相邻帧中相似的像素信息，这一点跟第二章中提出的两种相机运动模型有点类似，所以不需要显式使用相机运动模型。</p><h3 id="4-2SOT"><a href="#4-2SOT" class="headerlink" title="4.2SOT"></a>4.2SOT</h3><p>而基于SOT的方法，无论是使用传统的相关滤波算法还是使用Siamese类深度学习框架，都会在上一帧目标周围1.5~2.5倍区域搜索下一帧的目标，这里面会显式或者隐式用到特征的比对。只不过不同于上面的像素比对，这里是更加高层的特征比对。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200326/image-20200327184521981.png" alt="image-20200327184521981"></p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Wang G, Wang Y, Zhang H, et al. Exploit the connectivity: Multi-object tracking with trackletnet[C]. in: Proceedings of the 27th ACM International Conference on Multimedia. 2019. 482-490.</p><p>[2] Evangelidis G D, Psarakis E Z. Parametric image alignment using enhanced correlation coefficient maximization[J]. IEEE transactions on pattern analysis and machine intelligence, 2008, 30(10): 1858-1865.</p><p>[3] Bergmann P, Meinhardt T, Leal-Taixe L. Tracking without bells and whistles[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 941-951.</p><p>[4] Choi W. Near-online multi-target tracking with aggregated local flow descriptor[C]. in: Proceedings of the IEEE international conference on computer vision. 2015. 3029-3037.</p><p>[5] Feng W, Hu Z, Wu W, et al. Multi-object tracking with multiple cues and switcher-aware classification[J]. arXiv preprint arXiv:1901.06129, 2019.</p><p>[6] <a href="https://blog.csdn.net/ssw_1990/article/details/53355572">https://blog.csdn.net/ssw_1990/article/details/53355572</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;之前的博客中我介绍了Kalman滤波器，这个算法被广泛用于多目标跟踪任务中的行人运动模型。然而实际场景中存在有很多相机运动，仅仅依赖行人运动模型是不够的。这次我主要介绍下相机运动模型，以对极几何和ECC为主。完整的代码和示例我都放在了&lt;a href=&quot;https://github.com/nightmaredimple/libmot&quot;&gt;github&lt;/a&gt;。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="相机运动" scheme="https://huangpiao.tech/tags/%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8/"/>
    
      <category term="对极几何" scheme="https://huangpiao.tech/tags/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95/"/>
    
      <category term="ECC" scheme="https://huangpiao.tech/tags/ECC/"/>
    
  </entry>
  
  <entry>
    <title>目标检测中的特征冲突与不对齐问题</title>
    <link href="https://huangpiao.tech/2020/03/20/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%86%B2%E7%AA%81%E4%B8%8E%E4%B8%8D%E5%AF%B9%E9%BD%90%E9%97%AE%E9%A2%98/"/>
    <id>https://huangpiao.tech/2020/03/20/目标检测中的特征冲突与不对齐问题/</id>
    <published>2020-03-20T04:10:00.000Z</published>
    <updated>2020-03-20T04:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>昨天看到一篇商汤的刷榜文《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》，里面的每个技巧我们都见过，还有很多依靠大量计算资源的参数搜索和模型集成。不过其中关于回归和分类的冲突勾起了我的回忆，去年整理了一些相关的文章。我准备在简要介绍这片文章的同时，谈谈目标检测（two-stage和one-stage）中特征的冲突和不对齐问题，以及现有的改进方案。</p></blockquote><a id="more"></a><h2 id="1-Two-stage目标检测中的特征-任务冲突问题"><a href="#1-Two-stage目标检测中的特征-任务冲突问题" class="headerlink" title="1 Two-stage目标检测中的特征/任务冲突问题"></a>1 Two-stage目标检测中的特征/任务冲突问题</h2><h3 id="1-1-Two-stage目标检测的流程与原理"><a href="#1-1-Two-stage目标检测的流程与原理" class="headerlink" title="1.1 Two-stage目标检测的流程与原理"></a>1.1 Two-stage目标检测的流程与原理</h3><p>说起两阶段目标检测算法，大家耳熟能详的就是Faster RCNN系列了，目前的大多数两阶段算法也都是在其基础上进行的改进。不过现在新出的很多“N-阶段”的算法把大家搞混了。所以我这里申明一下两阶段的意义，<strong>我们通常说的两阶段是以Faster RCNN算法为基准的，第一阶段是特征提取和候选框提取，主要是RPN网络，第二阶段是对候选框进行进一步筛选、精修和细分类，主要是ROI Pooling/Align等网络。</strong></p><p>在我的上一篇<a href="https://zhuanlan.zhihu.com/p/113443895">博文</a>中提到过两阶段目标检测的关于平移不变性和相等性的矛盾问题，这里我们详细探讨一下。两阶段目标检测中，第一阶段做的事前/背景分类和候选框回归，第二阶段做的是候选框精修回归和细分类。正如之前所讨论的，<strong>分类</strong>任务希望无论目标的位置和形状怎么变化，什么类别的目标就是什么类别，即需要保证<strong>平移和尺度的不变性</strong>。而回归任务，我在上一篇博文中提到了，对于物体位置的回归很大程度可能依赖padding信息，当然这不是这次的讨论重点，<strong>回归</strong>需要保证目标的位置和形状变化反映在特征上，进而回归得到位置，即<strong>平移和尺度的相等性</strong>。这一问题在<strong>行人检索</strong>中更加严重，因为行人检索问题中的识别任务要求<strong>同类目标的不同身份要区分开</strong>，这一点就与目标检测中的分类任务相违背，因为检测中的分类不论什么身份，只要属于同一类别即可。</p><p><img src="https://pic4.zhimg.com/v2-d31266f7cd124a7995c409504cb6d91f_r.jpg" alt="preview" style="zoom:50%"></p><h3 id="1-2-现有的相关解决方案"><a href="#1-2-现有的相关解决方案" class="headerlink" title="1.2 现有的相关解决方案"></a>1.2 现有的相关解决方案</h3><p>在正式介绍相关改进策略之前，我们先提提Cascade RCNN算法[3]，其原理如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320105218022.png" alt="image-20200320105218022"></p><p>要注意的是Iterative bbox方式和Cascade RCNN方式的形式虽然一样，但是不同之处在于前者仅仅是用于测试阶段，可以观察到都是head network都是一样的，而后者各个head network都是训练来的。从形式来看，很明显就是将最后的分类和回归分支级联做了3次。这样做的依据就是：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320110607661.png" alt="image-20200320110607661"></p><p>第一幅图中横坐标是回归前候选框与gt的iou，纵坐标是回归后的iou，可以看到不同的候选框质量对于回归效果也有影响。第二幅图中基于不同iou阈值训练得到的网络对于AP也有影响。再考虑到训练集和测试集内样本分布的不同，作者采用分而治之的策略，分别用{0.5,0.6,0.7}三种IOU阈值级联训练。<a href="https://zhuanlan.zhihu.com/p/55416312">这里</a>提到了各阶段的具体训练方式：</p><p><img src="https://pic1.zhimg.com/v2-fab4d815d2c7ded5c092bf6cd86b0dc4_r.jpg" alt="preview"></p><p>分类和回归都是一个模式，不仅用gt的标签，还用到了上一阶段的结果作为标签，来保证结果的稳定性。最后我们来看看各种方案的对比实验结果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320111601784.png" alt="image-20200320111601784"></p><p>可以看到的是iterative bbox(以不同iou阈值做多次nms)和integral loss(以不同iou阈值并联多个回归和分类过程)都能提升一点点AP，但是Cascade RCNN(以不同iou阈值级联多个回归和分类过程)效果提升最大。</p><p>好了，我们回归正题，Cascade RCNN从样本质量分布mismatch和iou等角度进行了级联的refine操作。那么在IOU-Net[4]则是显式地说明了分类的分数不适合用于NMS的过滤，因为分类置信度高的样本不一定真的好。因此作者<strong>增加了一个样本与gt的iou预测阶段，以此作为NMS的排序依据</strong>。这里实际上就说明了分类和回归的冲突问题。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320112222479.png" alt="image-20200320112222479"></p><p>至于为了提高预测精度的PrROI-pooling,我就不仔细分析其原理了，不是这里的讨论重点：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320112445271.png" alt="image-20200320112445271"></p><p>那么真正意义上把分类和回归问题放在明面上的我觉得是Double-Head RCNN[2],来自于18年COCO检测冠军旷视团队。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320112852328.png" alt="image-20200320112852328"></p><p>我们可以看到，不同于传统的将回归和分类放在最后阶段，利用两个全连接分支来预测，Double-Head直接从ROI Align之后就将两个人任务分开了，尽可能减少二者共享的特征部分。而Double-Head-Ext方案则是让两个分支都能预测类别和位置。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320113336264.png" alt="image-20200320113336264"></p><p>可以看到，四种方案下平衡两个分支损失函数权重后，后两种的效果明显更好。最后我们来看看CVPR2019的Guided Anchoring算法。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320114058749.png" alt="image-20200320114058749"></p><p>这个算法解决的是anchor的设计问题，而<strong>anchor的设计需要解决形状对齐和特征一致性</strong>的问题。其中形状对齐指的是以往anchor的尺寸和长宽比都是预设的固定几个，首先这也是超参数，其次无法适应多样的样本形状，因此该算法以特征图每个点作为中心，先预测anchor的长宽，再用于预测。而特征一致性问题则是一个很巧妙的问题，原因在于，同一层的特征图上每个点的感受野一致，但是预测到的anchor尺寸却不同，那么<strong>基于不同大小的anchor来做的分类任务却基于相同的特征感受野</strong>，这显然是存在问题的。所以作者基于预测得到的anchor长宽，利用deform-conv为每个anchor分配了新的特征区域，其中deform-conv中的offset直接采用预测得到的anchor长宽。</p><h2 id="2-One-stage目标检测中的特征不对齐问题"><a href="#2-One-stage目标检测中的特征不对齐问题" class="headerlink" title="2 One-stage目标检测中的特征不对齐问题"></a>2 One-stage目标检测中的特征不对齐问题</h2><h3 id="2-1-One-stage目标检测中的问题"><a href="#2-1-One-stage目标检测中的问题" class="headerlink" title="2.1 One-stage目标检测中的问题"></a>2.1 One-stage目标检测中的问题</h3><p>One-stage目标检测算法，以YOLO系列、SSD系列、RetinaNet等为经典，下面是YOLOv3的网络流程：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181213163753559.png" alt="img"></p><p>我们可以看到的是单阶段的目标检测算法相当于取消了RPN阶段，所以两阶段目标检测中遇到的问题（分类与回归特征冲突，anchor与特征不对齐），在单阶段目标检测中只会更加严重。不过单阶段目标检测的目标就是提升速度，所以我目前并没有看到对第一个问题的解决方案，而去年对于anchor与特征不对齐的问题有好多解决策略。原因在于两阶段目标检测中ROI Pooling本身有一个利用候选框裁剪特征区域的过程，缓解了这一问题，而单阶段目标检测却没有这一过程。</p><h3 id="2-2-“1-5-stage”解决策略"><a href="#2-2-“1-5-stage”解决策略" class="headerlink" title="2.2 “1.5-stage”解决策略"></a>2.2 “1.5-stage”解决策略</h3><p>CVPR2018有一篇RefineDet算法[9]，这个算法是针对SSD算法的改进，融合了单阶段和两阶段的设计思路，但又不是我们之前所说的RPN+ROIPooling这类框架，所以就叫它“1.5stage”检测框架吧。RefineDet有两个模块，其中上面是ARM，用于调整anchors的位置和大小，下面是ODM，用于目标检测。这个跟Guided Anchoring的设计思路很像，不过比较简陋。除此之外，RefineDet还采用了级联预测的模式，利用中间的TCB模块，其通过Deconv和特征Concat反向级联，类似于FPN的模式。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320123202837.png" alt="image-20200320123202837" style="zoom:67%"></p><p>同样地，相同的团队在AAAI2019的一篇人脸检测算法SRN[6]也用了RefineDet的框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320124527745.png" alt="image-20200320124527745"></p><p>可以看到整体框架很像，但是却有所不同，SRN框架包含有STC+STR+RFE三个模块。其中STC模块作用于浅层网络，用于过滤掉大部分的负样本，STR作用于高层特征，用于粗略调整anchor，类似于RefineDet。而RFE则是在接受各个尺度特征的同时，利用非正方形的卷积核对感受野进行增强（考虑到人脸不一定是正的）。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320125111086.png" alt="image-20200320125111086" style="zoom:67%"></p><p>在ICCV2019中有一篇比较特别的检测算法Reppoints[7]，其出来的时机正好是anchor-free算法大火的时候，其框架比较特别，可以看作是DCN+Refine操作的集成，有人也称其为DCNv3：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320125445379.png" alt="image-20200320125445379"></p><p>这个框架的特别之处在于没有预测框，没有预测中心或者角点，而是预测的目标边缘的九个点。不过我觉得这几个特征点更像是一种解释，而不是出发点。其原理是以特征图上每个点为中心，预测包含该位置的目标的九个边缘点。其方式是通过卷积的方式预测各个点的相对位置(x,y)偏移，以此作为Deform Conv的偏移量对原特征图进行卷积，由此使得<strong>特征与目标区域更加重合</strong>，从而进行第二阶段的预测。可以发现，<strong>Reppoints很像anchor free版的Guided Anchoring，而之前提到的RefineDet和SRN虽然提到了anchor预更新，但是特征并没有校正</strong>。</p><p>WACV2020的一片P&amp;A算法[5]算是对上面的不足做了完善，但是我感觉像是把Guided Anchoring中的Feature Adaption直接搬过来了，为什么这么说呢。因为P&amp;A也是先预测anchor偏移和前景背景分类，然后以此作为deform conv的offsets对特征重提取，再进行目标位置回归和细分类。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320130502255.png" alt="image-20200320130502255"></p><p>同时间出来的AlignDet[8]则是提出了ROIConv：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320131112828.png" alt="image-20200320131112828"></p><p>上图中(a)指的就是RefineDet类的对齐，(b)就是Reppoints一类的对齐，(c)就是Guided Anchoring类的对齐，(d)就是AlignDet类的对齐。AlignDet把基于anchor偏移量的特征对齐称作ROIConv，还分析了具体的偏移校正过程：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320131842437.png" alt="image-20200320131842437"></p><p>可以看到有两次预测过程，作者采用了Cascade的 方式，两次的IOU阈值不同。其实仔细看的话P&amp;A和AlignDet的结构几乎一模一样，看评审怎么看吧，估计也是考虑到这方面因素给挂了。</p><h2 id="3-《1st-Place-Solutions-for-OpenImage2019-Object-Detection-and-Instance-Segmentation》介绍"><a href="#3-《1st-Place-Solutions-for-OpenImage2019-Object-Detection-and-Instance-Segmentation》介绍" class="headerlink" title="3 《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》介绍"></a>3 《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》介绍</h2><p>现在我们来看看商汤在OpenImage2019上的文章，可以当作技术报告来看。我们直接按照论文提到的创新点或者工作来一一说明。</p><ul><li><p><strong>Decoupling Head</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320132308113.png" alt="image-20200320132308113"></p><p>作者出发点是<strong>目标检测框架中分类的回归任务对于特征的要求不同</strong>，而这一点在我上面提到的Double Head RCNN已经提过了。Decoupling Head则是考虑到我们前文提到的anchor和特征不对齐问题，利用传统的ROI Pooling主干预测anchor的粗略位置，然后用deform conv的方式校正分类分支。再在主干上保留原始的回归和分类任务。总而言之可以将其概括为：Double Head RCNN + AlignDet + Faster RCNN</p></li><li><p><strong>Adj-NMS</strong></p><p>这部分作者的描述方案很“有意思”，作者考虑到NMS和soft-NMS的不足，先利用0.5的IOU阈值做了一次NMS，将靠得比较近的候选框过滤掉了，然后再用基于高斯核的soft-NMS做二次过滤。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320133448184.png" alt="image-20200320133448184"></p><p>我们可以根据这个公式来看看，假设分类置信度阈值为0.5，候选框分类置信度为1，那么Soft-NMS阶段要想留下，IOU必须小于0.59，而第一次的NMS已经将IOU&gt;0.5的候选框过滤掉了，所以这个理论上可行。因此我们可以认为作者几乎不怎么考虑特别密集拥挤的场景了。<br><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320134037397.png" alt="image-20200320134037397"></p><p>其效果也有0.174个点的提升。其实如果注意的话，有点像前文介绍Cascade RCNN是所提到了Iterative bbox策略，即做多次NMS。SoftNMS只能通过重新打分捞回原本得分比较低的样本，但是NMS已经将大部分的候选框给过滤掉了，所以我很好奇这是怎么生效的。</p></li><li><p><strong>Model Ensemble</strong></p><p>很多大型比赛的固定策略“Ensemble”，已经不奇怪了。naive ensemble的策略是借鉴的2018年的OpenImage第二名，给定bounding boxes(P)，以及topk个与之IOU较高的候选框，依据验证集的分数来分配各个模型在集成时的权重，这里还分各个目标类别，然后进行加权：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200319/image-20200320134804881.png" alt="image-20200320134804881" style="zoom:50%"></p></li></ul><p>这里作者训练了28个目标检测网络….，利用二叉树的方式进行模型空间搜索。</p><ul><li><p><strong>Data Re-sampling</strong></p><p>确保500个类别的目标中各个类别被选取的概率相等。</p></li><li><p><strong>Decoupling Backbone</strong></p><p>对于第25~28个模型，采取Decouple Head的策略，其中回归分支的权重较小。</p></li><li><p><strong>Elaborate Augmentation</strong></p><p>随机选择一个类别，利用旋转放缩裁剪等方式进行数据增强，这样可以使得一幅图中的类别数变少，缓解数据不平衡问题。</p></li><li><p><strong>Expert Model</strong></p><p>利用专门的网络训练专门的子类别数据集，这里面考虑了正负样本均衡的问题，容易混淆（标注标准不同，表观相似）的样本。</p></li><li><p><strong>AnchorSelecting</strong></p><p>跟YOLO系列一样，利用k-means方法得到18组anchors(6种长宽比，3种尺寸)。</p></li><li><p><strong>Cascade RCNN</strong></p><p>设置了0.5,0.5,0.6,0.7四个阶段的级联检测，这我就搞不懂Adj-NMS干嘛用的了。</p></li><li><p><strong>Weakly Supervised Training</strong></p><p>由于OpenImage数据集中各类别的“长尾分布”很明显，严重不均衡，所以作者增加了一些图像级的标注，结合有监督和WSDDN算法中的弱监督算法联合训练。</p></li><li><p><strong>Relationships Between Categories</strong></p><p>作者通过分析数据集中部分类别目标之间的联系，比如person和guitar等等，类似于条件概率，来修正分类置信度，比如一个有person在旁边的guitar要比没有person的guitar置信度要高。</p></li><li><p><strong>Data Understanding</strong></p><p>作者发现OpenImage数据集中对于特定类别的目标标注有歧义，比如火炬和手电筒，剑和匕首等，所以作者将有歧义的类别细分成了上面说的多类。同时作者也发现有些目标，比如葡萄缺乏个体检测框等，作者就利用葡萄串的实例标注，扩展了很多葡萄框。</p></li></ul><p>最后的分割部分我就不细讲了，就是基于HRNet和Ensemble的方式进行的实验。</p><h2 id="4-说在后面的话"><a href="#4-说在后面的话" class="headerlink" title="4 说在后面的话"></a>4 说在后面的话</h2><p>实际上目标检测任务与多目标跟踪（MOT）也有很多联系，比如MOT数据集中的MOT17Det，又比如新出的基于类检测框架的Tracktor++算法，检测跟踪结合的框架JDE算法等。多目标跟踪领域绝不是一个局限于数据关联的独立领域，应该是个多领域融合的方向。之前基于COCO的预训练模型在MOT17数据集上试了下，在MOT17Det上居然还有0.88AP，然后我基于这个又复现了下Tracktor++，居然也达到了58+MOTA，后面有机会我放github吧。对了，还有个Crowdhuman人体检测的<a href="https://zhuanlan.zhihu.com/p/68677880">算法分享</a>。</p><p>唉，公司又推迟入职时间了，先申请看能不能提前入职吧，不然只能在家减肥看论文做实验了…</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation.</p><p>[2] Rethinking Classification and Localization for Object Detection</p><p>[3] Cascade r-cnn_ Delving into high quality object detection</p><p>[4] Acquisition of Localization Confidence for Accurate Object Detection</p><p>[5] Propose-and-Attend Single Shot Detector</p><p>[6] Selective Refinement Network for Face Detection</p><p>[7] Reppoints_ Point set representation for object detection</p><p>[8] Revisiting feature alignment for one-stage object detection</p><p>[9] Single-Shot Refinement Neural Network for Object Detection</p><p>[10] <a href="https://zhuanlan.zhihu.com/p/63273342">https://zhuanlan.zhihu.com/p/63273342</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;昨天看到一篇商汤的刷榜文《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》，里面的每个技巧我们都见过，还有很多依靠大量计算资源的参数搜索和模型集成。不过其中关于回归和分类的冲突勾起了我的回忆，去年整理了一些相关的文章。我准备在简要介绍这片文章的同时，谈谈目标检测（two-stage和one-stage）中特征的冲突和不对齐问题，以及现有的改进方案。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="特征" scheme="https://huangpiao.tech/tags/%E7%89%B9%E5%BE%81/"/>
    
      <category term="检测" scheme="https://huangpiao.tech/tags/%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>谈谈CNN中的位置和尺度问题</title>
    <link href="https://huangpiao.tech/2020/03/16/%E8%B0%88%E8%B0%88CNN%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%92%8C%E5%B0%BA%E5%BA%A6%E9%97%AE%E9%A2%98/"/>
    <id>https://huangpiao.tech/2020/03/16/谈谈CNN中的位置和尺度问题/</id>
    <published>2020-03-15T17:10:00.000Z</published>
    <updated>2020-03-15T17:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:21 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>前段时间看到了几篇有意思的文章，也参考了一些相关的讨论，这里想对CNN中的平移和尺度的不变性和相等性，以及CNN对于目标相对和绝对位置、深度的预测原理进行探讨。这些内容对于一些特定任务很重要，比如目标检测、目标分割、深度估计、分类/识别以及单目标跟踪中的置信图预测等。</p></blockquote><a id="more"></a><h2 id="1-CNN是否存在平移和尺度的不变性和相等性"><a href="#1-CNN是否存在平移和尺度的不变性和相等性" class="headerlink" title="1 CNN是否存在平移和尺度的不变性和相等性"></a>1 CNN是否存在平移和尺度的不变性和相等性</h2><h3 id="1-1-不变性和相等性的定义"><a href="#1-1-不变性和相等性的定义" class="headerlink" title="1.1 不变性和相等性的定义"></a>1.1 不变性和相等性的定义</h3><p>​ 在介绍卷积神经网络（CNN）之前，我们对于不变性和相等性的理解可能来自于传统图像处理算法中的，平移、旋转、光照和尺度等不变性，比如HOG梯度方向直方图，由于<code>cell</code>的存在，其对于平移、旋转有一定的不变性，另外由于对图像局部对比度归一化的操作，使其对于光照也有着一定的不变性。又比如说SIFT特征提取，其对于以上四点都有着不变性，其中由于尺度金字塔，使得对尺度也有不变性。这里我们对于不变性的理解就是，同一对象发生平移、旋转、光照变化、尺度变换甚至形变等，其属性应该一致。下面我们给出具体的不变性和相等性的定义。</p><p>​ 其中<strong>不变性（invariance）</strong>的定义正如上文所说，因此其形式为：</p><p>$$ F\left( x \right) = F\left[ {transform\left( x \right)} \right] $$<br>​ 而对于<strong>相等性（equivalence）</strong>，顾名思义，就是对输入进行变换之后，输出也发生相应的变换：</p><p>$$ transform\left[ {F\left( x \right)} \right] = F\left[ {transform\left( x \right)} \right] $$<br>​ 不过如果我们只考虑输出对于输入不变性和相等性的情况，则会难以理解，因为我们更多地是想象着特征层面的映射，比如：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315190824050.png" alt="image-20200315190824050"></p><p>​ 那么特征层面对于输出的影响我们可能考虑得比较少，但是却实质存在，比如目标在图像中的平移和尺度等变换，在目标检测任务中，必须要使得网络具有相关的变换相等性，由此捕捉目标的位置和形状变化。而在图像分类、目标识别、行人重识别等任务中，又必须使得网络具有相关变换的不变性。这两点也是目标检测和行人检索领域中一个经典的矛盾问题，目前好像还没有特别好的解决，更多地是分阶段地执行不同的任务，防止特征共用。比如：经典的两阶段目标检测任务中，第一阶段是粗检测和前景背景分类，第二阶段是精修和具体类别分类，有一定的偏重。行人检索算法则大多是先检测后识别的策略。当然除了不变性和相等性的问题，还存在类内差异的问题，比如不同的人对于检测而言都是行人类别，对于识别而言则是不同的人，这对于特征提取也存在挑战。</p><h3 id="1-2-CNN网络的执行过程"><a href="#1-2-CNN网络的执行过程" class="headerlink" title="1.2 CNN网络的执行过程"></a>1.2 CNN网络的执行过程</h3><p>​ 我记得我几年前第一次接触到深度学习的时候，对于全连接和CNN的局部连接形式，都有平移、尺度不变性的说法。对于全连接网络，由于下一层的每个节点都会与上一层进行连接：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315191912522.png" alt="image-20200315191912522" style="zoom:33%"></p><p>​ 因此无论输入发生了平移、尺度等什么变换，只要其属性没变，全连接网络更能捕捉其中的不变性。而对于卷积神经网络，我们都知道两个特点：局部连接和权值共享。</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315192158684.png" alt="image-20200315192158684" style="zoom:67%"></p><p>​ 对于局部连接，因为全连接参数太多，容易造成过拟合，并且图像领域更多地关注局部细节信息，所以局部连接方式有效。至于权值共享，也有减少参数的作用，很像图像处理中的滤波器。我们早期对于其不变性的理解更多是遵循一个宏观的感受，即由于卷积核的移位滤波，上一层的特征到下一层的特征相对位置<strong>宏观不变</strong>，直到最后输出，类似于全连接的效果，从而获得不变性。</p><h3 id="1-3CNN网络潜在问题与改进"><a href="#1-3CNN网络潜在问题与改进" class="headerlink" title="1.3CNN网络潜在问题与改进"></a>1.3CNN网络潜在问题与改进</h3><p>​ 正因为我刚说的<strong>宏观不变</strong>，使得输入在经过多次卷积、池化之后，微观/细节的变化累积放大，从而失去了这种不变性，接下来我会结合两篇论文进行介绍。</p><p>​ 第一个是为了解决CNN平移不变性对抗性攻击的一篇ICML2019论文<a href="https://arxiv.org/pdf/1904.11486.pdf?fbclid=IwAR1VRPIrctulC6EhTAhKFjIFrlx_JjKR09JnESzLELUnlTL40iOT5tYwotA">《Making Convolutional Networks Shift-Invariant Again》</a>。这篇文章主要讨论了CNN网络中的降采样对于平移不变性的影响：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315211217914.png" alt="image-20200315211217914"></p><p>​ 上图是对于一个窗户分别采用从0~7的平移量，其特征图与不平移的差异，可以明显看到，特征图出现了波动。相应地，上半部分是利用pix2pix生成的图像，我们可以看到随着平移量的增大，窗户中的竖直线从两根变成了一根。这一点就表明传统的CNN网络并不具有平移不变性。</p><p>​ 首先，作者做了这样一个小实验，即采用maxpooling对一维向量[0011001100]进行池化，由此得到向量[01010]：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315211851283.png" alt="image-20200315211851283" style="zoom:33%"></p><p>​ 接着，如果将输入向右平移一个单位，则得到向量[111111]：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315212005863.png" alt="image-20200315212005863" style="zoom:33%"></p><p>​ 很明显，平移相等性和不变性都丢失了。接着作者做了进一步实验，利用余弦距离来刻画平移相等性，采用VGG网络对Cifar数据集进行试验：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315212412486.png" alt="image-20200315212412486"></p><p>​ 其中颜色越深说明差异越大，可以看到每次maxpooling都增加了特征的差异性，不过作者将max和pool操作分开了，为了区分取最大值和降采样的影响：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315212820790.png" alt="image-20200315212820790"></p><p>​ 很明显，降采样对于平移相等性的影响更大，而CNN中涉及到降采样的操作有：池化（maxpooling和average pooling）和带步长的卷积（strided convolution）。对此作者提出了一种名为<strong>Anti_aliasing</strong>方法，中文叫做抗锯齿处理。传统信号处理领域中对于抗锯齿的技术，一般要么增大采样频率，但由于图像处理任务一般都需要降采样，这一点不适合。要么采用图像模糊（bluring）技术，根据Nyquist采样理论，是给定采样频率，通过降低原始信号的频率来使得信号能够被重构出来，如下图所示。对模糊化处理和未处理的原图像进行下采样，得到图中底部的两张图，模糊化处理的原图像下采样的图像还能看出一些轮廓，而未处理的原图像下采样的图像就显得更加混乱。</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315213928286.png" alt="image-20200315213928286" style="zoom:50%"></p><p>​ 作者就是采用了模糊的方式，提出了三种不同的<strong>blur kernel</strong>：</p><ul><li>Rectangle-2：[1, 1]，类似于均值池化和最近邻插值；</li><li>Triangle-2：[1, 2, 1]，类似于双线性插值；</li><li>Binomial-5：[1, 4, 6, 4, 1]，这个被用在拉普拉斯金字塔中。</li></ul><p>​ 每个核都需要归一化，即除以核内所有元素之和，然后加入到降采样过程，即在降采样之前使用blur kernel进行卷积滤波：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315214459901.png" alt="image-20200315214459901"></p><p>​ 可以看到其效果很不错：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315214729743.png" alt="image-20200315214729743"></p><p>​ <img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315215357717.png" alt="image-20200315215357717"></p><p>​ 代码和模型见：<a href="https://richzhang.github.io/antialiased-cnns/或者https://github.com/adobe/antialiased-cnns">https://richzhang.github.io/antialiased-cnns/或者https://github.com/adobe/antialiased-cnns</a></p><p>​ 第二篇是同年发表在JMLR的一篇论文<a href="http://www.jmlr.org/papers/volume20/19-519/19-519.pdf">《Why do deep convolutional networks generalize so poorly to small image transformations?》</a>。作者首先给出了几组示例，分别表示了平移、尺度以及轻微图像差异对网络预测分类置信度的影响：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315215859331.png" alt="image-20200315215859331" style="zoom:50%"></p><p>​ 作者认为CNN忽视了采样定理，这一点之前Simoncelli等人已经在论文<a href="https://link.zhihu.com/?target=http%3A//persci.mit.edu/pub_pdfs/simoncelli_shift.pdf">Shiftable multiscale transforms</a>中验证了二次采样在平移不变性上的失败，他们在文中说：</p><blockquote><p>我们不能简单地把系统中的平移不变性寄希望于卷积和二次采样，输入信号的平移不意味着变换系数的简单平移，除非这个平移是每个二次采样因子的倍数。</p></blockquote><p>​ 我们现有的网络框架中，越深的网络，降采样次数越多，因此出现的问题更多。紧接着，作者提出了几点论述：</p><ul><li><p>如果$r\left( x \right)$是经过卷积操作且满足平移不变性的特征，那么全局池化$\sum\nolimits_x {r\left( x \right)} $操作也满足平移不变性；</p></li><li><p>对于特征提取器$r\left( x \right)$和降采样因子$s$，如果输入的平移都可以在输出上线性插值反映出来：<br>$$ r\left( x \right) = \sum\limits_i {B\left( {x - {x_i}} \right)} r\left( {{x_i}} \right) $$<br>由香农-奈奎斯特定理知，$r\left( x \right)$满足可移位性，要保证采样频率至少为最高信号频率的2倍。</p></li></ul><p>​ 接下来，作者对这些问题做了一些改进尝试：</p><ul><li><strong>抗锯齿</strong>，这个就是我们刚刚介绍的方法；</li><li><strong>数据增强</strong>，当前在很多图像任务中，我们基本都会采用随机裁剪、多尺度、颜色抖动等等数据增强手段，的确也让网络学习到了部分不变性；</li><li><strong>减少降采样</strong>，也就是说只依赖卷积对于输入尺度的减小来变化，这一点只对小图像适用，主要是因为计算代价太高。</li></ul><h2 id="2-CNN对于位置和深度信息的预测"><a href="#2-CNN对于位置和深度信息的预测" class="headerlink" title="2 CNN对于位置和深度信息的预测"></a>2 CNN对于位置和深度信息的预测</h2><h3 id="2-1CNN如何获取目标的位置信息"><a href="#2-1CNN如何获取目标的位置信息" class="headerlink" title="2.1CNN如何获取目标的位置信息"></a>2.1CNN如何获取目标的位置信息</h3><p>​ 最早接触神经网络和深度学习相关任务时，我的感觉就是这类算法本质是做的分类任务，比如图像分割是对前景背景的分类和具体类别分类，识别任务就是类间类内的区分任务。其中图像分割任务就利用了CNN中的部分相等性，那么对于目标检测任务中的目标位置回归是怎么获取的呢？我们可以知道的是同样是对目标位置的搜索，在单目标跟踪任务中，存在有置信图：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315225746938.png" alt="image-20200315225746938"></p><p>​ 但是置信图本质上是对区域进行搜索得到的，因此可以近似为对多个子区域的识别过程，所以单目标跟踪中的目标定位也可以用分类的理解，但是目标检测则不好用分类来理解了。</p><p>​ 接下来我们思考一个问题，我们所设计的网络究竟包含了哪些信息？图像特征、网络结构（卷积核大小、padding）。从上一章我们可以知道，网络可以学习到一定的相等性：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315230524738.png" alt="image-20200315230524738"></p><p>​ 因此，通过不断地训练，网络在最后的特征输出中是可以通过对应的特征数值和区域大小，结合一定的倍数（降采样大小）估计目标尺度的。但是对于目标的位置，我们人眼对于目标位置的判定是通过坐标系的，即目标距离图像的边缘距离，但是网络是如何了解这一信息的呢？<a href="https://arxiv.org/pdf/2001.08248">《How much Position Information Do Convolutional Neural Networks Encode?》</a>这篇文章做出了回答。</p><p>​ 作者首先做了一组实验：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315231658821.png" alt="image-20200315231658821"></p><p>​ 对于不同的输入图像，采用多种mask扰动，H、V、G分别代表水平、竖直和高斯等分布，用这种方式生成多种groundtruth，对于这点我们可能对单目标跟踪中以目标中心为均值的高斯分布比较熟悉。结果发现：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315231925160.png" alt="image-20200315231925160"></p><p>​ GT是三种分布生成的groundtruth，PosENet是作者提出的网络，没有padding。我们可以看到PosENet并没有得到位置分布信息，而是保持了相等性。而存在padding的VGG和ResNet则都预测出了位置分布。由此可见padding对于位置的作用，也对上一章的内容作了补充，padding对于平移不变性和相等性也有影响。</p><p>​ 不过这里我们可能不好理解，我做了个小测试，不过不一定是这么做的，仅仅方便我理解：</p><p>$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&1&2&3\\ 0&4&5&6\\ 0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} {12}&{21}\\ {12}&{21} \end{array}} \right]\\ \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&0&0&0\\ 1&2&3&0\\ 4&5&6&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 6&5\\ {21}&{16} \end{array}} \right] \end{array} $$<br>​ 上面是两个不同位置“目标”的卷积结果，可以看到，从输出结果得不到什么位置反映，如果加入padding：</p><p>$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&0&1&2&3&0\\ 0&0&4&5&6&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 1&3&6&5\\ 5&{12}&{21}&{16}\\ 5&{12}&{21}&{16}\\ 4&9&{15}&{11} \end{array}} \right]\\ \left[ {\begin{array}{*{20}{c}} 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&1&2&3&0&0\\ 0&4&5&6&0&0\\ 0&0&0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 3&6&5&3\\ {12}&{21}&{16}&9\\ {12}&{21}&{16}&9 \end{array}} \right] \end{array} $$<br>​ 首先我们可以知道的是，加入了zero-padding之后，目标边缘相对中心的输出更小了，其次目标距离图像边缘距离越远，其特征映射中出现0的次数越多。所以我猜网络就是在训练过程中让padding和这个相对的关系对应上了，如果没有padding，目标距离边缘越远，同样出现0的次数也会越多，但问题在于无法跟padding造成的边缘数值小，中心数值大的特殊分布相比。当然，以上仅仅是我个人的理解，为了帮助我加深印象罢了。也有人说加入了padding影响了CNN的平移相等性，从而使得CNN学到了位置信息，但这个不大好解释。</p><p>​ 不过有关padding的问题，在CVPR2019的一片单目标跟踪算法SiamRPN++中也做了探讨。其出发点在于为何Siamese网络无法扩展为大型网络，其原因就在于padding影响了平移相等性，从而让目标位置的分布发生了偏移。所以作者通过对目标中心的随机扰动，使得网络克服自身的偏移：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200315235951915.png" alt="image-20200315235951915"></p><p>​ 也有一些研究探索了如何让CNN结合绝对位置信息，比较出名的应该是当前很热门的SOLO单阶段实力分割算法。SOLO的出发点很简单，我们都知道语义分割只需要分割出不同类别的目标，而实力分割对于同一类别的个体还需要区分。但是显而易见，同一类别的目标只要位置和形状不同则可以区分。因此SOLO就是将位置和形状（用尺寸简化）信息结合进来。具体而言，就是将输入系统的图像统一划分为S x S的网格，如果对象的中心落入网格单元，那么这个网格单元就负责预测语义类别以及分割该对象实例。</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316001228867.png" alt="image-20200316001228867"></p><p>​ 特别地，SOLO算法中采用CoordConv策略（代码：<a href="https://github.com/uber-research/coordconv），该算法将每个特征区域的坐标信息结合进来，从而让网络显示地学会记忆特征的绝对位置信息。SOLO通过这个策略提升了3.6AP，并且论证只需要一层提供该信息即可达到稳定的提升。">https://github.com/uber-research/coordconv），该算法将每个特征区域的坐标信息结合进来，从而让网络显示地学会记忆特征的绝对位置信息。SOLO通过这个策略提升了3.6AP，并且论证只需要一层提供该信息即可达到稳定的提升。</a></p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316001545817.png" alt="image-20200316001545817"></p><h3 id="2-2CNN如何预测目标的深度信息"><a href="#2-2CNN如何预测目标的深度信息" class="headerlink" title="2.2CNN如何预测目标的深度信息"></a>2.2CNN如何预测目标的深度信息</h3><p>​ 深度估计也是一个类似的问题，不同的是，图像中并没有包含深度信息，但是网络是如何获取深度信息的呢。<a href="https://arxiv.org/pdf/1905.07005">How Do Neural Networks See Depth in Single Images?</a>这篇文章给出了回答，关于这篇文章NaiyanWang老师已经在<a href="https://zhuanlan.zhihu.com/p/95758284">博客</a>里讨论过，我这里也就再整理下。</p><p>​ <img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316004609899.png" alt="image-20200316004609899"></p><p>​ 我们可以看到，物体的绝对深度与相机位姿有着很大关系，那么CNN如何学习到这种需要几何先验的绝对信息的呢？作者做了这样一个实验：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316004802285.png" alt="image-20200316004802285"></p><p>​ 上图中作者做了三组实验：同时变化目标位置和尺寸、只变化位置以及只变化尺寸，我们从上面的定性结果好像看不出什么问题，下面是定量的结果：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316005049918.png" alt="image-20200316005049918" style="zoom:50%"></p><p>​ 可以发现，尺度对于深度信息的预测没有什么影响，也就是说CNN网络是通过目标纵坐标来估计深度的，所以说网络实际上是在过拟合训练集，从中学习到一些固定场景下的深度和相对位置的对应关系。</p><p>​ 作者又讨论了相机运动对于深度预测的影响，既然深度与目标纵坐标有关系，那么pitch角的影响应该很大：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316005800971.png" alt="image-20200316005800971"></p><p>​ 可以发现，pitch的确影响比较大，相对的roll的影响就比较小了：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316005907181.png" alt="image-20200316005907181" style="zoom:67%"></p><p>​ 最后作者还讨论了颜色和纹理对深度估计的影响：</p><p><img src="C:\Users\Hasee\Desktop\blogdata\20200316\image-20200316010031872.png" alt="image-20200316010031872"></p><p>​ 可以发现，仅仅是改变目标的颜色，深度估计的效果也会下降，可将CNN网络在训练时有多“偷懒”，不知道如果将上述实验变成数据增强的手段的话会让深度估计网络失效还是变强。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://zhuanlan.zhihu.com/p/99766566">https://zhuanlan.zhihu.com/p/99766566</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/95758284">https://zhuanlan.zhihu.com/p/95758284</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/38024868">https://zhuanlan.zhihu.com/p/38024868</a></p><p>[4] Zhang R. Making Convolutional Networks Shift-Invariant Again[C]//International Conference on Machine Learning. 2019: 7324-7334.</p><p>[5] Dijk T, Croon G. How Do Neural Networks See Depth in Single Images?[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 2183-2191.</p><p>[6]Islam M A, Jia S, Bruce N D B. How much Position Information Do Convolutional Neural Networks Encode?[C]//International Conference on Learning Representations. 2019.</p><p>[7]Wang X, Kong T, Shen C, et al. SOLO: Segmenting Objects by Locations[J]. arXiv preprint arXiv:1912.04488, 2019.</p><p>[8]Liu R, Lehman J, Molino P, et al. An intriguing failing of convolutional neural networks and the coordconv solution[C]//Advances in Neural Information Processing Systems. 2018: 9605-9616.</p><p>[9]Novotny D, Albanie S, Larlus D, et al. Semi-convolutional operators for instance segmentation[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 86-102.</p><p>[10]Li B, Wu W, Wang Q, et al. Siamrpn++: Evolution of siamese visual tracking with very deep networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 4282-4291.</p><p>[11] Azulay, Aharon, and Yair Weiss. “Why do deep convolutional networks generalize so poorly to small image transformations?.” <em>Journal of Machine Learning Research</em> 20.184 (2019): 1-25.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:21 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;前段时间看到了几篇有意思的文章，也参考了一些相关的讨论，这里想对CNN中的平移和尺度的不变性和相等性，以及CNN对于目标相对和绝对位置、深度的预测原理进行探讨。这些内容对于一些特定任务很重要，比如目标检测、目标分割、深度估计、分类/识别以及单目标跟踪中的置信图预测等。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度" scheme="https://huangpiao.tech/tags/%E6%B7%B1%E5%BA%A6/"/>
    
      <category term="平移" scheme="https://huangpiao.tech/tags/%E5%B9%B3%E7%A7%BB/"/>
    
      <category term="尺度" scheme="https://huangpiao.tech/tags/%E5%B0%BA%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>基于深度学习的单目深度估计综述</title>
    <link href="https://huangpiao.tech/2020/03/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8D%95%E7%9B%AE%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1%E7%BB%BC%E8%BF%B0/"/>
    <id>https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/</id>
    <published>2020-03-08T09:30:00.000Z</published>
    <updated>2020-03-08T09:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。</p></blockquote><a id="more"></a><h2 id="1任务介绍"><a href="#1任务介绍" class="headerlink" title="1任务介绍"></a>1任务介绍</h2><p>深度估计是计算机视觉领域的一个基础性问题，其可以应用在机器人导航、增强现实、三维重建、自动驾驶等领域。而目前大部分深度估计都是基于二维RGB图像到RBG-D图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的Shape from X方法，还有结合SFM(Structure from motion)和SLAM(Simultaneous Localization And Mapping)等方式预测相机位姿的算法。其中虽然有很多设备可以直接获取深度，但是设备造价昂贵。也可以利用双目进行深度估计，但是由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度也较高，尤其是对于低纹理场景的匹配效果不好。而单目深度估计则相对成本更低，更容易普及。</p><p>那么对于单目深度估计，顾名思义，就是利用一张或者唯一视角下的RGB图像，估计图像中每个像素相对拍摄源的距离。对于人眼来说，由于存在大量的先验知识，所以可以从一只眼睛所获取的图像信息中提取出大量深度信息。那么单目深度估计不仅需要从二维图像中学会客观的深度信息，而且需要提取一些经验信息，后者则对于数据集中相机和场景会比较敏感。</p><p>通过阅读文献，可以将基于深度学习的单目深度估计算法大致分为以下几类：</p><ul><li><strong>监督算法</strong></li></ul><p>顾名思义，直接以2维图像作为输入，以深度图为输出进行训练：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165941244.png" alt="image-20200308165941244"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165953514.png" alt="image-20200308165953514"></p><p>上面给的例子是KITTI数据集中的一组例子，不过深度图可能看的不是很明显，我重新将深度图涂色之后：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170010902.png" alt="image-20200308170010902"></p><ul><li><strong>无监督算法</strong></li></ul><p>由于深度数据的获取难度较高，所以目前有大量算法都是基于无监督模型的。即仅仅使用两个摄像机采集的双目图像数据进行联合训练。其中双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或是将双目图像中各个像素点的对应问题看作是立体匹配问题进行训练。左视图-右视图示例：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170123899.png" alt="image-20200308170123899"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170130969.png" alt="image-20200308170130969"></p><p>视差，以我们人眼为例，两只眼睛看到的图像分别位于不同的坐标系。将手指从较远地方慢慢移动到眼前，会发现，手指在左眼的坐标系中越来越靠右，而在右眼坐标系中越来越靠左，这种差异性就是视差。与此同时，可以说明，视差与深度成反比。除此之外，由于摄像机参数也比较容易获取，所以也可以以相机位姿作为标签进行训练。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170148431.png" alt="image-20200308170148431"></p><ul><li><strong>Structure from motion/基于视频的深度估计</strong></li></ul><p>这一部分中既包含了单帧视频的单目深度估计，也包含了多帧间视频帧的像素的立体匹配，从而近似获取多视角图像，对相机位姿进行估计。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170229040.png" alt="image-20200308170229040"></p><h2 id="2-数据集介绍"><a href="#2-数据集介绍" class="headerlink" title="2 数据集介绍"></a>2 数据集介绍</h2><h3 id="2-1-KITTI"><a href="#2-1-KITTI" class="headerlink" title="2.1 KITTI"></a>2.1 KITTI</h3><p>KITTI是一个多任务属性的数据集，其中原始数据采集平台装配有2个灰度摄像机，2个彩色摄像机，一个Velodyne 64线3D激光雷达，4个光学镜头，以及1个GPS导航系统。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170322182.png" alt="image-20200308170322182"></p><p>其中包含有200+G的原始数据，而有关户外场景的有175G数据。对于这些数据，所标注的任务包含：立体图像匹配、光流、场景流、深度估计（单目或者基于3D点云/激光数据的深度估计）、视觉测距、目标检测（2D/3D物体检测、俯视图物体检测）、目标跟踪、道路/车道线检测、目标分割等。</p><p>链接：<a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">http://www.cvlibs.net/datasets/kitti/eval_object.php</a></p><h3 id="2-2vKITTI"><a href="#2-2vKITTI" class="headerlink" title="2.2vKITTI"></a>2.2vKITTI</h3><p>从名字可以看出这个数据集跟KITTI有关联，其对应KITTI的原始数据和各类任务，创建了全新的虚拟图像，当然，并不是所有原始数据都能对应得上。这里的“虚拟”指的是：左右摄像头15°和30°偏转画面、清晨、晴天、多云、雾天、下雨等基于原图的渲染图像。共计14G原始RGB图像，对应的目标检测、目标跟踪、目标分割标注都存在。这一数据集的意义在于可以缓解深度信息对于光线的敏感问题。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172253721.png" alt="image-20200308172253721"></p><p>链接：<a href="http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds">http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds</a></p><h3 id="2-3Cityscapes"><a href="#2-3Cityscapes" class="headerlink" title="2.3Cityscapes"></a>2.3Cityscapes</h3><p>Cityscapes的数据取自德国的50多个城市的户外场景，其中数据包含有左右视角图像、视差深度图、相机校准、车辆测距、行人标定、目标分割等，同时也包含有类似于vKITTI的虚拟渲染场景图像。其中简单的左视角图像、相机标定、目标分割等数据需要利用学生账号注册获取，其他数据需要联系管理员获取。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172519943.png" alt="image-20200308172519943"></p><p>链接：<a href="https://www.cityscapes-dataset.com/downloads/">https://www.cityscapes-dataset.com/downloads/</a></p><h3 id="2-4NYU-Depth-V2"><a href="#2-4NYU-Depth-V2" class="headerlink" title="2.4NYU Depth V2"></a>2.4NYU Depth V2</h3><p>NYU Depth V2数据集中包含有428G室内场景数据，同时包含有目标分割标注、深度标注。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172545276.png" alt="image-20200308172545276"></p><p>链接：<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</a></p><h3 id="2-5-ScanNet"><a href="#2-5-ScanNet" class="headerlink" title="2.5 ScanNet"></a><strong>2.5 ScanNet</strong></h3><p>ScanNet中包含有约1500个视频序列的RGB-D数据，主要用于三维重建。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172617329.png" alt="image-20200308172617329"></p><p>链接：<a href="http://www.scan-net.org/#code-and-data">http://www.scan-net.org/#code-and-data</a></p><h3 id="2-6Make3D"><a href="#2-6Make3D" class="headerlink" title="2.6Make3D"></a>2.6Make3D</h3><p>Make3d数据集包含约1000张室外场景图片，50张室内场景，7000张合成物体。其中包含有激光-2D图像，立体图像、深度数据等。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172710149.png" alt="image-20200308172710149"></p><h2 id="3-数据处理"><a href="#3-数据处理" class="headerlink" title="3 数据处理"></a>3 数据处理</h2><h3 id="3-1数据组成"><a href="#3-1数据组成" class="headerlink" title="3.1数据组成"></a>3.1数据组成</h3><p>以KITTI数据集为例，它没有给出深度相关的标注信息。其数据组成包括多个场景下的原始图像数据（gray、color），实例分割、目标跟踪、2d/3d目标检测等任务信息。为了方便我后续使用，我将数据结构解析如下，由于知乎不支持表格，所以我就直接截图了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170434918.png" alt="image-20200308170434918"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171923296.png" alt="image-20200308171923296"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171722320.png" alt="image-20200308171722320"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171840905.png" alt="image-20200308171840905"></p><p>这一部分内容中，对于一个点云数据P:[X,Y,Z,1]^T，其中Z就是深度信息，将其转化为相机左视图的像素点坐标Q:[u,v,1]^T：</p><script type="math/tex;mode=display">ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times P</script><p>对于GPS/imu中的点G:[X,Y,Z]^T，将其转化为相机左视图的像素点坐标Q：</p><script type="math/tex;mode=display">ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times {\left( {\left. R \right|T} \right)_{imu\_to\_velo}} \times G</script><p>其中最需要注意的是第一个公式，用于深度的信息的提取，以及P_rect_xx，其前三列数据为修正后的相机内参。</p><h3 id="3-2-数据处理"><a href="#3-2-数据处理" class="headerlink" title="3.2 数据处理"></a>3.2 数据处理</h3><p>有关相机/像素/世界坐标系的知识我就不介绍了，对于相对位姿，如果将每个视频场景的第一帧的位姿视为初始位姿，那么每一帧的相对位姿计算如下：</p><p>$$ \begin{array}{l} imu2cam = \left[ {\begin{array}{*{20}{c}} {R\_rect\_00}&0\\ 0&1 \end{array}} \right] \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{imu2velo}}\\ scale = \cos \left( {latitude} \right)\\ t = \left[ {\begin{array}{*{20}{c}} {{t_x}}\\ {{t_y}}\\ {{t_z}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {scale \times longtitude \times \pi /180}\\ {scale \times er \times \log \left[ {\tan \left( {\frac{{90 + latitude}}{{360}}\pi } \right)} \right]}\\ {altitude} \end{array}} \right]\\ roll,yaw,pitch \Rightarrow R = {R_z}{R_y}{R_x}\\ \Rightarrow T = \left[ {\begin{array}{*{20}{c}} R&t\\ 0&1 \end{array}} \right]\\ \Rightarrow {T_{t \to s}} = imu2ca{m_t} \times T_s^{ - 1} \times {T_t} \times imu2cam_t^{ - 1} \end{array} $$<br>由于深度信息的转换需要用到相机内参，所以对于图像的缩放需要先处理，假如图像大小的放缩尺度为[zoom_x,zoom__y]，那么相机内参的变化如下：</p><p>$$ \begin{array}{l} \left\{ \begin{array}{l} P\_rect\_00[0,:] * = zoom\_x\\ P\_rect\_00[1,:] * = zoom\_ \end{array} \right.y\\ \Rightarrow K = P\_rect\_00[:,:3] \end{array} $$<br>根据世界坐标系的转换：</p><p>$$ P\_velo2im = P\_rect\_00 \times R\_rect\_00 \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} $$<br>由于要求点云数据的反射强度为1，所以需要先将点云数据的反射强度置为1：</p><p>$$ \begin{array}{l} Z\left[ {\begin{array}{*{20}{c}} x\\ y\\ 1 \end{array}} \right] = {\left( {P\_velo2im \times \left[ {\begin{array}{*{20}{c}} {{X_v}}\\ {{Y_v}}\\ {{Z_v}}\\ 1 \end{array}} \right]} \right)^T} = \left[ {\begin{array}{*{20}{c}} {X'}\\ {Y'}\\ Z \end{array}} \right]\\ \Rightarrow x = X'/Z,y = Y'/ZZ = Z \end{array} $$<br>最后我们只需要保留满足图像边界约束的点的深度信息，如果映射得到的点坐标相同，则只保留深度更小的。</p><p>那么对于网络训练过程中的数据增强，则可以进行多种变换，下面列出几种基础的：</p><ul><li><p>随机水平翻转，所以需要改变相机内参的水平平移量cx=w-cx；</p></li><li><p>随机尺度变换并剪切至固定大小：<br>$$ \left[ {\begin{array}{*{20}{c}} {{f_x}}&{{c_x}}\\ {{f_y}}&{{c_y}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{f_x} \times scal{e_x}}&{{c_x} \times scal{e_x} + offse{t_x}}\\ {{f_y} \times scal{e_y}}&{{c_y} \times scal{e_y} + offse{t_y}} \end{array}} \right] $$</p></li></ul><h3 id="3-3评价指标"><a href="#3-3评价指标" class="headerlink" title="3.3评价指标"></a>3.3评价指标</h3><p>KITTI数据集在考虑深度估计信息误差时，所以判定的时候只取0.40810811H ~ 0.99189189H，0.03594771W ~ 0.9640522229W部分图像区域，当然也经常会只取50m或者80m范围内的深度信息。为了让预测深度和真实深度的数量级范围一致，一般会用二者深度的中位数作为尺度，对预测深度信息进行尺度放缩。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308182452471.png" alt="image-20200308182452471"></p><h2 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4 相关工作"></a>4 相关工作</h2><h3 id="4-1基于单目视觉的深度估计"><a href="#4-1基于单目视觉的深度估计" class="headerlink" title="4.1基于单目视觉的深度估计"></a>4.1基于单目视觉的深度估计</h3><ul><li><p><strong>传统编解码结构</strong></p><p>深度估计任务是从二维图像到二维深度图像的预测，因此整个过程是一个自编码过程，包含编码和解码，通俗点就是下采样和上采样。这类结构主要有FCN框架和U-net框架，二者的下采样过程都是利用了卷积和池化，而上采样利用了逆卷积/转置卷积(Deconvolution)和upsample。</p></li><li><p><strong>深度回归网络</strong></p><p>早期的单目深度估计网络框架基本上都是直接利用了上面所提到的两个基础框架进行了预测，为了让这类框架更好的应用于深度估计问题，一般都从以下几个方面着手：更好的backbone模型、多尺度、更深的网络。</p><p>以3DV 2016中《Deeper Depth Prediction with Fully Convolutional Residual Networks》一文为例，其提出了FCRN网络框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173754057.png" alt="image-20200308173754057"></p><p>其网络框架主体是基于Resnet-50，在上采样过程中采用了独特的方式，将逆卷积用up-pooing+conv的方式替代了，将Resnet中的project模块进行了一定的改进。</p><p>其中上采样过程中将特征图利用0元素进行填充，然后利用一类特殊的卷积核进行特征提取，这一过程可以先卷积，然后错位相连得到，原理如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173806167.png" alt="image-20200308173806167"></p><p>其中可以发现卷积核并非是正方形，而是矩形，不过过程其实是一样的。而projection部分，即resnet中原图先经过1×1卷积再与特征图相连的部分，变为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173814812.png" alt="image-20200308173814812"></p><p>具体细节我就不在多讲了，其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173833858.png" alt="image-20200308173833858"></p><p>其代码链接为：<a href="https://github.com/iro-cp/FCRN-DepthPrediction，基于Tensorflow和matconvnet。">https://github.com/iro-cp/FCRN-DepthPrediction，基于Tensorflow和matconvnet。</a></p></li><li><p><strong>深度分类网络</strong></p><p>将深度估计问题变为回归问题的缺点在于，太依赖于数据集的场景，并且由于图像中深度往往是分层的，类似于等高线之类的，所以也有学者将深度估计变为一个分类问题，而类别数就是将最远实际距离划分为多份而制作的。</p><p>以此为代表的是CVPR2018中《Deep Ordinal Regression Network for Monocular Depth Estimation》所提出的DORN框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173857556.png" alt="image-20200308173857556"></p><p>该框架2018年在多个数据集上取得了第一的名次，不过现在有个别算法超越了。可以看到，原图在经过密集特征提取之后，增加了一个场景理解模块，这一模块包含5个部分。其中full-image encoder部分与传统的自编码器不同：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173908649.png" alt="image-20200308173908649"></p><p>可以看到其先是利用池化进行下采样，将其拉伸为向量之后，利用全连接层进行编解码，然后还原为原图大小。而ASPP模块是利用了膨胀卷积（dilated convolution）进行特征提取，膨胀倍数分别为6,12,18。五个部分concat得到最终的特征图。再进入有序回归模块，实质上是多分类器。其将深度范围划分为多个区间：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173916000.png" alt="image-20200308173916000"></p><p>最后输出W×H×2K的结果，K代表深度区间，2K是因为每两个相邻的通道值2n表示深度小于n的概率，2n+1表示深度大于n的概率，二者利用softmax归一化。</p><p>其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173932875.png" alt="image-20200308173932875"></p><p>可以看到，DORN对于深度的细节把握得非常好，其速度约为2fps，代码基于caffe平台，链接为：<a href="https://github.com/hufu6371/DORN">https://github.com/hufu6371/DORN</a></p><p>无论是回归还是分类，都是以深度信息作为标签的监督算法，因此其受限于训练集场景，仅限于刷榜。</p></li></ul><h3 id="4-2结合双目视觉的单目深度估计"><a href="#4-2结合双目视觉的单目深度估计" class="headerlink" title="4.2结合双目视觉的单目深度估计"></a>4.2结合双目视觉的单目深度估计</h3><p>既然监督算法受限于场景，那么近两年则出现了很多无监督算法，其中就包含有利用双目数据进行训练的算法，下面我用几个例子进行说明。</p><p>首先以CVPR2017中《Unsupervised Monocular Depth Estimation with Left-Right Consistency》一文所提出的Monodepth算法为例。这篇论文算是这类算法的一个开端吧，效果并没有非常优异，但是引出这样一条思路。</p><p>其网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174121034.png" alt="image-20200308174121034"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174130252.png" alt="image-20200308174130252"></p><p>该算法将深度估计问题变成了左右视图立体匹配问题，都是先从原始图预测另外一个视图的视差，然后结合输出另外一个视图。整体框架依赖DispNet，而DispNet又是在FlowNet基础上进行的改变，主要改变是在多尺度衔接出增加卷积层，以保证图像尽可能平滑。</p><p>经过自编码器之后，分别利用逆卷积、预测的右视图相对左视图的视差+upsample/双线性插值、预测的左视图相对右视图的视差+upsample/双线性插值、原图。有了这些之后，损失函数部分则同时包含有：</p><ul><li><p>外观匹配损失，即预测的视图和实际视图的区别：<br>$$ C_{ap}^l = \frac{1}{N}\sum\limits_{i,j} {\alpha \frac{{1 - SSIM\left( {I_{ij}^l,\tilde I_{ij}^l} \right)}}{2}} + \left( {1 - \alpha } \right)\left\| {I_{ij}^l - \tilde I_{ij}^l} \right\| $$<br>其中SSIM指的是结构相似性。</p></li><li><p>视差平滑性约束：<br>$$ C_{ds}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {{\partial _x}d_{ij}^l} \right|} {e^{ - \left\| {{\partial _x}I_{ij}^l} \right\|}} + \left| {{\partial _y}d_{ij}^l} \right|{e^{ - \left\| {{\partial _y}I_{ij}^l} \right\|}} $$</p></li><li><p>左右视差一致性损失：<br>$$ C_{lr}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {d_{ij}^l - d_{ij + d_{ij}^l}^r} \right|} $$</p></li></ul><p>由于其主要在KITTI_outdoor和Cityscapes上训练的，所以对于室外场景效果会略好，又因为其算法框架比较简单，所以深度信息中的细节比较模糊，尤其是存在遮挡或者物体相连的情况时。测试效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174828519.png" alt="image-20200308174828519"></p><p>通过其原理和论文中的测试效果来看，其对于室外场景下的深度估计效果还行，不过对于边缘部分的把握不是很好。再加上大多是街景数据，所以对于室内场景的视角具有很大的不适应性。另外，由于立体匹配对于大面积的纯色或者颜色相近的图像块效果很差，所以Monodepth不适用于纹理不清晰的场景，容易将大片颜色类似的图像块视为一个整体。</p><p>代码是基于tensorflow进行开发的：<a href="https://github.com/mrharicot/monodepth，也有pytorch0.4.1的复现版本：https://github.com/ClubAI/MonoDepth-PyTorch。">https://github.com/mrharicot/monodepth，也有pytorch0.4.1的复现版本：https://github.com/ClubAI/MonoDepth-PyTorch。</a></p><p>与此同时，在CVPR2018中，由商汤团队在《Single View Stereo Matching》一文中提出了类似的SVS算法，其相对于Monodepth，在细节和场景适应性方面有了很大的提升。</p><p>SVS网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174851298.png" alt="image-20200308174851298"></p><p>从图中不难看出，得到预测的右视图之后，两个视角的图像进行类似于DispNet的立体匹配，从而获得左视图的视差。而关键在于怎么从左视图预测右视图。可以发现，SVS在利用卷积层对左视图进行特征提取之后，分别将每一通道的特征图和原图进行元素乘法，然后再相加。这一部分实际上是借鉴了Deep3D的框架。Deep3D模型是用来将2D图像转化为3D图像对的，其框架为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174909501.png" alt="image-20200308174909501"></p><p>假设每个通道分别代表着在左视图中每个像素点相对右视图中的视差偏移量的概率分布。综上，SVS实质上就是Deep3D+Dispnet的合体版，其效果图如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174943651.png" alt="image-20200308174943651"></p><p>同时可以看看基于KITTI数据集训练的SVS模型在其他数据集上的测试效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174958809.png" alt="image-20200308174958809"></p><p>代码是基于caffe开发的：<a href="https://github.com/lawy623/SVS">https://github.com/lawy623/SVS</a></p><h3 id="4-3基于视频的相机位姿估计和视觉测距"><a href="#4-3基于视频的相机位姿估计和视觉测距" class="headerlink" title="4.3基于视频的相机位姿估计和视觉测距"></a><strong>4.3基于视频的相机位姿估计和视觉测距</strong></h3><p>基于视频的单目深度估计大多都是面向相机位姿估计和视觉测距的，其核心就是利用相邻视频帧所产生的运动，近似多视角图像，并对相机位姿进行估计，从而可以估计出相机的移动路线，进一步完成SLAM工作。</p><p>那么在CVPR2017的一篇《Unsupervised Learning of Depth and Ego-Motion from Video》中则是提出了SFM算法，这篇文章中对于深度估计的求解较为简单，所以效果不是很好，但是提出了基于视频帧序列的相机位姿估计算法。</p><p>其论文中使用了相邻3帧的信息，不过代码却是用的相邻5帧的信息，整体框架比较简单：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175216815.png" alt="image-20200308175216815"></p><p>可以看到，单目深度估计部分仅仅是针对一帧的，直接采用了Dispnet的网络框架，不过我发现实际上是U-net，而相机位姿估计则是将相邻帧的相对相机位姿变化看作一个含有6个元素的向量（可以理解为x,y,z方向的平移量和旋转量）进行预测。有意思的是，SFM并没有使用深度信息作为标签，而是将深度信息作为一个过程变量，将前后帧图像联系起来，从而做到无监督学习，不过相机位姿的训练还是有监督的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175228213.png" alt="image-20200308175228213"></p><p>利用预测的相机位姿和深度信息，可估计出目标视图相对原视图的像素点位置，由于预测的像素点位置可能不是整数，为了保证其为整数，将采用双线性插值，其中K是相机参数矩阵：</p><p>$$ \left\{ \begin{array}{l} {p_s} \sim K{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over T} }_{t \to s}}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over D} }_t}\left( {{p_t}} \right){K^{ - 1}}{p_t}\\ {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( {{p_t}} \right) = {I_s}\left( {{p_s}} \right) = \sum\nolimits_{i \in \left\{ {t,b} \right\},j \in \left\{ {l,r} \right\}} {{w^{ij}}{I_s}\left( {p_s^{ij}} \right)} \\ \sum\nolimits_{i,j} {{w^{ij}}} = 1 \end{array} \right. $$<br>可以看到这里的插值方式是对估计像素点位置处的相邻4个位置的像素进行加权平均，然后作为目标像素点位置处的像素值，新合成的视图和目标视图进行一致性约束。</p><p>不过上述这种做法<strong>受限于静态场景，且无遮挡情况</strong>，为了缓解这种问题，作者又加入了一个可解释性网络：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175304525.png" alt="image-20200308175304525"></p><p>该网络的编码部分输出的是相机位姿，解码部分输出的是多个尺度的解释性眼膜，其意义是合成视图中的每个像素点能够被成功建模的概率。而这一部分是没有标签的，所以作者通过设计损失函数将其进行了约束：</p><p>$$ \begin{array}{l} {L_{vs}} = \sum\limits_{\left\langle {{I_1},...,{I_N}} \right\rangle \in S} {\sum\limits_p {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} }_s}\left( p \right)\left| {{I_t}\left( p \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( p \right)} \right|} } \\ L = \sum\limits_l {L_{vs}^l} + {\lambda _s}L_{smooth}^l + {\lambda _e}\sum\limits_s {{L_{reg}}\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} _s^l} \right)} \end{array} $$<br><em>l</em>指的是尺度，<em>s</em>指的是图片，其中的平滑性约束跟上一节所讲的Monodepth一样，由于解释性掩膜无标签，如果不加约束的话会自动为0，所以利用交叉熵损失函数对其进行了约束，默认为全1矩阵。其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175351205.png" alt="image-20200308175351205"></p><p>可以看到，深度估计的效果并不是很好，不过整体的设计思路很新颖，也可以看看其对于解释性掩膜的预测效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175401181.png" alt="image-20200308175401181"></p><p>可以发现，对于发生变化的部分，即前景部分，其不可解释性很高，其实这个也能用来估计光流。</p><p>代码是基于tensorflow的：<a href="https://github.com/tinghuiz/SfMLearner，">https://github.com/tinghuiz/SfMLearner，</a></p><p>不过有pytorch的复现版本：<a href="https://github.com/ClementPinard/SfmLearner-Pytorch">https://github.com/ClementPinard/SfmLearner-Pytorch</a></p><p>果不其然，在CVPR2018中商汤又提出了GeoNet，该网络在SFM的基础上增加了光流监督信息：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175411913.png" alt="image-20200308175411913"></p><p>可以看到，前半部分的深度估计和相机位姿估计都跟SFM一样，只是在后面增加了光流的输出，先利用前半部分得到刚性结构的光流，后半部分增加一个非刚性光流预测环节，二者之和就是最终的光流。效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175421097.png" alt="image-20200308175421097"></p><p>可以看到，GeoNet的深度估计效果并没有特别突出，代码是基于Tensorflow的：<a href="https://github.com/yzcjtr/GeoNet">https://github.com/yzcjtr/GeoNet</a></p><p>同样的还有CVPR2018的《Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction》一文中提到的Depth-VO-Feat:</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175431143.png" alt="image-20200308175431143"></p><p>直接从这个网络架构可以看到包含了两个部分的图像重构，一个是左视图和右视图的重构，一个是前后两帧间的重构，重构的意义在于找到对应像素点的联系，并非直接利用左右视图进行误差计算，可以看到图中对于右视图的边缘填充。由于该框架假设场景是Lambertian的，即无论从哪个角度观察，其光照强度是一致的，那么这对于图像的重构就很敏感，因此，作者又添加了特征的重构，框架一致。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175529893.png" alt="image-20200308175529893"></p><p>对于训练细节，除了图像和特征的L1重构误差之外，也加入了边缘平滑性约束，骨干网络是Resnet50的变种。对于深度估计，其预测的是深度信息的倒数。效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175539828.png" alt="image-20200308175539828"></p><p>可以看到，深度估计的效果还是中规中矩，不过其可以用来做视频中相机的移动轨迹预测，这一点在多目标跟踪（MOT）中对于手持相机的场景有所帮助。代码是基于caffe的：<a href="https://github.com/Huangying-Zhan/Depth-VO-Feat">https://github.com/Huangying-Zhan/Depth-VO-Feat</a></p><p>相应的，近几年关于无监督单目深度估计的研究越来越多，我抽空了看了下，比如有Google出品的vid2depth和struct2depth算法，二者的代码链接如下：</p><p>vid2depth:<a href="https://github.com/tensorflow/models/tree/master/research/vid2depth">https://github.com/tensorflow/models/tree/master/research/vid2depth</a></p><p>struct2depth: <a href="https://github.com/tensorflow/models/tree/master/research/struct2depth。">https://github.com/tensorflow/models/tree/master/research/struct2depth。</a></p><p>其他的也挺多的，后面章节我会再补充一点，不过肯定不全。</p><h3 id="4-4基于图像风格迁移的单目深度估计"><a href="#4-4基于图像风格迁移的单目深度估计" class="headerlink" title="4.4基于图像风格迁移的单目深度估计"></a>4.4基于图像风格迁移的单目深度估计</h3><p>实质上，深度图像也是一种图像风格，如果我们要将生成学习引入深度估计的话，就需要注意两个地方，一个是原始图像到深度图像的风格转变，这一点可以获取类似于分割的map，另一点就是对像素点的深度进行回归。这里的方式与第一节讲的深度回归模型不一样，因为第一步的风格转变，已经对于场景和相机位姿有了很好的适应性。</p><p>ECCV2018中《T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks》所提出的T2Net尝试性地将图像风格迁移引入单目深度估计领域，虽然效果只是2016年的水平，不过也算是一次很好的尝试了。下面介绍下T2Net的思路，首先给出其网络框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180333423.png" alt="image-20200308180333423"></p><p>框架很明显，对于室外场景，其训练集用到了KITTI和VKITTI中的sunset场景，对于室内场景，则使用了NYU Depth v2和SUNCG,没仔细看怎么下载，相关工具在<a href="https://github.com/shurans/SUNCGtoolbox。">https://github.com/shurans/SUNCGtoolbox。</a></p><p>从图中可以看到，作者做了两个模块，一个是图像风格迁移模块，一个是单目深度估计模块。其中图像风格迁移模块中包含有合成图像到真实图像的迁移，真实图像到真实图像的迁移，二者共用一个GAN。其中的Loss包含有：</p><ul><li>由合成图像风格迁移生成的图像与原始图像的GAN Loss，即利用判别器进行判定的误差；</li><li>由真实图像风格迁移生成的图像预原始图像的重构误差，这一部分计算L1 Loss；</li><li>由合成图像风格迁移生成的图像与原始图像的编码特征的GAN Loss。</li></ul><p>然后<strong>仅对合成图像分支进行深度估计</strong>，同样地，也加入了深度图的平滑性约束。从不匹配的图像对可以看出，其基础框架为CycleGAN。</p><p>可以看到风格迁移的效果和深度估计的效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180412887.png" alt="image-20200308180412887"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180416682.png" alt="image-20200308180416682"></p><p>从结果中我们发现有一个版本的实现效果超过了完整框架，通过查阅发现，是只利用真实数据进行深度估计的效果，也就是说效果比加入图像迁移的效果更好，打自己脸。。。实际上他是在跟只用合成图像进行深度估计训练的效果作比较，确实好了些。</p><p>代码链接：<a href="https://github.com/lyndonzheng/Synthetic2Realistic">https://github.com/lyndonzheng/Synthetic2Realistic</a></p><p>除此之外，在CVPR2018也有一篇类似的算法《Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer》，其效果则是达到了state-of-art,我们暂且称其为MDEDA,网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180432526.png" alt="image-20200308180432526"></p><p>熟悉CycleGAN框架的话，应该很容易看懂其中的关系，其中存在三种图像序列，一种是原始图像，一种是合成的图像，一种是深度图像，不同的是三种图像内容是一致的，而非CycleGAN那样不匹配的。其中原始图像和合成图像之间进行图像风格的循环迁移和重构，合成图像与深度图像进行单向的风格迁移。</p><p>效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180445762.png" alt="image-20200308180445762"></p><p>左侧的是直接对原图进行深度估计的效果，中间是其他图像迁移算法的效果，右侧是采用本文算法后的合成以及深度估计效果，速度大概为44fps。合成图像对于深度估计的效果提升也反映了一个问题，即图像光暗条件对于深度估计有很大影响，所以对于一些出现了阴影，如影子等的场景，深度估计会出现偏差，如：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180455841.png" alt="image-20200308180455841"></p><p>代码只提供了测试模型：<a href="https://github.com/atapour/monocularDepth-Inference">https://github.com/atapour/monocularDepth-Inference</a></p><h3 id="4-5多任务深度估计"><a href="#4-5多任务深度估计" class="headerlink" title="4.5多任务深度估计"></a>4.5多任务深度估计</h3><p>在ICRA2019中《Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations》中基于图像分割算法RefineNet设计了一个多任务框架。其中RefineNets是CVPR2017中提出的算法，其全局框架是基于Resnet的U-net网络框架，可以输出多尺度的分割图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180520856.png" alt="image-20200308180520856"></p><p>可以看到的是，RefineNet在每一个尺度的上采样部分都增加了一个局部提升的网络，用于多尺度输出的融合：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180529831.png" alt="image-20200308180529831"></p><p>所以其主要创新在于采用skip-connection和 Resnet Block的方式不断融合各种分辨率的特征，用于增加更多的细粒度特征，从而方便生成更高分辨率的预测：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180539369.png" alt="image-20200308180539369"></p><p>那么在BMVC2018中则是提出了一种Light-weighted RefineNet算法，顾名思义，就是RefineNet的轻量级网络，其对于512×512大小的图像，速度从RefineNet的20FPS提升到了55FPS（1080Ti），效果略微下降。代码基于Pytorch: <a href="https://github.com/DrSleep/light-weight-refinenet">https://github.com/DrSleep/light-weight-refinenet</a></p><p>那么回到正题，我们提到的这个同时进行深度估计和目标分割的网络框架，对于1200×350大小的输入，其速度为60FPS。网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180614033.png" alt="image-20200308180614033"></p><p>以上的结构通过之前我介绍的深度估计框架以及Light-Weighted RefineNet框架很容易能看懂，之所以比原本的Light-Weighted RefineNet还要快，是因为将其中的部分1×1卷积替换成了MobileNetV2中采用的depthwise卷积方式。</p><p>对于分割和深度估计任务的结合，从网络框架和损失函数的设计来看可以发现，其除了特征是共享的之外，预测任务是独立的。效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180623440.png" alt="image-20200308180623440"></p><p>代码仅提供了测试用例：<a href="https://github.com/drsleep/multi-task-refinenet">https://github.com/drsleep/multi-task-refinenet</a></p><p>ECCV2018中《DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency》一文提出了单目深度估计和光流预测的联合任务框架。不同于单独训练两个任务的方式，作者将二者的一致性进行了考虑，从而做到二者的相互促进，可以看到对比效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180707605.png" alt="image-20200308180707605"></p><p>其主要思路是利用无监督的方式从视频中预测深度信息和相机位姿变化，这一部分对于刚性流场景比较适用，即静态背景。通过几何一致性的约束监督，可以将3D的场景流映射到2D光流上，由此与光流预测模型的结果进行一致性约束。具体框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180715231.png" alt="image-20200308180715231"></p><p>乍一看可以发现网络框架的前半部分很眼熟，图中展示的是分别对前后帧做单目深度估计，然后利用前后帧做相机位姿变化预测和光流预测，结合SFM网络中像素点转移的计算公式，可以利用深度信息和相机位姿变化关系求得在t+1时刻对应像素点位置，由此可以计算刚性流场景下的光流。</p><p>​ 对于刚性流场景下的合成光流信息和直接预测到的光流信息，二者都反映了相邻两帧的像素点的对应关系，因此作者对此引入了光照约束（利用对比映射和插值，计算每个像素点的像素值差异）和深度的平滑性约束。</p><p>​ 再来看Forward-Backward模块，由于我们在上面提到了光照一致性约束，但实际上对于重叠区域并不适用，因此加入了前后向一致性的约束。即图中的Valid Mask部分，利用刚性流信息可以检测出一些无效的像素区域，如运动物体、画面边缘等，因为这些都不符合刚性这一条件，那么再在有效区域使用光照一致性假设：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180726951.png" alt="image-20200308180726951"></p><p>感觉这个跟SFM中的Explain Mask一样，然后前后的一致性约束，则是分光流和深度估计两部分，其中深度的一致性跟光照一致性的计算方式一样，而光流的一致性则是真的计算了前向和反向的光流一致性。最后对于深度和光流的共同有效区域，保证二者预测的光流尽可能一致。为了保证更好的训练效果，作者先在SYNTHIA数据集上预训练光流预测，采用的是UnFlownet-C网络，在KITTI和Cityscapes上预训练深度估计和相机位姿预测，采用的是SFM框架，然后进行联合训练。代码基于Tensorflow: <a href="https://github.com/vt-vl-lab/DF-Net">https://github.com/vt-vl-lab/DF-Net</a></p><p>我前段时间还发现一个多任务的集成框架CVPR2019的CCN算法《Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation》，效果目前好像还是SOTA，其融合了单目深度估计、相机位姿估计、光流估计和运动分割多个任务，代码：<a href="https://github.com/anuragranj/cc">https://github.com/anuragranj/cc</a></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180928316.png" alt="image-20200308180928316"></p><p>本小节的内容都是基于无监督的单目深度估计算法。</p><h2 id="5总结"><a href="#5总结" class="headerlink" title="5总结"></a><strong>5总结</strong></h2><p>对于单目深度估计模型，目前主要分为基于回归/分类的监督模型，基于双目训练/视频序列的无监督模型，以及基于生成学习的图像风格迁移模型。大概从2017年起，即CVPR2018开始，单目深度估计的效果就已经达到了双目深度估计的效果，主要是监督模型。但是由于现有的数据集主要为KITTI、Cityscapes、NYU DepthV2等，其场景和相机都是固定的，从而导致监督学习下的模型无法适用于其他场景，尤其是多目标跟踪这类细节丰富的场景，可以从论文中看到，基本上每个数据集都会有一个单独的预训练模型。</p><p>对于GAN，其对于图像风格的迁移本身是一个很好的泛化点，既可以用于将场景变为晴天、雾天等情况，也可以用于图像分割场景。但是深度估计问题中，像素点存在相对大小，因此必定涉及到回归，因此其必定是监督学习模型，所以泛化性能也不好，以CVPR2018的那篇GAN模型为例可以对比：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181145568.png" alt="image-20200308181145568"></p><p>左边是KITTI的测试效果，右边是MOT的测试效果，从上到下依次是原图、合成图，以及深度图。可以看到，其泛化性能特别差。而对于无监督模型，从理论上来讲，其泛化性能更好。那么对于无监督模型，我们分两部分进行讨论，第一部分是利用双目视差进行训练的无监督模型，这里的无监督模型中包含有左右视图预测的监督信息，所以存在一定程度的局限性。以Monodepth为例：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181208321.png" alt="image-20200308181208321"></p><p>对于无监督的算法，可能场景适应性会更好，但依旧不适用于对行人深度的估计。</p><h2 id="6参考文献"><a href="#6参考文献" class="headerlink" title="6参考文献"></a>6参考文献</h2><p>[1] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.</p><p>[2] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.</p><p>[3] Laina I, Rupprecht C, Belagiannis V, et al. Deeper depth prediction with fully convolutional residual networks[C]//2016 Fourth international conference on 3D vision (3DV). IEEE, 2016: 239-248.</p><p>[4] Fu H, Gong M, Wang C, et al. Deep ordinal regression network for monocular depth estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2002-2011.</p><p>[5] Godard C, Mac Aodha O, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 270-279.</p><p>[6] Dosovitskiy A, Fischer P, Ilg E, et al. Flownet: Learning optical flow with convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2758-2766.</p><p>[7] Ilg E, Mayer N, Saikia T, et al. Flownet 2.0: Evolution of optical flow estimation with deep networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2462-2470.</p><p>[8] Mayer N, Ilg E, Hausser P, et al. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4040-4048.</p><p>[9] Xie J, Girshick R, Farhadi A. Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 842-857.</p><p>[10] Luo Y, Ren J, Lin M, et al. Single View Stereo Matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p><p>[11] Zhou T, Brown M, Snavely N, et al. Unsupervised learning of depth and ego-motion from video[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 1851-1858.</p><p>[12] Yin Z, Shi J. Geonet: Unsupervised learning of dense depth, optical flow and camera pose[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1983-1992.</p><p>[13] Zhan H, Garg R, Saroj Weerasekera C, et al. Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 340-349.</p><p>[14] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680.</p><p>[15] Radford A , Metz L , Chintala S . Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[J]. Computer Science, 2015.</p><p>[16] Arjovsky M, Chintala S, Bottou L. Wasserstein gan[J]. arXiv preprint arXiv:1701.07875, 2017.</p><p>[17] Gulrajani I, Ahmed F, Arjovsky M, et al. Improved training of wasserstein gans[C]//Advances in Neural Information Processing Systems. 2017: 5767-5777.</p><p>[18] Mao X, Li Q, Xie H, et al. Least squares generative adversarial networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2794-2802.</p><p>[19] Mirza M, Osindero S. Conditional generative adversarial nets[J]. arXiv preprint arXiv:1411.1784, 2014.</p><p>[20] Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.</p><p>[21] Wang T C, Liu M Y, Zhu J Y, et al. High-resolution image synthesis and semantic manipulation with conditional gans[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8798-8807.</p><p>[22] Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232.</p><p>[23] Wang T C , Liu M Y , Zhu J Y , et al. Video-to-Video Synthesis[J]. arXiv preprint arXiv:1808.06601,2018.</p><p>[24] Zheng C, Cham T J, Cai J. T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 767-783.</p><p>[25] Atapour-Abarghouei A, Breckon T P. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2800-2810.</p><p>[26] Nekrasov V , Dharmasiri T , Spek A , et al. Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations[J]. arXiv preprint arXiv:1809.04766,2018.</p><p>[27] Nekrasov V , Shen C , Reid I . Light-Weight RefineNet for Real-Time Semantic Segmentation[J]. arXiv preprint arXiv:1810.03272, 2018.</p><p>[28] Lin G , Milan A , Shen C , et al. RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.,2017:1925-1934</p><p>[29] Zou Y , Luo Z , Huang J B . DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018:36-53.</p><p>[30] Ranjan A, Jampani V, Balles L, et al. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 12240-12249.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="3D视觉" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3D%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="深度估计" scheme="https://huangpiao.tech/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>多目标跟踪中的数据关联代码实践(下)</title>
    <link href="https://huangpiao.tech/2020/03/06/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5(%E4%B8%8B)/"/>
    <id>https://huangpiao.tech/2020/03/06/多目标跟踪中的数据关联代码实践(下)/</id>
    <published>2020-03-06T09:30:00.000Z</published>
    <updated>2020-03-06T09:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到<a href="https://github.com/nightmaredimple/libmot">github</a>。</p></blockquote><a id="more"></a><h2 id="4-最小代价流"><a href="#4-最小代价流" class="headerlink" title="4 最小代价流"></a>4 最小代价流</h2><h3 id="4-1-算法形式"><a href="#4-1-算法形式" class="headerlink" title="4.1 算法形式"></a>4.1 算法形式</h3><p>在了解最小代价流之前，我们需要先铺垫一下几个常见图模型，以帮助我们理解，比如最短路、最大流、最小费用最大流，最小割（闭嘴，我暂时没看懂）。下图是一个很常见的图网络：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306145213784.png" alt="image-20200306145213784"></p><p>我们可以看到，图上有很多节点和边，这两个元素是组成图模型的核心。其次，每条边上都会有对应的数值，比如最短路问题中的相邻两节点的距离，最大流中的边容量，最小费用最大流问题中的边容量和费用。那么我们来看看几个问题的具体定义：</p><ul><li><strong>最短路问题</strong></li></ul><p>最短路问题一般特指单源单汇最短路问题，即给定起点和终点，从各种路径中选择最短的路径。</p><p>$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{W_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{j = 1}^N {{x_{ij}} - \sum\limits_{k = 1}^N {{x_{ki}}} } {\rm{ = }}\left\{ \begin{array}{l} 1,i = 1\\ - 1,i = N\\ 0,i \ne 1,N \end{array} \right.\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$<br>上面公式中如果结合图模型来思考会简单很多，即中间节点无论会不会通过，其流入边和流出边一定有且只有0或1个，不可能经过这个点而不经过与之相邻的边。不过对于起点和终点则允许有一条流出边或一条流入边。</p><ul><li><strong>最大流问题</strong></li></ul><p>最大流问题就是选择从起点到终点的最大流量分配，与最短路的最大区别在于<strong>最短路问题中每个节点只能选择一条流出边和一条流入边，而最大流问题则只要满足边容量限制，则可任意选择流入流出边数量。</strong></p><p>$$ \begin{array}{l} max = v\left( f \right) = \sum\limits_j {{f_{sj}}} - \sum\limits_i {{f_{is}}} \\ s.t.\left\{ \begin{array}{l} \sum\limits_j {{f_{ij}}} = \sum\limits_k {{f_{ki}}} \\ \sum\limits_j {{f_{sj}}} = \sum\limits_k {{f_{kt}}} = v\left( f \right)\\ 0 \le {f_{ij}} \le {w_{ij}} \end{array} \right. \end{array} $$<br>上面公式的意思是，即中间节点无论会不会通过，其流入边流量之和=流出边流量之和，从起点流出的总流量=流入终点的总流量，每条边的流量有上限。</p><ul><li><strong>最小费用流问题</strong></li></ul><p>最小费用流的约束条件和最大流的一样，只不过为了更好描述目标函数我改写成了类似于最短路问题的形式。<strong>其目标是选择费用最短的流，当然，它跟最大流问题不同，这里需要设置起始点的流出流量，而且，如果在最大流限制下求解最小费用流，那么就是最小费用最大流问题了</strong>。</p><p>$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{j = 1}^N {{x_{ij}} - \sum\limits_{k = 1}^N {{x_{ki}}} } {\rm{ = }}\left\{ \begin{array}{l} 负数,i = 1\\ 正数,i = N\\ 0,i \ne 1,N \end{array} \right.\\ 0 \le {x_{ij}} \le {w_{ij}} \end{array} \right. \end{array} $$<br>联系到多目标跟踪任务，其数据关联任务从短期来看就是一个二分图匹配问题，从长期来看就是一个图网络模型。</p><p>（1）如果我们要用最短路模型来描述数据关联问题，其节点就是跟踪对象id，边代表跟踪轨迹和检测之间的代价。那单源单汇最短路模型就远远不足以描述，因为跟踪轨迹和检测数量是大于1的，所以从形式上来讲是<strong>多源多汇</strong>最短路，但是最短路没有限制<strong>中间节点的可重复性</strong>，所以这个问题应该用路由问题中的<strong>K –最短不相交路线（K shortest disjoint paths）</strong>来描述；</p><p>（2）如果我们要用最大流模型来描述该问题，那么不同于最短路，<strong>边容量代表跟踪轨迹和检测之间的连接可能性，所以只可能是0和1</strong>。最终要求的就是最大流量，由于边容量的限制，所以不可能重复，也就是最多可能轨迹。而且这么看来，匈牙利算法很像是最大流模型的特例；</p><p>（3）如果我们要用最小费用流模型来描述该问题，那么就跟第一部分中的最短路问题一样了，只不过多源多汇问题变成了给定初始流量的情形，距离变成了费用。最大的区别在于需要<strong>合理设定初始流量</strong>（代表了最终有多少条轨迹），还要设定<strong>边容量</strong>，不然容易所有流都流向同一条边；</p><p>（4）结合（2）（3）来看，最大流模型只需要设定跟踪轨迹和检测的连接可能性，但是缺乏了相对性。而最小费用流则只需要设定代价值，但是需要设定初始流量。这里的初始流量代表了轨迹数量，所以先用最大流模型求出最大流，即可作为初始的轨迹数量，然后再求最小费用流即可，也就是<strong>最小费用最大流</strong>。不过两个任务都有着相同的任务，那就是寻找目标轨迹，所以这样来说时间效率会较低。一般来说我们会直接使用最大流模型/最小割模型，或者直接使用最小费用流+搜索算法。</p><p>总的来说，最大流模型的优点是参数量少，但是确定跟匈牙利算法一样，我们无法对于0.9和0.5相似度的边进行相对选择，因为都是1。最小费用流模型的优点是保留了相似度，但是初始流量这一超参数不好设定。K-最短不相交路模型跟最小费用流一样，都需要设定轨迹数量。所以我们会选择用搜索算法使用最小费用流，通过搜索阈值使用最大流模型.</p><h3 id="4-2基于最大化后验概率模型的网络流建图"><a href="#4-2基于最大化后验概率模型的网络流建图" class="headerlink" title="4.2基于最大化后验概率模型的网络流建图"></a>4.2基于最大化后验概率模型的网络流建图</h3><p>对于最小费用流而言，最难的地方在于设定初始流量和边容量，使得跟踪轨迹不交叉，而且跟踪轨迹尽可能多而合理。最重要的是，我们不知道在网络模型中哪个节点是轨迹的起点或者终点，这些都需要我们去建模。再加上我们的目标是使得代价最小，极可能最终出现每条轨迹只有一个节点的情形。</p><p>下面我们要设定几个代价值，由于每个点都有属于轨迹起点和终点的可能性，所以网络会非常大，为了更好地借鉴已有的最小费用流模型，我们可以转化为单一起点和终点的网络图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306145842980.png" alt="image-20200306145842980"></p><p>我们可以看到，简单的最小代价流结构存在几个问题：</p><ul><li><p>一开始我们就要选定哪些节点有可能成为起点，哪些节点有可能成为终点，这无疑增加了参数量；</p></li><li><p>类似于最大流模型，我们通过设定边容量为1可以保证每条边最多被选择一次。但是，我们无法确保最多只有一个节点可以连接到目标节点，这就不能保证跟踪轨迹的不重叠。</p></li></ul><p>针对以上问题，我们可以引入过渡节点的概念，同时也就引入了过渡边，每个节点连接一条过渡边，这样通过设定过渡边容量，可以限制每个节点的流出流量，相应地就可以限制最多只有一个节点可以连接到目标节点。而且，我们让每个节点都连接起点，每个节点的过渡节点连接终点，这样就保证上面两个问题都解决了。具体网络结构如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306150000886.png" alt="image-20200306150000886" style="zoom:67%"></p><p>我们可以看到每个节点u都连接了起点，每个节点v都连接了终点，每个节点u都连接了过渡节点v。正如我们之前说的，每条边容量都是1，可以有效防止轨迹重叠。另外我在图中注明了每条边费用的取值范围，我们定义每条包含起点和终点的边的费用都是比较大的正数，节点与过渡节点的边的费用是负数，这样可以避免过早的终止轨迹，过渡节点与节点之间的边的费用就是跟踪轨迹和检测的代价值，取正数，不然每条轨迹都会在最后一帧终止。所以这里的参数有：<strong>初始流量的大小（轨迹数量）、节点属于轨迹起点/终点的概率、节点到过渡节点的补偿（选择这个节点的补偿）、过渡节点到节点的概率（匹配代价）。</strong></p><p>​ 下面我们联系多目标跟踪模型的形式来为这些参数赋予特殊的含义，首先给出后验概率形式，T表示已有轨迹，Z表示观测值：</p><p>$$ \begin{array}{l} {T^ * } = \mathop {argmax}\limits_T P\left( {\left. T \right|Z} \right)\\ \;\;\;\; = \mathop {argmax}\limits_T \frac{{P\left( {\left. Z \right|T} \right)P\left( T \right)}}{{P\left( Z \right)}}\\ \;\;\;\; = \mathop {argmax}\limits_T \frac{{\prod\limits_i {P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)} }}{{P\left( Z \right)}} \end{array} $$<br>如果我们不考虑目标之间的联系，即假设目标相互独立，将联系归于代价值之中。那么上式就可以转化为：</p><p>$$ {T^ * } = \mathop {argmax}\limits_T \frac{{\prod\limits_i {P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)} }}{{\prod\limits_i {P\left( {{z_i}} \right)} }} = \mathop {argmax}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} $$<br>这里我们就需要了解三个概率：</p><p>$$ \left\{ \begin{array}{l} P\left( {{z_i}} \right) = {\theta _i}\\ P\left( {\left. {{z_i}} \right|T} \right) = P\left( {\left. {{z_i}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_k}}}}}} \right\}} \right)\\ P\left( {{T_k}} \right) = P\left( {\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_k}}}}}} \right\}} \right)\\ \;\;\;\;\;\;\;\;\;\; = {P_s}\left( {{x_{{k_0}}}} \right)P\left( {\left. {{x_{{k_1}}}} \right|{x_{{k_0}}}} \right)P\left( {\left. {{x_{{k_2}}}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}}} \right\}} \right)...P\left( {\left. {{x_{{k_{{l_k}}}}}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_{k - 1}}}}}}} \right\}} \right) \end{array} \right. $$<br>另外，我们还需要补充节点的概念来完善数据关联模型，因为上面的几个概念中我们还没有加入轨迹的终点的概率，所以明确一下联合概率数据关联模型：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306150308865.png" alt="image-20200306150308865" style="zoom:50%"></p><p>上图中虚线部分代表非当前时刻的节点，这样我们就将虚警和杂波利用起点和终点消除了，由于我们可以跨帧连接，所以虚拟目标就可以近似忽略。</p><p>接下来我们开始分析概率模型，我们可以利用对数似然概率来描述：</p><p>$$ \begin{array}{l} {T^ * } = \mathop {argmax}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} \\ \;\;\;\; = \mathop {argmin - ln}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} \\ \;\;\;\; = \mathop {argmin}\limits_T \sum\limits_i {\left( { - ln\left[ {P\left( {\left. {{z_i}} \right|T} \right)} \right] - ln\left[ {P\left( T \right)} \right] + ln\left[ {P\left( {{z_i}} \right)} \right]} \right)} \end{array} $$<br>其中$P\left( {\left. {{z_i}} \right|T} \right)$ 就是当前跟踪轨迹和检测之间的相似度，$P\left( T \right)$就是当前跟踪轨迹存在的概率，$P\left( \right)$表示该观测存在的概率，因为观测有可能是杂波或者虚警，我们可以用于过渡边的代价描述。最后就只剩下${P_s}\left( {{x_{{k_0}}}} \right)$和$P\left( {\left. t \right|T} \right)$两个概率值，这个我们可以当做是一个参数进行试验。就这样，我们最小代价流模型中的每个节点和每条边都赋予了有意义的概念。</p><h3 id="4-3-在线和离线跟踪分析"><a href="#4-3-在线和离线跟踪分析" class="headerlink" title="4.3 在线和离线跟踪分析"></a>4.3 在线和离线跟踪分析</h3><p>这里我们所说的在线和离线模型的意思是一帧一帧使用min cost flow或者多帧一起优化。对于在线的优化，我们就不需要考虑有多个节点连接同一个节点的特殊情况了，也就是说我们可以直接忽略过渡边，这样就跟KM算法一模一样了，所以我们可以认为匈牙利算法是最大流模型的特殊情况，KM算法是最小费用流的特殊情况。</p><p>接下来我们分别对在线和离线的最小代价流模型进行对比实验，其中在线最小代价流模型我们还是采用Kalman+马氏距离的方式构建代价矩阵。而离线的方式下我们则直接使用IOU和HSV直方图作为构建代价矩阵的指标。而对于观测量的概率，即决定过渡边权的指标，我们采用检测的置信度(ln(a*confidence+b))作为指标，而对于起点和终点的判定，我们将其作为超参数，连同代价阈值和特征衰减变量作为超参数。</p><p>其中特征衰减变量是对轨迹短暂消失的惩罚：</p><p>$$ similarity = similarity \times miss\_rat{e^{time\_gap - 1}} $$<br>另外，无论是在线跟踪还是离线跟踪，MinCostFlow这个任务本身都需要设定初始流量，也就是跟踪轨迹数量，这个值我们都知道是最少是1，最多是总id数。那么我们就需要用搜索算法来解决，为了保证求解效率，我们简单假设这个问题是一维凸优化问题，采用二分搜索或者斐波那契搜索来进行。</p><p>其中二分搜索很简单，对于斐波那契搜索，我们知道斐波那契数列{0,1,1,2,3…}，即<em>f(n)=f(n-</em>1<em>)+f(n-</em>2<em>)</em>。对于这个通项公式，我们可以看到对于长度为f(n)的搜索空间，可以将其分为f(n-1)和f(n-2)两个部分，这样就实现了搜索空间的缩减。下面给出具体的算法：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306150957100.png" alt="image-20200306150957100"></p><p>可以看到我上面用了<strong>哈希表</strong>来存储搜索过程中的结果，避免重复运算，保证O(1)的查询效率。另外，对于求解结果，一般返回的是匹配点对，我们如何将其变成轨迹呢？这就是一个经典的“朋友圈”问题，可以采用<strong>并查集</strong>来求解，只需要O(n)的时间复杂度和O(n)的空间复杂度。不过我们这个问题简单一点，不存在一个点对应多个点的情况，所以可以简单利用数组或哈希表建立多叉树求解。</p><h3 id="4-4-代码实验"><a href="#4-4-代码实验" class="headerlink" title="4.4 代码实验"></a>4.4 代码实验</h3><p>为了方便，我们直接用IOU和HSV颜色直方图作为特征进行试验。由于代码太长，我这里这放一部分，其中的Fibonacci搜索过程代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(self, n)</span>:</span></span><br><span class="line">    <span class="string">"""Use Fibonacci Search to speed up Searching</span></span><br><span class="line"><span class="string">    there can exist u~v flows(id), so we need to find the min cost flows</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    n: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    fn: int</span></span><br><span class="line"><span class="string">        the n th fibonacci number</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> n &gt; <span class="number">-1</span>, <span class="string">"n must be non-negative number"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> self.fib:</span><br><span class="line">        <span class="keyword">return</span> self.fib[n]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.fib.setdefault(n, self.fibonacci(n - <span class="number">1</span>) + self.fibonacci(n - <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci_search</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Run Fibonacci Searching to find the min cost flow</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    trajectories: List[List]</span></span><br><span class="line"><span class="string">        List of trajectories</span></span><br><span class="line"><span class="string">    min_cost: float</span></span><br><span class="line"><span class="string">        cost of assignments</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    r = max(<span class="number">0</span>, self.max_flow - self.min_flow)</span><br><span class="line">    s = self.min_flow</span><br><span class="line">    cost = &#123;&#125;</span><br><span class="line">    trajectories = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># find the nearest pos of fibonacci</span></span><br><span class="line">    <span class="keyword">while</span> r &gt; self.fibonacci(k):</span><br><span class="line">        k = k + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> k &gt; <span class="number">1</span>:</span><br><span class="line">        u = min(self.max_flow, s + self.fibonacci(k - <span class="number">1</span>))</span><br><span class="line">        v = min(self.max_flow, s + self.fibonacci(k - <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> cost:</span><br><span class="line">            self.graph.SetNodeSupply(<span class="number">0</span>, u)</span><br><span class="line">            self.graph.SetNodeSupply(<span class="number">1</span>, -u)</span><br><span class="line">            <span class="keyword">if</span> self.graph.Solve() == self.graph.OPTIMAL:</span><br><span class="line">                cost[u] = self.graph.OptimalCost()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cost[u] = np.inf</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> cost:</span><br><span class="line">            self.graph.SetNodeSupply(<span class="number">0</span>, v)</span><br><span class="line">            self.graph.SetNodeSupply(<span class="number">1</span>, -v)</span><br><span class="line">            <span class="keyword">if</span> self.graph.Solve() == self.graph.OPTIMAL:</span><br><span class="line">                cost[v] = self.graph.OptimalCost()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cost[v] = np.inf</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cost[v] == cost[u]:</span><br><span class="line">            s = v</span><br><span class="line">            k = k - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> cost[v] &lt; cost[u]:</span><br><span class="line">            k = k - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            s = u</span><br><span class="line">            k = k - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    self.graph.SetNodeSupply(<span class="number">0</span>, s)</span><br><span class="line">    self.graph.SetNodeSupply(<span class="number">1</span>, -s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.graph.Solve() == self.graph.OPTIMAL:</span><br><span class="line">        min_cost =  self.graph.OptimalCost() / multi_factor</span><br><span class="line">        hashlist = &#123;<span class="number">0</span>: []&#125;</span><br><span class="line">        <span class="comment"># create disjoint set</span></span><br><span class="line">        <span class="keyword">for</span> arc <span class="keyword">in</span> range(self.graph.NumArcs()):</span><br><span class="line">            <span class="keyword">if</span> self.graph.Flow(arc) &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> self.graph.Tail(arc) == <span class="number">0</span>:</span><br><span class="line">                    hashlist[<span class="number">0</span>].append(self.graph.Head(arc))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    hashlist[self.graph.Tail(arc)] = self.graph.Head(arc)</span><br><span class="line">        <span class="keyword">for</span> entry <span class="keyword">in</span> hashlist[<span class="number">0</span>]:</span><br><span class="line">            tracklet = [(</span><br><span class="line">                        self.node[entry][<span class="string">'frame_idx'</span>],</span><br><span class="line">                        self.node[entry][<span class="string">'box_idx'</span>],</span><br><span class="line">                        self.node[entry][<span class="string">'box'</span>]</span><br><span class="line">                         )]</span><br><span class="line">            point = hashlist[entry]</span><br><span class="line">            <span class="keyword">while</span> point != <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> self.node[point][<span class="string">'type'</span>] == <span class="string">'object'</span>:</span><br><span class="line">                    tracklet.append((</span><br><span class="line">                        self.node[point][<span class="string">'frame_idx'</span>],</span><br><span class="line">                        self.node[point][<span class="string">'box_idx'</span>],</span><br><span class="line">                        self.node[point][<span class="string">'box'</span>]</span><br><span class="line">                         ))</span><br><span class="line">                <span class="keyword">if</span> point <span class="keyword">in</span> hashlist:</span><br><span class="line">                    point = hashlist[point]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            trajectories.append(tracklet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        min_cost = inf_cost</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trajectories, min_cost</span><br></pre></td></tr></table></figure><p>跟踪部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, boxes, scores, image = None, features = None, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">"""Process one frame of detections.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    boxes : ndarray</span></span><br><span class="line"><span class="string">        An Nx4 dimensional array of bounding boxes in</span></span><br><span class="line"><span class="string">        format (top-left-x, top-left-y, width, height).</span></span><br><span class="line"><span class="string">    scores : ndarray</span></span><br><span class="line"><span class="string">        An array of N associated detector confidence scores.</span></span><br><span class="line"><span class="string">    image : Optional[ndarray]</span></span><br><span class="line"><span class="string">        Optionally, a BGR color image;</span></span><br><span class="line"><span class="string">    features : Optional[ndarray]</span></span><br><span class="line"><span class="string">        Optionally, an NxL dimensional array of N feature vectors</span></span><br><span class="line"><span class="string">        corresponding to the given boxes. If None given, bgr_image must not</span></span><br><span class="line"><span class="string">        be None and the tracker must be given a feature model for feature</span></span><br><span class="line"><span class="string">        extraction on construction.</span></span><br><span class="line"><span class="string">    **kwargs : other parameters that model needed</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    trajectories: List[List[Tuple[int, int, ndarray]]]</span></span><br><span class="line"><span class="string">        Returns [] if the tracker operates in offline mode. Otherwise,</span></span><br><span class="line"><span class="string">        returns the set of object trajectories at the current time step.</span></span><br><span class="line"><span class="string">    entire_trajectories: List[List[Tuple[int, int, ndarray]]]</span></span><br><span class="line"><span class="string">        entire time steps trajectories</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># save the first node id in current frame</span></span><br><span class="line">    first_node_id = deepcopy(self.node_idx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize graph in every time step when online</span></span><br><span class="line">    <span class="keyword">if</span> self.mode == <span class="string">"online"</span> <span class="keyword">and</span> self.current_frame_idx &gt; <span class="number">1</span>:</span><br><span class="line">        self.graph = pywrapgraph.SimpleMinCostFlow()</span><br><span class="line">        self.trajectories = []</span><br><span class="line">        <span class="keyword">if</span> self.powersave:</span><br><span class="line">             self.node = &#123;key: self.node[key] <span class="keyword">for</span> key <span class="keyword">in</span> self.node \</span><br><span class="line">                          <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> range(<span class="number">2</span>, self.last_frame_id)&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.last_frame_id, self.node_idx):</span><br><span class="line">            self.graph.AddArcWithCapacityAndUnitCost(<span class="number">0</span>, int(i), <span class="number">1</span>, \</span><br><span class="line">                                                     int(multi_factor * self.entry_exit_cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute features if necessary.</span></span><br><span class="line">    parameters = &#123;<span class="string">'image'</span>: image, <span class="string">'boxes'</span>: boxes, <span class="string">'scores'</span>: scores,</span><br><span class="line">                  <span class="string">'miss_rate'</span>: self.miss_rate, <span class="string">'batch_size'</span>: self.batch_size&#125;</span><br><span class="line">    parameters.update(kwargs)</span><br><span class="line">    <span class="keyword">if</span> features <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> self.feature_model <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">"No feature model given"</span></span><br><span class="line">        features = self.feature_model(**parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add nodes to graph for detections observed at this time step.</span></span><br><span class="line">    observation_costs = (self.observation_model(**parameters)</span><br><span class="line">        <span class="keyword">if</span> len(scores) &gt; <span class="number">0</span> <span class="keyword">else</span> np.zeros((<span class="number">0</span>,)))</span><br><span class="line">    node_ids = []</span><br><span class="line">    <span class="keyword">for</span> i, cost <span class="keyword">in</span> enumerate(observation_costs):</span><br><span class="line">        self.node.update(&#123;self.node_idx:</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"type"</span>: <span class="string">'object'</span>,</span><br><span class="line">                <span class="string">"box"</span>: boxes[i],</span><br><span class="line">                <span class="string">"feature"</span>: features[i],</span><br><span class="line">                <span class="string">"frame_idx"</span>: self.current_frame_idx,</span><br><span class="line">                <span class="string">"box_idx"</span>: i,</span><br><span class="line">                <span class="string">'cost'</span>: cost</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># save object node id to this time step</span></span><br><span class="line">        node_ids.append(self.node_idx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">'online'</span>:</span><br><span class="line">            <span class="keyword">if</span> self.current_frame_idx == <span class="number">0</span>:</span><br><span class="line">                self.graph.AddArcWithCapacityAndUnitCost(<span class="number">0</span>, int(self.node_idx), <span class="number">1</span>, \</span><br><span class="line">                                                     int(multi_factor*self.entry_exit_cost))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx), <span class="number">1</span>, <span class="number">1</span>, \</span><br><span class="line">                                                         int(multi_factor * self.entry_exit_cost))</span><br><span class="line">            self.node_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.node.update(&#123;self.node_idx + <span class="number">1</span>:</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">'transition'</span>,</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            self.graph.AddArcWithCapacityAndUnitCost(<span class="number">0</span>, int(self.node_idx), <span class="number">1</span>, \</span><br><span class="line">                                                     int(multi_factor * self.entry_exit_cost))</span><br><span class="line">            self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx), int(self.node_idx + <span class="number">1</span>), \</span><br><span class="line">                                                     <span class="number">1</span>, int(multi_factor * cost))</span><br><span class="line">            self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx + <span class="number">1</span>), <span class="number">1</span>, <span class="number">1</span>, \</span><br><span class="line">                                                     int(multi_factor * self.entry_exit_cost))</span><br><span class="line">            self.node_idx += <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Link detections to candidate predecessors.</span></span><br><span class="line">    predecessor_time_slices = (</span><br><span class="line">        self.nodes_in_timestep[-(<span class="number">1</span> + self.max_num_misses):])</span><br><span class="line">    <span class="keyword">for</span> k, predecessor_node_ids <span class="keyword">in</span> enumerate(predecessor_time_slices):</span><br><span class="line">        <span class="keyword">if</span> len(predecessor_node_ids) == <span class="number">0</span> <span class="keyword">or</span> len(node_ids) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        predecessors = [self.node[x] <span class="keyword">for</span> x <span class="keyword">in</span> predecessor_node_ids]</span><br><span class="line">        predecessor_boxes = np.asarray(</span><br><span class="line">            [node[<span class="string">"box"</span>] <span class="keyword">for</span> node <span class="keyword">in</span> predecessors])</span><br><span class="line">        <span class="keyword">if</span> isinstance(features,np.ndarray):</span><br><span class="line">            predecessor_features = np.asarray(</span><br><span class="line">                [node[<span class="string">"feature"</span>] <span class="keyword">for</span> node <span class="keyword">in</span> predecessors])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            predecessor_features = torch.cat(</span><br><span class="line">                [node[<span class="string">"feature"</span>].unsqueeze(<span class="number">0</span>) <span class="keyword">for</span> node <span class="keyword">in</span> predecessors])</span><br><span class="line"></span><br><span class="line">        time_gap = len(predecessor_time_slices) - k</span><br><span class="line"></span><br><span class="line">        transition_costs = self.transition_model(</span><br><span class="line">            miss_rate = self.miss_rate,</span><br><span class="line">            time_gap = time_gap, predecessor_boxes = predecessor_boxes,</span><br><span class="line">            predecessor_features = predecessor_features,</span><br><span class="line">            boxes = boxes, features = features, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, costs <span class="keyword">in</span> enumerate(transition_costs):</span><br><span class="line">            <span class="keyword">for</span> j, cost <span class="keyword">in</span> enumerate(costs):</span><br><span class="line">                <span class="keyword">if</span> cost &gt; self.cost_threshold:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> self.mode == <span class="string">'online'</span>:</span><br><span class="line">                    last_id = int(predecessor_node_ids[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    last_id = int(predecessor_node_ids[i] + <span class="number">1</span>)</span><br><span class="line">                self.graph.AddArcWithCapacityAndUnitCost(last_id, int(node_ids[j]), <span class="number">1</span>,</span><br><span class="line">                                                         int(multi_factor * cost))</span><br><span class="line">    self.nodes_in_timestep.append(node_ids)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute trajectories if in online mode</span></span><br><span class="line">    <span class="keyword">if</span> self.mode == <span class="string">'online'</span>:</span><br><span class="line">        <span class="keyword">if</span> self.current_frame_idx &gt; <span class="number">0</span>:</span><br><span class="line">            min_cost, n_flow = self.binary_search(high = min(len(predecessor_time_slices[<span class="number">0</span>]), len(node_ids)))</span><br><span class="line">            <span class="keyword">if</span> n_flow &gt; <span class="number">0</span>:</span><br><span class="line">                self.trajectories = self.get_trajectory()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.trajectories = self.node2trajectory(first_node_id, self.node_idx)</span><br><span class="line">            self.entire_trajectories = self.merge_trajectories(self.trajectories, self.entire_trajectories)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.trajectories = self.node2trajectory(<span class="number">2</span>, self.node_idx)</span><br><span class="line">            self.entire_trajectories = deepcopy(self.trajectories)</span><br><span class="line"></span><br><span class="line">    self.current_frame_idx += <span class="number">1</span></span><br><span class="line">    self.last_frame_id = first_node_id</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.trajectories, self.entire_trajectories</span><br></pre></td></tr></table></figure><p>完整版的请前往我的<a href="https://github.com/nightmaredimple/libmot">github</a>，下面是我以MOT17-10视频为例跑的结果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/online_mincostflow.png" alt="online_mincostflow"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/offline_mincostflow.png" alt="offline_mincostflow"></p><p>在线MOTA=0.676，离线的MOTA=0.594，离线的特征关联方法很简单，而在线的用的是Kalman+马氏距离。以上都是我自己根据自己理解写的，可能理解有误，也有可能代码实现有问题。</p><h2 id="5-基于深度学习的端到端数据关联"><a href="#5-基于深度学习的端到端数据关联" class="headerlink" title="5 基于深度学习的端到端数据关联"></a>5 基于深度学习的端到端数据关联</h2><p>近几年由于深度学习框架的兴起，端到端的训练和推理框架展现出一定的数据利用优势，而传统的数据关联算法基本都不满足可导可微的特性，因此出现了很多近似的端到端数据关联框架。这里由于篇幅有限，如果专栏和github反响还可以，后续我会考虑单独开一个基于深度学习的数据关联算法专题，现在我只简要介绍几类出现的框架。</p><p>我将近期出现的端到端数据关联框架大致可分为：</p><ul><li><strong>多特征输入，输出关联矩阵</strong></li></ul><p>这类框架只完成了数据关联的任务，即完成对多个目标的匹配，如PAMI2019中的DAN网络结构：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306185313082.png" alt="image-20200306185313082"></p><p>这种框架就是典型的输入历史帧多条跟踪轨迹的特征和当前帧多个特征序列，输出多对多的关联矩阵，这种方式是通过形式的拟合来近似数据关联。又比如ICCV2019的FAMNet：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306190345338.png" alt="image-20200306190345338"></p><p>这个框架将SOT和数据关联相集成。综上，这些方法虽然从形式上近似了数据关联算法，但是都要解决两个问题，一个是所有跟踪轨迹和观测的匹配交互，一个是如何过滤虚警和误检。</p><ul><li><strong>可微数据关联模块</strong></li></ul><p>这类框架就是讲传统不可微的数据关联模块改造成可微的模块，比如DeepMOT:</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306190848267.png" alt="image-20200306190848267"></p><p>这种方式基于匈牙利算法求解过程中的row-wise和colunm-wise操作，利用Bi-RNN完成全局的关联记忆，最后将关联矩阵通过连续的0~1的数据代替0-1匹配关系，从而实现可微。</p><ul><li><strong>基于RNN的数据关联预测</strong></li></ul><p>这种方式的特点在于，利用过去时间的跟踪记忆，基于不同行人的空间分布进行位置关系预测，比如ICCV2017的AMIR算法：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306191616736.png" alt="image-20200306191616736"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306191634767.png" alt="image-20200306191634767"></p><p>不过这类算法严格来说不能划分为数据关联类算法，这里我提出来肯定是有争议的~</p><ul><li><p><strong>基于图卷积的数据关联</strong></p><p>近几年图卷积网络在视觉领域开始热门起来，也有个别团队采用了这种方式，即利用图卷积网络的消息传递机制，模拟离线数据关联的网络图，这种方式的优点在于可以在线学习：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200306191259937.png" alt="image-20200306191259937"></p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><p>[1] SUN S, AKHTAR N, SONG H, et al. Deep affinity network for multiple object tracking[J]. IEEE transactions on pattern analysis and machine intelligence, 2019.</p><p>[2] CHU P, LING H. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 6172-6181.</p><p>[3] XU Y, BAN Y, ALAMEDA-PINEDA X, et al. DeepMOT: A Differentiable Framework for Training Multiple Object Trackers[J]. arXiv preprint arXiv:1906.06618, 2019.</p><p>[4] BRASó G, LEAL-TAIXé L. Learning a Neural Solver for Multiple Object Tracking[J]. arXiv preprint arXiv:1912.07515, 2019.</p><p>[5] SADEGHIAN A, ALAHI A, SAVARESE S. Tracking the untrackable: Learning to track multiple cues with long-term dependencies[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2017. 300-311.</p><h2><a href="#" class="headerlink"></a></h2><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到&lt;a href=&quot;https://github.com/nightmaredimple/libmot&quot;&gt;github&lt;/a&gt;。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="多目标跟踪" scheme="https://huangpiao.tech/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="数据关联" scheme="https://huangpiao.tech/tags/%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94/"/>
    
      <category term="最小代价流 - End2End" scheme="https://huangpiao.tech/tags/%E6%9C%80%E5%B0%8F%E4%BB%A3%E4%BB%B7%E6%B5%81-End2End/"/>
    
  </entry>
  
  <entry>
    <title>多目标跟踪中的数据关联代码实践(上)</title>
    <link href="https://huangpiao.tech/2020/03/04/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5(%E4%B8%8A)/"/>
    <id>https://huangpiao.tech/2020/03/04/多目标跟踪中的数据关联代码实践(上)/</id>
    <published>2020-03-04T15:30:00.000Z</published>
    <updated>2020-03-04T15:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到<a href="https://github.com/nightmaredimple/libmot">github</a>。</p></blockquote><a id="more"></a><h2 id="1-背景简介"><a href="#1-背景简介" class="headerlink" title="1 背景简介"></a>1 背景简介</h2><p>之前我们也说到，目前主流的MOT框架是DBT框架，这种框架的特点就是离不开数据关联算法，不论是对不同帧之间跟踪轨迹的关联还是跟踪轨迹和观测量的关联， 有数据关联才能更好的保证目标ID的连续性。数据关联算法以偶图匹配类为主，尤其是在离线跟踪中存在有大量基于图论的算法，比如：最小代价流、最大流、最小割、超图等等。而近两年也出现了一批端到端的数据关联算法，对于这一点呢，有几个很大的问题，一个是数据关联是一个n:m的问题，而网络对于输出的尺寸一般要求是固定的，如果构建关联矩阵是一个问题。另外，数据关联从理论上来讲是一个不可导的过程，其包含有一些排序的过程，如何处理这一点也很重要。</p><h2 id="2-基于IOU的贪婪匹配"><a href="#2-基于IOU的贪婪匹配" class="headerlink" title="2 基于IOU的贪婪匹配"></a>2 基于IOU的贪婪匹配</h2><h3 id="2-1-IOU-Tracker-amp-V-IOU-Tracker"><a href="#2-1-IOU-Tracker-amp-V-IOU-Tracker" class="headerlink" title="2.1 IOU Tracker &amp; V-IOU Tracker"></a>2.1 IOU Tracker &amp; V-IOU Tracker</h3><p>不得不说IOU Tracker和他的改进版V-IOU Tracker算是多目标跟踪算法中的一股清流，方法特别简单粗暴，对于检测质量很好的场景效果比较好。首先我们交代一下IOU的度量方式：</p><p>$$ IOU\left( {a,b} \right) = \frac{{Area\left( a \right) \cap Area\left( b \right)}}{{Area\left( a \right) \cup Area\left( b \right)}} $$<br>IOU Tracker的跟踪方式没有跟踪，只有数据关联，关联指标就是IOU，关联算法就是一种基于IOU的贪婪匹配算法：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200305162120637.png" alt="image-20200305162120637"></p><p>这里我们留到下一节将，我会利用不同的贪婪方式进行IOU匹配。那么V-IOU的改进就是，由于IOU Tracker仅仅是对观测量进行了关联，当目标丢失或者检测不到的时候，便无法重建轨迹，因此V-IOU加入了KCF单目标跟踪器来弥补这一漏洞，也很粗暴。。</p><h3 id="2-2-IOU-Matching"><a href="#2-2-IOU-Matching" class="headerlink" title="2.2 IOU Matching"></a>2.2 IOU Matching</h3><p>基于贪婪算法的数据关联的核心思想就是，不考虑整体最优，只考虑个体最优。这里我设计了两种贪婪方式，第一种Local IOU Matching,即依次为每条跟踪轨迹分配观测量，只要IOU满足条件即可，流程如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200305162851792.png" alt="image-20200305162851792" style="zoom:67%"></p><p>IOU Tracker就是采用的这种局部贪心方式，这种贪心策略的特点是每次为当前跟踪轨迹选择与之IOU最大的观测量。那么我们再提出一种全局的贪婪策略，即每次选择所有关联信息中IOU最大的匹配对，流程如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200305170556313.png" alt="image-20200305170556313" style="zoom:67%"></p><p>这两种贪婪方式的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GreedyAssignment</span><span class="params">(cost, threshold = None, method = <span class="string">'global'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Using iou matching to make linear assignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    cost : ndarray</span></span><br><span class="line"><span class="string">        A NxM matrix for costs between each track_ids with dection_ids</span></span><br><span class="line"><span class="string">    threshold: float</span></span><br><span class="line"><span class="string">        if cost &gt; threshold, then will not be considered</span></span><br><span class="line"><span class="string">    method: str</span></span><br><span class="line"><span class="string">        eg: global, local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    row_idx: List of matched tracks (&lt;=N,)</span></span><br><span class="line"><span class="string">        assigned tracklets' id</span></span><br><span class="line"><span class="string">    col_idx: List of matched dets (&lt;=M,)</span></span><br><span class="line"><span class="string">        assigned dets' id</span></span><br><span class="line"><span class="string">    unmatched_rows: List of unmatched tracks</span></span><br><span class="line"><span class="string">        unassigned tracklets' id</span></span><br><span class="line"><span class="string">    unmatched_cols: List of unmatched dets</span></span><br><span class="line"><span class="string">        unassigned dets' id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cost_c = np.atleast_2d(cost)</span><br><span class="line">    sz = cost_c.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        threshold = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    row_idx = []</span><br><span class="line">    col_idx = []</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'global'</span>:</span><br><span class="line">        vector_in = list(range(sz[<span class="number">0</span>]))</span><br><span class="line">        vector_out = list(range(sz[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">while</span> min(len(vector_in), len(vector_out)) &gt; <span class="number">0</span>:</span><br><span class="line">            v = cost_c[np.ix_(vector_in, vector_out)]</span><br><span class="line">            min_cost = np.min(v)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> min_cost &lt;= threshold:</span><br><span class="line">                place = np.where(v == min_cost)</span><br><span class="line">                row_idx.append(vector_in[place[<span class="number">0</span>][<span class="number">0</span>]])</span><br><span class="line">                col_idx.append(vector_out[place[<span class="number">1</span>][<span class="number">0</span>]])</span><br><span class="line">                <span class="keyword">del</span> vector_in[place[<span class="number">0</span>][<span class="number">0</span>]]</span><br><span class="line">                <span class="keyword">del</span> vector_out[place[<span class="number">1</span>][<span class="number">0</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vector_in = []</span><br><span class="line">        vector_out = list(range(sz[<span class="number">1</span>]))</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> min(sz[<span class="number">0</span>] - len(vector_in), len(vector_out)) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> index &gt;= sz[<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            place = np.argmin(cost_c[np.ix_([index], vector_out)])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cost_c[index, vector_out[place]] &lt;= threshold:</span><br><span class="line">                row_idx.append(index)</span><br><span class="line">                col_idx.append(vector_out[place])</span><br><span class="line">                <span class="keyword">del</span> vector_out[place]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                vector_in.append(index)</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        vector_in += list(range(index, sz[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(row_idx), np.array(col_idx), np.array(vector_in), np.array(vector_out)</span><br></pre></td></tr></table></figure><p>下面我们先看看两种方式的跟踪效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200305174522946.png" alt="image-20200305174522946"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/image-20200305174641634.png" alt="image-20200305174641634"></p><p>可以看到跟踪效果并不是很差，那么我们观察二者定量的结果则为：</p><p>Local: MOTA=0.77, IDF1=0.83</p><p>Global: MOTA=0.79, IDF1=0.88</p><p>相比之下全局的贪心策略比局部的要好</p><h2 id="3-线性分配"><a href="#3-线性分配" class="headerlink" title="3 线性分配"></a>3 线性分配</h2><h3 id="3-1-匈牙利算法和KM算法简介"><a href="#3-1-匈牙利算法和KM算法简介" class="headerlink" title="3.1 匈牙利算法和KM算法简介"></a>3.1 匈牙利算法和KM算法简介</h3><p>线性分配问题也叫指派问题，通常的线性分配任务是给定N个workers和N个tasks，结合相应的N×N的代价矩阵，就能得到匹配组合。其模型如下：</p><p>$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} = 1\\ \sum\limits_{j = 1}^N {{x_{ij}}} = 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$<br>上述模型有个问题，即要求workers和tasks的数量对等，然而在MOT问题中待匹配的跟踪轨迹和检测数量大概率不相同，而且我们经常还会设定阈值来限制匹配。</p><p>匈牙利算法是专门用来求解指派问题的算法，并且通常用于求解二分图最大匹配的。也就是说我们需要先利用规则判断边是否连接，然后匹配，因此不会出现非边节点存在匹配，只有可能出现剩余未匹配：</p><p>$$ \begin{array}{l} max = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^M {{L_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} \le 1\\ \sum\limits_{j = 1}^M {{x_{ij}}} \le 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$<br>匈牙利算法的问题在于一旦确定边之后就没有相对优劣了，所以我们这里介绍带权二分图匹配KM，顾名思义，就是求最小代价，只不过是不对等匹配：</p><p>$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^M {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} {\rm{ = }}1\\ \sum\limits_{j = 1}^M {{x_{ij}}} \le 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$<br>这里我们可以看到有一个维度的和要求是1，原因就是必须要保证有一个维度满分配，不然就会直接不建立连接了。</p><h3 id="3-2-实验对比"><a href="#3-2-实验对比" class="headerlink" title="3.2 实验对比"></a>3.2 实验对比</h3><p>这里我们借用<code>scipy</code>工具箱实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">inf_cost = <span class="number">1e+5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LinearAssignment</span><span class="params">(cost, threshold = None, method = <span class="string">'KM'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Using Hungarian or KM algorithm to make linear assignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    cost : ndarray</span></span><br><span class="line"><span class="string">        A NxM matrix for costs between each track_ids with dection_ids</span></span><br><span class="line"><span class="string">    threshold: float</span></span><br><span class="line"><span class="string">        if cost &gt; threshold, then will not be considered</span></span><br><span class="line"><span class="string">    method : str</span></span><br><span class="line"><span class="string">        'KM': weighted assignment</span></span><br><span class="line"><span class="string">        'Hungarian': 01 assignment</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    row_idx: List of matched tracks (&lt;=N,)</span></span><br><span class="line"><span class="string">        assigned tracklets' id</span></span><br><span class="line"><span class="string">    col_idx: List of matched dets (&lt;=M,)</span></span><br><span class="line"><span class="string">        assigned dets' id</span></span><br><span class="line"><span class="string">    unmatched_rows: List of unmatched tracks</span></span><br><span class="line"><span class="string">        unassigned tracklets' id</span></span><br><span class="line"><span class="string">    unmatched_cols: List of unmatched dets</span></span><br><span class="line"><span class="string">        unassigned dets' id</span></span><br><span class="line"><span class="string">    min_cost: float</span></span><br><span class="line"><span class="string">        cost of assignments</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cost_c = deepcopy(np.atleast_2d(cost))</span><br><span class="line">    sz = cost_c.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        cost_c = np.where(cost_c &gt; threshold, inf_cost, cost_c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'Hungarian'</span>:</span><br><span class="line">        t = threshold <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> inf_cost</span><br><span class="line">        cost_c = np.where(cost_c &lt; t, <span class="number">0</span>, cost_c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># linear assignment</span></span><br><span class="line">    row_ind, col_ind = linear_sum_assignment(cost_c)</span><br><span class="line">    <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        t = inf_cost - <span class="number">1</span> <span class="keyword">if</span> threshold == inf_cost <span class="keyword">else</span> threshold</span><br><span class="line">        mask = cost_c[row_ind, col_ind] &lt;= t</span><br><span class="line">        row_idx = row_ind[mask]</span><br><span class="line">        col_idx = col_ind[mask]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        row_idx, col_idx = row_ind, col_ind</span><br><span class="line"></span><br><span class="line">    unmatched_rows = np.array(list(set(range(sz[<span class="number">0</span>])) - set(row_idx)))</span><br><span class="line">    unmatched_cols = np.array(list(set(range(sz[<span class="number">1</span>])) - set(col_idx)))</span><br><span class="line"></span><br><span class="line">    min_cost = cost[row_idx, col_idx].sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> row_idx, col_idx, np.sort(unmatched_rows), np.sort(unmatched_cols), min_cost</span><br></pre></td></tr></table></figure><p>同样地，为了更好地对比，我们以IOU为代价指标，分别以匈牙利算法和KM算法为数据关联算法进行实验，有意思的是对于MOT-04-SDP视频而言，二者并没有太大区别，整体效果跟Global IOU Assignment一致。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200304/linear_assignment.png" alt="linear_assignment"></p><p>以上代码我都放到了<a href="https://github.com/nightmaredimple/libmot">github</a>。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到&lt;a href=&quot;https://github.com/nightmaredimple/libmot&quot;&gt;github&lt;/a&gt;。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="多目标跟踪" scheme="https://huangpiao.tech/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="数据关联" scheme="https://huangpiao.tech/tags/%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94/"/>
    
      <category term="iou track - 匈牙利算法 - KM算法" scheme="https://huangpiao.tech/tags/iou-track-%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95-KM%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Kalman滤波在MOT中的应用(三)——实践篇</title>
    <link href="https://huangpiao.tech/2020/03/01/Kalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%B8%89)%E2%80%94%E2%80%94%E5%AE%9E%E8%B7%B5%E7%AF%87/"/>
    <id>https://huangpiao.tech/2020/03/01/Kalman滤波在MOT中的应用(三)——实践篇/</id>
    <published>2020-03-01T09:30:00.000Z</published>
    <updated>2020-03-01T09:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以代码实践进行讲解。下文中所有的代码都会开源在<a href="https://github.com/nightmaredimple/libmot。">https://github.com/nightmaredimple/libmot。</a></p></blockquote><a id="more"></a><h2 id="1Kalman-Filter"><a href="#1Kalman-Filter" class="headerlink" title="1Kalman Filter"></a>1Kalman Filter</h2><p>本章将结合Kalman理论部分进行讲述，Kalman滤波器主要分为预测和更新两个阶段，在这之前能，我们需要预先设定<strong>状态变量和观测变量维度、协方差矩阵、运动形式和转换矩阵</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim_x, dim_z, dim_u = <span class="number">0</span>, x = None, P = None,</span></span></span><br><span class="line"><span class="function"><span class="params">             Q = None, B = None, F = None, H = None, R = None)</span>:</span></span><br><span class="line">    <span class="string">"""Kalman Filter</span></span><br><span class="line"><span class="string">        Refer to http:/github.com/rlabbe/filterpy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Method</span></span><br><span class="line"><span class="string">        -----------------------------------------</span></span><br><span class="line"><span class="string">         Predict        |        Update</span></span><br><span class="line"><span class="string">        -----------------------------------------</span></span><br><span class="line"><span class="string">                        |  K = PH^T(HPH^T + R)^-1</span></span><br><span class="line"><span class="string">        x = Fx + Bu     |  y = z - Hx</span></span><br><span class="line"><span class="string">        P = FPF^T + Q   |  x = x + Ky</span></span><br><span class="line"><span class="string">                        |  P = (1 - KH)P</span></span><br><span class="line"><span class="string">        -----------------------------------------</span></span><br><span class="line"><span class="string">        note: In update unit, here is a more numerically stable way: P = (I-KH)P(I-KH)' + KRK'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        dim_x: int</span></span><br><span class="line"><span class="string">            dims of state variables, eg:(x,y,vx,vy)-&gt;4</span></span><br><span class="line"><span class="string">        dim_z: int</span></span><br><span class="line"><span class="string">            dims of observation variables, eg:(x,y)-&gt;2</span></span><br><span class="line"><span class="string">        dim_u: int</span></span><br><span class="line"><span class="string">            dims of control variables,eg: a-&gt;1</span></span><br><span class="line"><span class="string">            p = p + vt + 0.5at^2</span></span><br><span class="line"><span class="string">            v = v + at</span></span><br><span class="line"><span class="string">            =&gt;[p;v] = [1,t;0,1][p;v] + [0.5t^2;t]a</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> dim_x &gt;= <span class="number">1</span>, <span class="string">'dim_x must be 1 or greater'</span></span><br><span class="line">    <span class="keyword">assert</span> dim_z &gt;= <span class="number">1</span>, <span class="string">'dim_z must be 1 or greater'</span></span><br><span class="line">    <span class="keyword">assert</span> dim_u &gt;= <span class="number">0</span>, <span class="string">'dim_u must be 0 or greater'</span></span><br><span class="line"></span><br><span class="line">    self.dim_x = dim_x</span><br><span class="line">    self.dim_z = dim_z</span><br><span class="line">    self.dim_u = dim_u</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialization</span></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    self.x = np.zeros((dim_x, <span class="number">1</span>)) <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> x      <span class="comment"># state</span></span><br><span class="line">    self.P = np.eye(dim_x)  <span class="keyword">if</span> P <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> P            <span class="comment"># uncertainty covariance</span></span><br><span class="line">    self.Q = np.eye(dim_x)  <span class="keyword">if</span> Q <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> Q            <span class="comment"># process uncertainty for prediction</span></span><br><span class="line">    self.B = <span class="keyword">None</span> <span class="keyword">if</span> B <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> B                      <span class="comment"># control transition matrix</span></span><br><span class="line">    self.F = np.eye(dim_x)  <span class="keyword">if</span> F <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> F            <span class="comment"># state transition matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    self.H = np.zeros((dim_z, dim_x)) <span class="keyword">if</span> H <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> H  <span class="comment"># Measurement function z=Hx</span></span><br><span class="line">    self.R = np.eye(dim_z)  <span class="keyword">if</span> R <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> R            <span class="comment"># observation uncertainty</span></span><br><span class="line">    self._alpha_sq = <span class="number">1.</span>                              <span class="comment"># fading memory control</span></span><br><span class="line">    self.z = np.array([[<span class="keyword">None</span>] * self.dim_z]).T       <span class="comment"># observation</span></span><br><span class="line">    self.K = np.zeros((dim_x, dim_z))                <span class="comment"># kalman gain</span></span><br><span class="line">    self.y = np.zeros((dim_z, <span class="number">1</span>))                    <span class="comment"># estimation error</span></span><br><span class="line">    self.S = np.zeros((dim_z, dim_z))                <span class="comment"># system uncertainty, S = HPH^T + R</span></span><br><span class="line">    self.SI = np.zeros((dim_z, dim_z))               <span class="comment"># inverse system uncertainty, SI = S^-1</span></span><br><span class="line"></span><br><span class="line">    self.inv = np.linalg.inv</span><br><span class="line">    self._mahalanobis = <span class="keyword">None</span>                         <span class="comment"># Mahalanobis distance of measurement</span></span><br><span class="line">    self.latest_state = <span class="string">'init'</span>                       <span class="comment"># last process name</span></span><br></pre></td></tr></table></figure><p>上述即对Kalman滤波器中各个参数的初始化，一般各个协方差矩阵都会初始化为单位矩阵，因此具体的矩阵初始化还需要针对特定场景设计，会在下一章介绍。</p><p>然后进入预测环节，这里我们为了保证通用性，引入了遗忘系数$\alpha$，其作用在于调节对过往信息的依赖程度，$\alpha$越大对历史信息的依赖越小。</p><p>$$ \begin{array}{l}x = Fx + Bu\\P = \alpha FxF^T + Q\end{array} $$<br>相应的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, u = None, B = None, F = None, Q = None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predict next state (prior) using the Kalman filter state propagation equations:</span></span><br><span class="line"><span class="string">                         x = Fx + Bu</span></span><br><span class="line"><span class="string">                         P = fading_memory*FPF^T + Q</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    u : ndarray</span></span><br><span class="line"><span class="string">        Optional control vector. If not `None`, it is multiplied by B</span></span><br><span class="line"><span class="string">        to create the control input into the system.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    B : ndarray of (dim_x, dim_z), or None</span></span><br><span class="line"><span class="string">        Optional control transition matrix; a value of None</span></span><br><span class="line"><span class="string">        will cause the filter to use `self.B`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    F : ndarray of (dim_x, dim_x), or None</span></span><br><span class="line"><span class="string">        Optional state transition matrix; a value of None</span></span><br><span class="line"><span class="string">        will cause the filter to use `self.F`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Q : ndarray of (dim_x, dim_x), scalar, or None</span></span><br><span class="line"><span class="string">        Optional process noise matrix; a value of None will cause the</span></span><br><span class="line"><span class="string">        filter to use `self.Q`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> B <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        B = self.B</span><br><span class="line">    <span class="keyword">if</span> F <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        F = self.F</span><br><span class="line">    <span class="keyword">if</span> Q <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        Q = self.Q</span><br><span class="line">    <span class="keyword">elif</span> np.isscalar(Q):</span><br><span class="line">        Q = np.eye(self.dim_x) * Q</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x = Fx + Bu</span></span><br><span class="line">    <span class="keyword">if</span> B <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> u <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        self.x = F @ self.x + B @ u</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.x = F @ self.x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># P = fading_memory*FPF' + Q</span></span><br><span class="line">    self.P = self._alpha_sq * (F @ self.P @ F.T) + Q</span><br><span class="line">    self.latest_state = <span class="string">'predict'</span></span><br></pre></td></tr></table></figure><p>而对于更新阶段，有：</p><p>$$ \left\{ \begin{array}{l} K = PH^T/\left( {HPH^T + R} \right)\\ x = Hx + K\left( {z - Hx} \right)\\ P = (1 - KH)P \end{array} \right. $$<br>不过在实际工程应用中通常会做一些微调：</p><p>$$ P = \left( {1 - KH} \right)P{\left( {1 - KH} \right)^T} + KR{K^T} $$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, z, R = None, H = None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update Process, add a new measurement (z) to the Kalman filter.</span></span><br><span class="line"><span class="string">                K = PH^T(HPH^T + R)^-1</span></span><br><span class="line"><span class="string">                y = z - Hx</span></span><br><span class="line"><span class="string">                x = x + Ky</span></span><br><span class="line"><span class="string">                P = (1 - KH)P or P = (I-KH)P(I-KH)' + KRK'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If z is None, nothing is computed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    z : (dim_z, 1): array_like</span></span><br><span class="line"><span class="string">        measurement for this update. z can be a scalar if dim_z is 1,</span></span><br><span class="line"><span class="string">        otherwise it must be convertible to a column vector.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    R : ndarray, scalar, or None</span></span><br><span class="line"><span class="string">        Optionally provide R to override the measurement noise for this</span></span><br><span class="line"><span class="string">        one call, otherwise  self.R will be used.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    H : ndarray, or None</span></span><br><span class="line"><span class="string">        Optionally provide H to override the measurement function for this</span></span><br><span class="line"><span class="string">        one call, otherwise self.H will be used.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> z <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        self.z = np.array([[<span class="keyword">None</span>] * self.dim_z]).T</span><br><span class="line">        self.y = np.zeros((self.dim_z, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    z = reshape_z(z, self.dim_z, self.x.ndim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> R <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        R = self.R</span><br><span class="line">    <span class="keyword">elif</span> np.isscalar(R):</span><br><span class="line">        R = np.eye(self.dim_z) * R</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> H <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        H = self.H</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.latest_state == <span class="string">'predict'</span>:</span><br><span class="line">        <span class="comment"># common subexpression for speed</span></span><br><span class="line">        PHT = self.P @ H.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># S = HPH' + R</span></span><br><span class="line">        <span class="comment"># project system uncertainty into measurement space</span></span><br><span class="line">        self.S = H @ PHT + R</span><br><span class="line"></span><br><span class="line">        self.SI = self.inv(self.S)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># K = PH'inv(S)</span></span><br><span class="line">        <span class="comment"># map system uncertainty into kalman gain</span></span><br><span class="line">        self.K = PHT @ self.SI</span><br><span class="line"></span><br><span class="line">        <span class="comment"># P = (I-KH)P(I-KH)' + KRK'</span></span><br><span class="line">        <span class="comment"># This is more numerically stable and works for non-optimal K vs</span></span><br><span class="line">        <span class="comment"># the equation P = (I-KH)P usually seen in the literature.</span></span><br><span class="line">        I_KH = np.eye(self.dim_x) - self.K @ H</span><br><span class="line">        self.P = I_KH @ self.P @ I_KH.T + self.K @ R @ self.K.T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># y = z - Hx</span></span><br><span class="line">    <span class="comment"># error (residual) between measurement and prediction</span></span><br><span class="line">    self.y = z - H @ self.x</span><br><span class="line"></span><br><span class="line">    self._mahalanobis = math.sqrt(float(self.y.T @ self.SI @ self.y))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x = x + Ky</span></span><br><span class="line">    <span class="comment"># predict new x with residual scaled by the kalman gain</span></span><br><span class="line"></span><br><span class="line">    self.x = self.x + self.K @ self.y</span><br><span class="line">    self.latest_state = <span class="string">'update'</span></span><br></pre></td></tr></table></figure><p>其中我们可以注意到马氏距离的计算方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self._mahalanobis = math.sqrt(float(self.y.T @self.SI @ self.y))</span><br></pre></td></tr></table></figure><p>另外，我们保存上个阶段的状态，其主要作用在于，第一防止多次更新带来的重复运算，第二防止前一次更新对下一次更新参数造成影响。</p><h2 id="2KalmanTracker"><a href="#2KalmanTracker" class="headerlink" title="2KalmanTracker"></a>2KalmanTracker</h2><p>那么对于Kalman滤波器的跟踪器设计，我们这里直接借鉴<a href="[https://huangpiao.tech/2020/02/29/Kalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%BA%8C">DeepSort的参数设计方案</a>%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E7%AF%87/](<a href="https://huangpiao.tech/2020/02/29/Kalman滤波在MOT中的应用(二)——应用篇/),因此基于KalmanFilter的设计，需要在初始化阶段之后修改对应的参数：">https://huangpiao.tech/2020/02/29/Kalman滤波在MOT中的应用(二)——应用篇/),因此基于KalmanFilter的设计，需要在初始化阶段之后修改对应的参数：</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, box, fading_memory = <span class="number">1.0</span>, dt = <span class="number">1.0</span>, std_weight_position = <span class="number">0.05</span>, std_weight_velocity = <span class="number">0.00625</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Tracking bounding boxes in assumption of uniform linear motion</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The 8-dimensional state space</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        x, y, a, h, vx, vy, va, vh</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    contains the bounding box center position (x, y), aspect ratio a, height h,</span></span><br><span class="line"><span class="string">    and their respective velocities.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Object motion follows a constant velocity model. The bounding box location</span></span><br><span class="line"><span class="string">    (x, y, a, h) is taken as direct observation of the state space (linear</span></span><br><span class="line"><span class="string">    observation model).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    --------------</span></span><br><span class="line"><span class="string">    box: array like</span></span><br><span class="line"><span class="string">        1x4 matrix of boxes (x,y,w,h)</span></span><br><span class="line"><span class="string">    fading_memory: float</span></span><br><span class="line"><span class="string">        larger means fading more</span></span><br><span class="line"><span class="string">    dt: float</span></span><br><span class="line"><span class="string">        time step for each update</span></span><br><span class="line"><span class="string">    std_weight_position: float</span></span><br><span class="line"><span class="string">        std for position</span></span><br><span class="line"><span class="string">    std_weight_velocity:flaot</span></span><br><span class="line"><span class="string">        std for velovity</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    box = np.atleast_2d(box) <span class="comment">#1x4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialization</span></span><br><span class="line">    self.x_dim = <span class="number">8</span></span><br><span class="line">    self.z_dim = <span class="number">4</span></span><br><span class="line">    self.dt = dt</span><br><span class="line"></span><br><span class="line">    state = self.box2state(box)</span><br><span class="line">    state = np.r_[state, np.zeros_like(state)] <span class="comment">#8x1</span></span><br><span class="line"></span><br><span class="line">    self.F = np.eye(self.x_dim, self.x_dim)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.z_dim):</span><br><span class="line">        self.F[i, self.z_dim + i] = self.dt</span><br><span class="line"></span><br><span class="line">    self._std_weight_position = std_weight_position</span><br><span class="line">    self._std_weight_velocity = std_weight_velocity</span><br><span class="line"></span><br><span class="line">    std = [</span><br><span class="line">        <span class="number">2</span> * self._std_weight_position * state[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">2</span> * self._std_weight_position * state[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-2</span>,</span><br><span class="line">        <span class="number">2</span> * self._std_weight_position * state[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">10</span> * self._std_weight_velocity * state[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">10</span> * self._std_weight_velocity * state[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-5</span>,</span><br><span class="line">        <span class="number">10</span> * self._std_weight_velocity * state[<span class="number">3</span>][<span class="number">0</span>]]</span><br><span class="line">    covariance = np.diag(np.square(std))</span><br><span class="line"></span><br><span class="line">    self.H = np.eye(self.z_dim, self.x_dim)</span><br><span class="line"></span><br><span class="line">    self.kf = KalmanFilter(dim_x = self.x_dim, dim_z = self.z_dim , x = state,</span><br><span class="line">                           P = covariance, F = self.F, H = self.H)</span><br><span class="line">    self.kf.alpha = fading_memory</span><br><span class="line">    self._x = self.kf.x</span><br><span class="line">    self._mahalanobis = self.kf.mahalanobis</span><br></pre></td></tr></table></figure><p>其中，DeepSort对于Q和R的设计中，为了保证各自对目标尺度更加敏感，采用了自适应的方式，即同样的公式应用在每一次跟踪：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Predict next state (prior) using the Kalman filter state propagation equations:</span></span><br><span class="line"><span class="string">                         x = Fx + Bu</span></span><br><span class="line"><span class="string">                         P = fading_memory*FPF^T + Q</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    std_pos = [</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-2</span>,</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>]]</span><br><span class="line">    std_vel = [</span><br><span class="line">        self._std_weight_velocity * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        self._std_weight_velocity * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-5</span>,</span><br><span class="line">        self._std_weight_velocity * self.kf.x[<span class="number">3</span>][<span class="number">0</span>]]</span><br><span class="line">    motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))</span><br><span class="line"></span><br><span class="line">    self.kf.predict(Q = motion_cov)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, measurement)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update Process, add a new measurement (z) to the Kalman filter.</span></span><br><span class="line"><span class="string">                K = PH^T(HPH^T + R)^-1</span></span><br><span class="line"><span class="string">                y = z - Hx</span></span><br><span class="line"><span class="string">                x = x + Ky</span></span><br><span class="line"><span class="string">                P = (1 - KH)P or P = (I-KH)P(I-KH)' + KRK'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    --------------</span></span><br><span class="line"><span class="string">    measurement: array like</span></span><br><span class="line"><span class="string">        1x4 matrix of boxes (x,y,w,h)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    box = np.atleast_2d(measurement)  <span class="comment"># 1x4</span></span><br><span class="line">    z = self.box2state(box)   <span class="comment"># 4x1</span></span><br><span class="line"></span><br><span class="line">    std = [</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-1</span>,</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>]]</span><br><span class="line">    innovation_cov = np.diag(np.square(std))</span><br><span class="line"></span><br><span class="line">    self.kf.update(z = z, R = innovation_cov)</span><br></pre></td></tr></table></figure><p>另外，为了方便多个观测量对Kalman滤波器的多次更新，我加入了一个批处理模块，主要作用有：防止重复更新造成的变量内存改变、消除重复更新参数部分.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_filter</span><span class="params">(self, zs)</span>:</span></span><br><span class="line">    <span class="string">""" Batch processes a sequences of measurements.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    zs : list-like</span></span><br><span class="line"><span class="string">        list of measurements at each time step `self.dt`. Missing</span></span><br><span class="line"><span class="string">        measurements must be represented by `None`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    means : np.array((n,dim_x,1))</span></span><br><span class="line"><span class="string">        array of the state for each time step after the update. Each entry</span></span><br><span class="line"><span class="string">        is an np.array. In other words `means[k,:]` is the state at step</span></span><br><span class="line"><span class="string">        `k`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    covariance : np.array((n,dim_x,dim_x))</span></span><br><span class="line"><span class="string">        array of the covariances for each time step after the update.</span></span><br><span class="line"><span class="string">        In other words `covariance[k,:,:]` is the covariance at step `k`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    mahalanobis: np.array((n,1))</span></span><br><span class="line"><span class="string">        array of the mahalanobises for each time step during the update</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    zs = np.atleast_2d(zs)</span><br><span class="line">    n = zs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mean estimates from Kalman Filter</span></span><br><span class="line">    x_copy = deepcopy(self.kf.x)</span><br><span class="line">    means = np.zeros((n, self.x_dim, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># state covariances from Kalman Filter</span></span><br><span class="line">    covariances = np.zeros((n, self.x_dim, self.x_dim))</span><br><span class="line">    mahalanobis = np.zeros(n)</span><br><span class="line"></span><br><span class="line">    std = [</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>],</span><br><span class="line">        <span class="number">1e-1</span>,</span><br><span class="line">        self._std_weight_position * self.kf.x[<span class="number">3</span>][<span class="number">0</span>]]</span><br><span class="line">    innovation_cov = np.diag(np.square(std))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i, z <span class="keyword">in</span> enumerate(zs):</span><br><span class="line">            self.kf.x = deepcopy(x_copy)</span><br><span class="line"></span><br><span class="line">            measurement = self.box2state(z)  <span class="comment"># 4x1</span></span><br><span class="line">            self.kf.update(z = measurement, R = innovation_cov)</span><br><span class="line">            means[i, :] = deepcopy(self.kf.x)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                covariances = np.tile(self.kf.P[np.newaxis, :, :],(n,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            mahalanobis[i] = deepcopy(self.kf._mahalanobis)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (means, covariances, mahalanobis)</span><br></pre></td></tr></table></figure><h2 id="3Example"><a href="#3Example" class="headerlink" title="3Example"></a>3Example</h2><p>前两章将Kalman滤波器和跟踪器的代码层面都设计好了，接下来我们以MOT17-10数据集为例进行跟踪实验。这里不采用DeepSort的方式，我自己简单搭建了一套流程：</p><p>Step1 我们先初始化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">track_len = <span class="number">655</span>               <span class="comment"># total tracking length</span></span><br><span class="line">thresh_det = <span class="number">0.75</span>             <span class="comment"># threshold for detection，this is for SDP detector</span></span><br><span class="line">thresh_track = chi2inv95[<span class="number">4</span>]   <span class="comment"># threshold for data association, specifically the mahalanobis distance</span></span><br><span class="line">fading_memory = <span class="number">1.14</span>          <span class="comment"># fading memory for prediction</span></span><br><span class="line">dt = <span class="number">0.15</span>                     <span class="comment"># time step for prediction</span></span><br><span class="line">std_weight_position = <span class="number">0.04</span>    <span class="comment"># std of position prediction</span></span><br><span class="line">std_weight_velocity = <span class="number">0.05</span>    <span class="comment"># std of velocity prediction</span></span><br><span class="line">patience = <span class="number">2</span>                  <span class="comment"># patience for waiting reconnection</span></span><br><span class="line">min_len = <span class="number">4</span>                   <span class="comment"># mininum length of active trajectory</span></span><br></pre></td></tr></table></figure><p>Step2 读取detection和groundtruth文件，筛选出满足行人类别和检测置信度阈值的目标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prefetch</span></span><br><span class="line">gt = np.genfromtxt(dir_path + <span class="string">'gt\\gt.txt'</span>, delimiter = <span class="string">','</span>)</span><br><span class="line">gt = gt[(gt[:, <span class="number">0</span>] &lt; track_len)&amp;(gt[:, <span class="number">6</span>] == <span class="number">1</span>) , :]</span><br><span class="line">mask = (gt[:, <span class="number">7</span>] == <span class="number">1</span>) | (gt[:, <span class="number">7</span>] == <span class="number">2</span>) | (gt[:, <span class="number">7</span>] == <span class="number">7</span>)</span><br><span class="line">gt = gt[mask].astype(np.int32)</span><br><span class="line"></span><br><span class="line">dets = np.genfromtxt(dir_path + <span class="string">'det\\det.txt'</span>, delimiter = <span class="string">','</span>)</span><br><span class="line">dets = dets[(dets[:, <span class="number">0</span>] &lt; track_len)&amp;(dets[:, <span class="number">6</span>] &gt; thresh_det) , :]</span><br><span class="line">dets = dets.astype(np.int32)</span><br></pre></td></tr></table></figure><p>Step3 对每个目标新建一个Kalman滤波器，逐一进行预测、更新、数据关联。其中如果数据关联失败的话，对于匹配失败的跟踪轨迹，在一定时间内，我们依旧允许其预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># begin track</span></span><br><span class="line">total_id = len(dets[dets[:, <span class="number">0</span>] == <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i, det <span class="keyword">in</span> enumerate(dets[dets[:, <span class="number">0</span>] == <span class="number">1</span>]):</span><br><span class="line">    tracks.append(&#123;<span class="string">'id'</span> : i + <span class="number">1</span>, <span class="string">'pause'</span>: <span class="number">0</span>,</span><br><span class="line">                   <span class="string">'kf'</span>: LinearMotion(det[<span class="number">2</span>:<span class="number">6</span>], fading_memory = fading_memory, \</span><br><span class="line">                                      dt = dt, std_weight_position = std_weight_position, \</span><br><span class="line">                                      std_weight_velocity = std_weight_velocity)</span><br><span class="line">                   &#125;)</span><br><span class="line">    record.append([<span class="number">1</span>, i+<span class="number">1</span>, det[<span class="number">2</span>], det[<span class="number">3</span>], det[<span class="number">4</span>], det[<span class="number">5</span>], <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, track_len):</span><br><span class="line">    det = dets[dets[:, <span class="number">0</span>] == i]</span><br><span class="line">    cost = (thresh_track + <span class="number">1</span>)*np.ones((len(tracks), len(det)))</span><br><span class="line">    save = [<span class="keyword">None</span>] * len(tracks)</span><br><span class="line"></span><br><span class="line">    track_copy = deepcopy(tracks)</span><br><span class="line">    track_boxes = np.zeros((len(tracks), <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    <span class="keyword">for</span> j, track <span class="keyword">in</span> enumerate(tracks):</span><br><span class="line">        track[<span class="string">'kf'</span>].predict()</span><br><span class="line">        track_boxes[j] = track[<span class="string">'kf'</span>].x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iou_blocking</span></span><br><span class="line">    <span class="keyword">if</span> len(tracks) &gt; <span class="number">0</span> <span class="keyword">and</span> len(det) &gt; <span class="number">0</span>:</span><br><span class="line">        keep = iou_blocking(track_boxes, det[:, <span class="number">2</span>:<span class="number">6</span>], <span class="number">2</span>*track_boxes[:, <span class="number">2</span>:])</span><br><span class="line">        xs = np.zeros((det.shape[<span class="number">0</span>], tracks[<span class="number">0</span>][<span class="string">'kf'</span>].x_dim, <span class="number">1</span>))</span><br><span class="line">        Ps = np.zeros((det.shape[<span class="number">0</span>], tracks[<span class="number">0</span>][<span class="string">'kf'</span>].x_dim, tracks[<span class="number">0</span>][<span class="string">'kf'</span>].x_dim))</span><br><span class="line">        ds = np.zeros(det.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        <span class="keyword">for</span> j, track <span class="keyword">in</span> enumerate(tracks):</span><br><span class="line">            xs[keep[j], :, :], Ps[keep[j], :, :], ds[keep[j]] = track[<span class="string">'kf'</span>].batch_filter(det[keep[j], <span class="number">2</span>:<span class="number">6</span>])</span><br><span class="line">            save[j] = &#123;<span class="string">'xs'</span>: deepcopy(xs), <span class="string">'Ps'</span>: deepcopy(Ps)&#125;</span><br><span class="line">            cost[j, keep[j]] = ds[keep[j]]</span><br><span class="line">        <span class="comment"># data association</span></span><br><span class="line">        row_idx, col_idx, unmatched_rows, unmatched_cols, _ = LinearAssignment(cost, threshold=thresh_track, method = <span class="string">'KM'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        row_idx = []</span><br><span class="line">        col_idx = []</span><br><span class="line">        unmatched_rows = np.arange(len(tracks))</span><br><span class="line">        unmatched_cols = np.arange(len(det))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r, c <span class="keyword">in</span> zip(row_idx, col_idx):</span><br><span class="line"></span><br><span class="line">        tracks[r][<span class="string">'kf'</span>].kf.x = save[r][<span class="string">'xs'</span>][c, :, :]</span><br><span class="line">        tracks[r][<span class="string">'kf'</span>].kf.P = save[r][<span class="string">'Ps'</span>][c, :, :]</span><br><span class="line">        tracks[r][<span class="string">'pause'</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> np.flip(unmatched_rows, <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">if</span> tracks[r][<span class="string">'pause'</span>] &gt;= patience:</span><br><span class="line">            <span class="keyword">del</span> tracks[r]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tracks[r]= deepcopy(track_copy[r])</span><br><span class="line">            tracks[r][<span class="string">'kf'</span>].predict()</span><br><span class="line">            tracks[r][<span class="string">'pause'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> unmatched_cols:</span><br><span class="line">        tracks.append(&#123;<span class="string">'id'</span>: total_id + <span class="number">1</span>, <span class="string">'pause'</span>: <span class="number">0</span>,</span><br><span class="line">                       <span class="string">'kf'</span>: LinearMotion(det[c, <span class="number">2</span>:<span class="number">6</span>], fading_memory=fading_memory, \</span><br><span class="line">                                          dt=dt, std_weight_position=std_weight_position, \</span><br><span class="line">                                          std_weight_velocity=std_weight_velocity)</span><br><span class="line">                       &#125;)</span><br><span class="line">        total_id += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> track <span class="keyword">in</span> tracks:</span><br><span class="line">        <span class="keyword">if</span> track[<span class="string">'pause'</span>] == <span class="number">0</span>:</span><br><span class="line">            record.append([i, track[<span class="string">'id'</span>], track[<span class="string">'kf'</span>].x[<span class="number">0</span>], track[<span class="string">'kf'</span>].x[<span class="number">1</span>],</span><br><span class="line">                           track[<span class="string">'kf'</span>].x[<span class="number">2</span>], track[<span class="string">'kf'</span>].x[<span class="number">3</span>], <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            record.append([i, track[<span class="string">'id'</span>], track[<span class="string">'kf'</span>].x[<span class="number">0</span>], track[<span class="string">'kf'</span>].x[<span class="number">1</span>],</span><br><span class="line">                           track[<span class="string">'kf'</span>].x[<span class="number">2</span>], track[<span class="string">'kf'</span>].x[<span class="number">3</span>], <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">record = np.array(record)</span><br></pre></td></tr></table></figure><p>值得注意的是其中的iou_blocking部分，这个模块使我们基于iou mask改进升级的，原本的iou是用来删除iou&lt;0.3的关联边，现在我们可以放宽要求，将不在目标邻域的观测删除：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou_blocking</span><span class="params">(tracks, dets, region_shape)</span>:</span></span><br><span class="line">    <span class="string">"""Blocking regions for each tracks</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    tracks: 2-dim ndarray</span></span><br><span class="line"><span class="string">        Nx4 matrix of (x,y,w,h)</span></span><br><span class="line"><span class="string">    dets: 2-dim ndarray</span></span><br><span class="line"><span class="string">        Mx4 matrix of (x,y,w,h)</span></span><br><span class="line"><span class="string">    region_shape: Tuple(w,h) or array-like (Nx2)</span></span><br><span class="line"><span class="string">        region shape for each track</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ---------</span></span><br><span class="line"><span class="string">    blocks: ndarray of boolean(NxM)</span></span><br><span class="line"><span class="string">        block sets for each track,</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    tracks = np.atleast_2d(tracks)</span><br><span class="line">    dets = np.atleast_2d(dets)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(region_shape, tuple):</span><br><span class="line">        region_shape = np.atleast_2d(region_shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        region_shape = np.array([[region_shape[<span class="number">0</span>], region_shape[<span class="number">1</span>]]])</span><br><span class="line">        region_shape = np.tile(region_shape, (tracks.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    centers = tracks[:, :<span class="number">2</span>] + tracks[:, <span class="number">2</span>:]/<span class="number">2.</span></span><br><span class="line"></span><br><span class="line">    overlap = iou(np.c_[centers - region_shape/<span class="number">2.</span>, region_shape], dets)</span><br><span class="line">    keep = overlap &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure><p>Step4 我们将轨迹中有效长度较短的轨迹视为无效轨迹：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># post processure</span></span><br><span class="line">max_id = record[:, <span class="number">1</span>].flatten().max()</span><br><span class="line">new_record = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, max_id + <span class="number">1</span>):</span><br><span class="line">    temp = record[record[:, <span class="number">1</span>] == i]</span><br><span class="line">    index = int(temp[:, <span class="number">-1</span>].nonzero()[<span class="number">0</span>][<span class="number">-1</span>])</span><br><span class="line">    temp = temp[:(index+<span class="number">1</span>), :<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">if</span> len(temp) &gt; min_len <span class="keyword">or</span> temp[<span class="number">-1</span>, <span class="number">0</span>] == track_len <span class="keyword">or</span> temp[<span class="number">0</span>, <span class="number">0</span>] &gt; track_len - min_len - <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> new_record <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            new_record = np.r_[new_record, temp]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_record = temp</span><br></pre></td></tr></table></figure><p>上述过程呢，我们可以得到以下结果：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Detection</th><th style="text-align:center">MOTA↑</th><th style="text-align:center">MOTP↑</th><th style="text-align:center">IDF1↑</th><th style="text-align:center">ID Sw.↓</th></tr></thead><tbody><tr><td style="text-align:center">SDP</td><td style="text-align:center">0.675</td><td style="text-align:center">0.203</td><td style="text-align:center">0.518</td><td style="text-align:center">201</td></tr></tbody></table></div><p>可视化效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200301/image-20200301184848145.png" alt="image-20200301184848145"></p><p>可以看到，在检测质量较好时，跟踪效果也还不错，以上的代码我都放在了<a href="https://github.com/nightmaredimple/libmot，">https://github.com/nightmaredimple/libmot，</a></p><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1]WOJKE N, BEWLEY A, PAULUS D. Simple online and realtime tracking with a deep association metric[C]. in: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. 3645-3649.</p><p>[2]<a href="https://github.com/rlabbe/filterpy/tree/master/filterpy/">https://github.com/rlabbe/filterpy/tree/master/filterpy/</a></p><p>[3]<a href="https://github.com/nwojke/deep_sort">https://github.com/nwojke/deep_sort</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以代码实践进行讲解。下文中所有的代码都会开源在&lt;a href=&quot;https://github.com/nightmaredimple/libmot。&quot;&gt;https://github.com/nightmaredimple/libmot。&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="Kalman" scheme="https://huangpiao.tech/tags/Kalman/"/>
    
  </entry>
  
  <entry>
    <title>Kalman滤波在MOT中的应用(二)——应用篇</title>
    <link href="https://huangpiao.tech/2020/02/29/Kalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%BA%8C)%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E7%AF%87/"/>
    <id>https://huangpiao.tech/2020/02/29/Kalman滤波在MOT中的应用(二)——应用篇/</id>
    <published>2020-02-29T09:30:00.000Z</published>
    <updated>2020-02-29T09:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以经典应用为主。其中应用算法主要介绍Sort和Deepsort算法。</p></blockquote><a id="more"></a><p>Sort系列算法的原理不复杂，但是为近些年多目标跟踪的发展提供了很多的实验性baseline帮助，也帮助很多新人入门了。首先我们先谈谈Sort算法，这个算法实际上就是一个很直接的Kalman算法应用。正如我们上一讲所介绍的，Kalman滤波算法需要设计以下几个点：<strong>状态量x和观测量z的确定、各个协方差参数(P,Q,R)的确定、运动形式F和转换矩阵H的预设</strong>。</p><h2 id="1Sort"><a href="#1Sort" class="headerlink" title="1Sort"></a>1Sort</h2><ol><li><p>Sort对于状态量x的设定是一个七维向量：<br>$$ x = {\left[ {u,v,s,r,\dot u,\dot v,\dot s} \right]^T} $$<br>分别表示目标中心位置的x,y坐标，相对原始目标框大小的尺度s（可以看做面积尺度）和当前目标框的纵横比，最后三个则是横向，纵向，尺度的变化速率，其中速度部分初始化为0；</p></li><li><p>对于Detection based Tracking框架，由于提供了检测结果，因此相较于Kalman滤波器预测的状态，检测结果就充当了观测量的角色；</p></li><li><p>各个协方差参数的设定，从Sort提供的代码来看，应该是经验参数：</p><blockquote><p>self.kf.R[2:,2:] *= 10.</p><p>self.kf.P[4:,4:] *= 1000.</p><p>self.kf.P *= 10.</p><p>self.kf.Q[-1,-1] *= 0.01</p><p>self.kf.Q[4:,4:] *= 0.01</p></blockquote><p>可以看到，对于Kalman的各个协方差，初始状态为：<br>$$ \begin{array}{l} P = diag\left( {{{\left[ {\begin{array}{*{20}{c}} {10}&{10}&{10}&{10}&{1e4}&{1e4}&{1e4} \end{array}} \right]}^T}} \right)\\ Q = diag\left( {{{\left[ {\begin{array}{*{20}{c}} 1&1&1&1&{0.01}&{0.01}&{1e{\rm{ - }}4} \end{array}} \right]}^T}} \right)\\ R = diag\left( {{{\left[ {\begin{array}{*{20}{c}} 1&1&{10}&{10} \end{array}} \right]}^T}} \right) \end{array} $$<br>可以看到，作者主要是基于速度状态不确定性大于位置形状状态不确定性的依据，以及观测行人框的检测位置比形状置信度高的先验；</p></li><li><p>运动形式和转换矩阵的确定，无论是Sort还是Deepsort都是基于匀速运动的假设，即：<br>$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} u\\ v\\ s \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} u\\ v\\ s \end{array}} \right] + \left[ {\begin{array}{*{20}{c}} {\dot u}\\ {\dot v}\\ {\dot s} \end{array}} \right] \Rightarrow F = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&1&0&0\\ 0&1&0&0&0&1&0\\ 0&0&1&0&0&0&1\\ 0&0&0&1&0&0&0\\ 0&0&0&0&1&0&0\\ 0&0&0&0&0&1&0\\ 0&0&0&0&0&0&1 \end{array}} \right]\\ z = Hx \Rightarrow \left[ {\begin{array}{*{20}{c}} u\\ v\\ s\\ r \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&0&0&0\\ 0&1&0&0&0&0&0\\ 0&0&1&0&0&0&0\\ 0&0&0&1&0&0&0 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} u\\ v\\ s\\ r\\ {\dot u}\\ {\dot v}\\ {\dot s} \end{array}} \right] \end{array} $$</p></li><li><p>然而对于Kalman滤波器的更新环节而言，观测量如何选择也是一个很重要的部分，即对于n条跟踪轨迹而言，如果存在m个观测，如果分配的问题。通常我们采用匈牙利算法/KM算法先对Klaman滤波器预测阶段得到的运动估计和观测进行关联，代价矩阵为二者之间的IOU。匹配成功的则进行Kalman滤波器的更新，匹配失败的跟踪轨迹则视为丢失，匹配失败的观测量则视为新增轨迹；</p></li><li><p>综上就可以实现各个目标的运动估计和更新。其中对于丢失的轨迹，考虑到跟踪对象或者检测质量的影响，目标经常容易短暂消失，一般情况下我们是允许他参与到后续帧的匹配关联以继续跟踪的。然而Sort跟踪器主要依据的是运动信息，也比较简单，所以实际实现过程中只允许丢失一帧。</p></li></ol><h2 id="2Deepsort"><a href="#2Deepsort" class="headerlink" title="2Deepsort"></a>2Deepsort</h2><p>同样地，DeepSort对于Sort算法进行了改进，改进方向有：状态量的调整、协方差参数的调整、代价矩阵的确定方式。</p><ol><li><p>DeepSort对于状态量x的设定是一个八维向量：<br>$$ x = {\left[ {u,v,\gamma ,h,\dot u,\dot v,\dot \gamma ,\dot h} \right]^T} $$<br>分别表示目标中心位置的x,y坐标，当前目标框的纵横比和高，以及上述四个状态的速度变量。</p></li><li><p>对于Detection based Tracking框架，由于提供了检测结果，因此相较于Kalman滤波器预测的状态，检测结果就充当了观测量的角色；</p></li><li><p>各个协方差参数的设定，从DeepSort提供的代码来看，可以看到，对于Kalman的各个协方差，初始状态为：<br>$$ \left\{ \begin{array}{l} P = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {2{\sigma _p}h}&{2{\sigma _p}h}&{1e - 2}&{2{\sigma _p}h}&{10{\sigma _v}h}&{10{\sigma _v}h}&{1e - 5}&{10{\sigma _v}h} \end{array}} \right]}^T}} \right)^2}\\ Q = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {{\sigma _p}h}&{{\sigma _p}h}&{1e - 2}&{{\sigma _p}h}&{{\sigma _v}h}&{{\sigma _v}h}&{1e - 5}&{{\sigma _v}h} \end{array}} \right]}^T}} \right)^2}\\ R = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {{\sigma _p}h}&{{\sigma _p}h}&{1e - 1}&{{\sigma _p}h} \end{array}} \right]}^T}} \right)^2} \end{array} \right. $$<br>可以看到，作者引入了位置和速度的标准差这两个额外的参数，整体的设计来看，依旧是速度相对于位置形状不确定性要高，长宽比尽可能变化幅度小。</p></li><li><p>运动形式和转换矩阵的确定，无论是Sort还是Deepsort都是基于匀速运动的假设，不过Deepsort新增了运动的步长即：<br>$$ \begin{array}{l} F = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&{dt}&0&0&0\\ 0&1&0&0&0&{dt}&0&0\\ 0&0&1&0&0&0&{dt}&0\\ 0&0&0&1&0&0&0&{dt}\\ 0&0&0&0&1&0&0&0\\ 0&0&0&0&0&1&0&0\\ 0&0&0&0&0&0&1&0\\ 0&0&0&0&0&0&0&1 \end{array}} \right]\\ z = Hx \Rightarrow \left[ {\begin{array}{*{20}{c}} u\\ v\\ \gamma \\ h \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&0&0&0&0\\ 0&1&0&0&0&0&0&0\\ 0&0&1&0&0&0&0&0\\ 0&0&0&1&0&0&0&0 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} u\\ v\\ \gamma \\ h\\ {\dot u}\\ {\dot v}\\ {\dot \gamma }\\ {\dot h} \end{array}} \right] \end{array} $$</p></li><li><p>对于预测估计和观测量的关联，DeepSort引入了更多的设定，其中IOU的判定作为初筛标准，将不符合要求的关联删除，实际采用的是马氏距离和余弦距离的加权值。其中马氏距离，利用的是Kalman滤波器更新阶段的系统协方差：<br>$$ {d^{\left( 1 \right)}}\left( {i,j} \right) = {\left( {{d_j} - {y_i}} \right)^T}S_i^{ - 1}\left( {{d_j} - {y_i}} \right) $$<br>不过马氏距离的取值范围并没有上限，这不利于确定阈值，因此作者巧妙地利用马氏距离和卡方分布的联系，不同维度状态下马氏距离满足95%置信度的阈值如下：</p><p>| 维度 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |<br>| :—: | :—-: | :—-: | :—-: | :——: | :——: | :——: | :——: | :——: | :——: |<br>| 阈值 | 3.842 | 5.992 | 7.815 | 9.4877 | 11.070 | 12.592 | 14.067 | 15.507 | 16.919 |</p><p>这里实际比对的时候并不会考虑速度状态， 因此只有四维即9.4877。对于表观特征，作者训练了一个Re-ID网络用于特征提取，然后计算余弦距离，最后对两个距离加权。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200301112018392.png" alt="image-20200301112018392"></p><p>其中作者对于特征向量的处理采用了比较简陋的特征融合方式，即保存轨迹的历史特征，最多100个，每次计算余弦距离之前先将历史特征平均。</p></li><li><p>最后由于轨迹丢失会暂时保留，因此针对不同时效性的轨迹，采用了级联匹配的方式：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200301112223777.png" alt="image-20200301112223777"></p><p>只不过这个看起来可能比较不清楚，CSDN博客有个整理得比较好的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/2018111821025542.png" alt="img"></p></li></ol><p>整理而言呢，Sort系列算法原理比较简单，尤其是Sort算法，而Deepsort算法则是开创了Kalman滤波器+Re-ID这种模式的MOT算法先河。两种算法的速度也非常快，不过对于其论文里面所说的SOTA效果，感觉还是有一定的作弊，因为他们采用的是自己训练的Faster RCNN检测器，以及POI检测器。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200301134338439.png" alt="image-20200301134338439"></p><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1]BEWLEY A, GE Z, OTT L, et al. Simple online and realtime tracking[C]. in: 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016. 3464-3468.</p><p>[2]WOJKE N, BEWLEY A, PAULUS D. Simple online and realtime tracking with a deep association metric[C]. in: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. 3645-3649.</p><p>[3]<a href="https://github.com/abewley/sort/blob/master/sort.py">https://github.com/abewley/sort/blob/master/sort.py</a></p><p>[4]<a href="https://github.com/nwojke/deep_sort">https://github.com/nwojke/deep_sort</a></p><p>[5]<a href="https://blog.csdn.net/zjc910997316/article/details/83721573">https://blog.csdn.net/zjc910997316/article/details/83721573</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以经典应用为主。其中应用算法主要介绍Sort和Deepsort算法。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="Kalman" scheme="https://huangpiao.tech/tags/Kalman/"/>
    
  </entry>
  
  <entry>
    <title>Kalman滤波在MOT中的应用(一)——理论篇</title>
    <link href="https://huangpiao.tech/2020/02/29/Kalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%B8%80)%E2%80%94%E2%80%94%E7%90%86%E8%AE%BA%E7%AF%87/"/>
    <id>https://huangpiao.tech/2020/02/29/Kalman滤波在MOT中的应用(一)——理论篇/</id>
    <published>2020-02-28T16:30:00.000Z</published>
    <updated>2020-02-28T16:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Kalman滤波器是多目标跟踪任务中一个经典的运动模型，接下来会从理论、发展和代码实践三个方面对其进行展开，本次主要以理论基础为主。 在这篇之前还有一个关于多目标背景任务的介绍，不过被知乎删了，可以去<a href="https://huangpiao.tech">我的博客</a>看</p></blockquote><a id="more"></a><h2 id="1背景介绍"><a href="#1背景介绍" class="headerlink" title="1背景介绍"></a>1背景介绍</h2><p>卡尔曼滤波（Kalman）无论是在单目标还是多目标领域都属于很基础的一种算法，这个是属于自动控制理论中的一种方法，所以对于工科比较容易理解。为了方便理解，我下面对其进行详细的讲解。</p><p>首先，卡尔曼滤波器处理的是随机信号，例如，假设我们研究一个房间的温度，依据我们的经验判断，房间温度是恒定的（假设为23℃），但是由于我们的经验不完全可信，所以我们可以引入高斯白噪声来衡量一定的偏差程度（假设上一时刻的偏差是3，但是自己对于预测的不确定度为4，那么此时预测的偏差为5，这里采用的勾股定理）。这时候我们在房间放一个温度计，但是温度计本身也不一定准确，测量值（25℃）与实际值的偏差也视为一种高斯白噪声（假设为4），那么现在同时存在一个根据上一个时刻温度得来的估计量和这个时刻的测量值。</p><p>假设这两个值为23℃和25℃，这其实就对应着跟踪问题中的运动估计和实际跟踪结果，此时我们究竟相信谁呢？可利用他们的均方误差来计算：</p><p>$$ \begin{array}{l} {H^2} = \frac{{{5^2}}}{{{5^2} + {4^2}}} = 0.78 \Rightarrow T = 23 + 0.78 * \left( {25 - 23} \right) = 24.56\\ Err = \sqrt {\left( {1 - H} \right) * {5^2}} = 2.35 \end{array} $$<br>由此可将均方误差不断地传递下去，从而估计估算出最优的温度值。</p><p>为了方便解释卡尔曼滤波方程，下面以一辆小车的运动为例，假设我们已知上一时刻小车的状态，现在要估计当前时刻的状态：</p><p>$$ \begin{array}{l}\left\{ \begin{array}{l}{p_t} = {p_{t - 1}} + {v_{t - 1}} \times \Delta t + \frac{1}{2}{a_t} \times \Delta {t^2}\\{v_t} = {v_{t - 1}} + {a_t} \times \Delta t\end{array} \right.\\ \Rightarrow \left[ {\begin{array}{*{20}{c}}{{p_t}}\\{{v_t}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}1&{\Delta t}\\0&1\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{p_{t - 1}}}\\{{v_{t - 1}}}\end{array}} \right] + \left[ {\begin{array}{*{20}{c}}{\frac{{\Delta {t^2}}}{2}}\\{\Delta t}\end{array}} \right]{a_t}\\ \Rightarrow \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} _t^ - = {F_t}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_{t - 1}} + {B_t}{a_t}\end{array} $$<br>可以看到，对于一个线性系统，我们能根据上一时刻状态得到一个简单的估计量，其中F代表了状态转移矩阵，B则代表了控制矩阵，反映了加速度如何作用于当前状态。假设每一时刻的各个维度的不确定性都通过协方差矩阵来描述，另外预测模型本身也不一定准确，所以系统状态的不确定性如下：</p><p>$$ \Sigma _t^ - = F\Sigma _{t - 1}^ - {F^T} + Q $$<br>有了预测值，现在我们通过在路上布设装置来测定小汽车的位置，观测值的误差记为V，然后将真实状态x通过一定变换，可以得到真实状态x和观测状态y的关系：</p><p>$$ {y_t} = H{x_t} + V $$<br>显然，这里的H是[1 0]，因为观测到的是位置信息p，同样的我们需要用一个协方差矩阵R来取代上式中的V，以衡量观测不确定性。现在，我们已知此时刻的预测值，观测值，以及几个不确定性矩阵，可以得到此时刻最终的估计：</p><p>$$ \begin{array}{l} {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t} = {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t}^ - + {K_t}\left( {{y_t} - H{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t}^ - } \right)\\ {K_t} = \Sigma _t^ - {H^T}{\left( {H\Sigma _t^ - {H^T} + R} \right)^{ - 1}} \end{array} $$<br>其中 ${y_t} - H{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} _t}^ - $表示实际观测值和预估观测值之间的残差，$K_t$为卡尔曼系数，又叫滤波增益矩阵。可以看到其中同时包含预测状态的协方差矩阵和观测误差矩阵，如果我们相信预测模型多一点，那么对应的协方差矩阵会更小，则K会小一点，反之如果我们相信观测模型多一点，那么K会更大。</p><p>最后我们需要更新最有估计值的噪声分布：</p><p>$$ {\Sigma _t} = \left( {1 - {K_t}H} \right)\Sigma _t^ - $$</p><h2 id="2高斯分布融合"><a href="#2高斯分布融合" class="headerlink" title="2高斯分布融合"></a>2高斯分布融合</h2><p>先从高斯分布说起，Kalman滤波算法的假设分布即为高斯分布，而一维的高斯分布概率密度函数及其分布示意图如下：</p><p>$$ f\left( {x,\mu ,\sigma } \right) = \frac{1}{{\sigma \sqrt {2\pi } }}\exp \left[ { - \frac{{{{\left( {x - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right] $$<br><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228204639633.png" alt="image-20200228204639633"></p><p>这是一个很简单的高斯分布图，标准差决定分布曲线宽窄，均值决定其中心位置。那么如果我们对任意两个高斯分布进行运算，则会得如下的效果图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228204700610.png" alt="image-20200228204700610"></p><p>其中两个高斯分布的运算都加入了归一化，可以发现，他们的分布融合也是高斯分布，下面我们对这两个新的高斯分布进行求解证明，主要讲乘法融合，加法以此类推。对于任意两个高斯分布，将二者相乘之后可得：</p><p>$$ f\left( {{x_1}} \right)f\left( {{x_2}} \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right] $$<br>对于这个概率分布函数，我们利用高斯分布的两个特性进行求解，其一是均值处分布函数取极大值，其二是均值处分布曲线的曲率为其二阶导数，并且与s2成反比。</p><p>$$ \begin{array}{l} \because f\left( x \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right]\\ \therefore f'\left( x \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right] * \left[ { - \left( {\frac{{x - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{x - {\mu _2}}}{{{\sigma _2}^2}}} \right)} \right]\\ \because f'\left( \mu \right) = 0\\ \therefore \frac{{\mu - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{\mu - {\mu _2}}}{{{\sigma _2}^2}} = \mu \frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}} - \frac{{{\mu _2}{\sigma _1}^2 + {\mu _1}{\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}} = 0\\ \therefore \mu = \frac{{{\mu _2}{\sigma _1}^2 + {\mu _1}{\sigma _2}^2}}{{{\sigma _1}^2 + {\sigma _2}^2}}\\ \because f''\left( x \right) = f'\left( x \right) * \left[ { - \left( {\frac{{x - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{x - {\mu _2}}}{{{\sigma _2}^2}}} \right)} \right] - f\left( x \right)\frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}}\\ \therefore f''\left( \mu \right) = - f\left( \mu \right)\frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}}\\ \because f''\left( {{\mu _1}} \right) = - f\left( {{\mu _1}} \right)\frac{1}{{\sigma _1^2}},f''\left( {{\mu _2}} \right) = - f\left( {{\mu _2}} \right)\frac{1}{{\sigma _2^2}}\\ \therefore {\sigma ^2} = \frac{{{\sigma _1}^2{\sigma _2}^2}}{{{\sigma _1}^2 + {\sigma _2}^2}} \end{array} $$<br>因此我们可以得到如下结论：</p><p>$$ \begin{array}{l} f\left( {{x_1}} \right)f\left( {{x_2}} \right) \sim N\left( {\frac{{\sigma _1^2{\mu _2} + \sigma _2^2{\mu _1}}}{{\sigma _1^2 + \sigma _2^2}},\frac{{\sigma _1^2\sigma _2^2}}{{\sigma _1^2 + \sigma _2^2}}} \right)\\ f\left( {{x_1}} \right) + f\left( {{x_2}} \right) \sim N\left( {{\mu _1} + {\mu _2},\sigma _1^2 + \sigma _2^2} \right) \end{array} $$<br>当然我们遇到的问题大多是多阶的所以要引入多维高斯分布：</p><p>$$ \begin{array}{l} f\left( {x,\mu ,\Sigma } \right) = \frac{1}{{\sqrt {{{\left( {2\pi } \right)}^n}\left| \Sigma \right|} }}\exp \left[ { - \frac{1}{2}{{\left( {x - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( {x - \mu } \right)} \right]\\ \Rightarrow {f_1}{f_2} \sim N\left( {{\Sigma _2}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\mu _1} + {\Sigma _1}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\mu _2},{\Sigma _1}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\Sigma _2}} \right) \end{array} $$</p><h2 id="3线性卡尔曼滤波"><a href="#3线性卡尔曼滤波" class="headerlink" title="3线性卡尔曼滤波"></a>3线性卡尔曼滤波</h2><h3 id="3-1理论推导"><a href="#3-1理论推导" class="headerlink" title="3.1理论推导"></a>3.1理论推导</h3><p>首先假设状态变量为x，观测量为z，那么结合贝叶斯后验概率模型：</p><p>$$ posterior = \frac{{likelihood \times prior}}{{normalization}} \Leftrightarrow P\left( {\left. x \right|z} \right) = \frac{{P\left( {\left. z \right|x} \right)P\left( x \right)}}{{P\left( z \right)}} $$<br>多目标跟踪从形式上讲可以理解为最大化后验概率，现在结合第二节的内容，假设状态变量x服从高斯分布，反映的是运动模型的不稳定性。基于状态变量x的估计先验，观测量z也服从高斯分布，反映的是量测误差，比如传感器误差。那么我们就可以利用高斯分布的融合来刻画Kalman滤波器的更新部分。</p><p>这里我们先给出一阶Kalman滤波器的公式，其中预测环节就是基于线性运动特性对状态变量的预测，即：</p><p>$$ \begin{array}{l} x = Fx + Bu\\ P = FxF^T + Q \end{array} $$<br>其中$x$为状态变量的均值，$P$为预测方差，那么$Fx$对应的高斯分布方差即为 $FxF^T$ ，而$Q$则是线性运动模型本身的误差，由此得到预测环节。即预测结果服从高斯分布$N(x,P)$。</p><p>对于更新环节，同样地，假设量测误差分布满足$N(z,R)$，那么：</p><p>$$ \begin{array}{l} \mu = \frac{{{\mu _z}{\sigma _x}^2 + {\mu _x}{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}} = \frac{{{\sigma _x}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}{\mu _z} + \frac{{{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}{\mu _x}\\ K = \frac{{{\sigma _x}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}\\ \Rightarrow \mu = K{\mu _z} + \left( {1 - K} \right){\mu _x} = {\mu _x} + K\left( {{\mu _z} - {\mu _x}} \right)\\ \therefore {\sigma ^2} = \frac{{{\sigma _x}^2{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}} = K{\sigma _z}^2 = \left( {1 - K} \right){\sigma _x}^2 \end{array} $$<br>代入变量得：</p><p>$$ \left\{ \begin{array}{l} K = PH^T/\left( {HPH^T + R} \right)\\ x = Hx + K\left( {z - Hx} \right)\\ P = (1 - KH)P \end{array} \right. $$<br>上式即为Kalman滤波器是更新环节，其中H是从状态变量到观测量/输出变量的转换矩阵。</p><h3 id="3-2实验分析"><a href="#3-2实验分析" class="headerlink" title="3.2实验分析"></a>3.2实验分析</h3><p>我们可以看到的是，Kalman滤波器有很多参数，除去运动模型形式假设的F和B参数，存在有多个协方差矩阵P、Q、R。下面我们逐一分析各个参数的影响。</p><ul><li><p><strong>协方差矩阵变化规律</strong></p><p>在此之前可以看看不经过更新校正的状态变量均值和方差的变化，假设有如下的运动方式：<br>$$ \begin{array}{l} \left\{ \begin{array}{l} x = x + \Delta tv\\ v = v \end{array} \right. \Rightarrow \left[ {\begin{array}{*{20}{c}} x\\ v \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ v \end{array}} \right]\\ \Rightarrow P = \left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]P{\left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]^T} \end{array} $$<br>则有状态变量分布如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235651355.png" alt="image-20200228235651355"></p><p>可以看到，在不引入量测的情况下，物体一直保持匀速直线运动，所以其误差的协方差分布一直向水平方向倾斜。</p><p>（1）<strong>不同Kalman模型</strong></p><p>下面我们分别用一阶和二阶的Kalman滤波器去跟踪一个直线运动的物体，其中一阶Kalman滤波完全依赖量测的矫正，二阶Kalman滤波加入了速度因素，可见二阶模型跟踪效果更好，不过其实在这里，如果加入控制变量u，也能恰好达到匀速直线运动的效果。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235741212.png" alt="image-20200228235741212"></p><p>（2） <strong>R和Q的影响</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235840876.png" alt="image-20200228235840876"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235849008.png" alt="image-20200228235849008"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235855450.png" alt="image-20200228235855450"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235933972.png" alt="image-20200228235933972"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228235937838.png" alt="image-20200228235937838"></p><p>对于匀速直线运动，我们保持量测误差R不变，对比运动估计误差Q发现，Q越小，模型越相信运动规律，而模型正好也是匀速直线运动，因此跟踪效果更好。而当R变大时，模型会更加不相信量测结果，从而使得状态变量的协方差越来越大，但是由于预测环节模型的准确性，跟踪依然比较准确，可以从图中看出，当初始状态偏差很大时，模型不相信量测，导致跟踪轨迹很难与目标轨迹一致，而当R变小却可以重新跟踪到。</p><p>（3）<strong>P的影响</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200229000131703.png" alt="image-20200229000131703"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200229000136848.png" alt="image-20200229000136848"></p><p>对于上面两幅图，表面上看上去P=1时，跟踪轨迹跟贴近于真实轨迹，但是如果将协方差矩阵P中的参数绘制出来即为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200229000151523.png" alt="image-20200229000151523"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200229000155905.png" alt="image-20200229000155905"></p><p>我们可以发现，后者关于位置的方差变化趋势比较复杂，虽然二者均能跟踪到，但是当初始状态估计不好时，P过小会使得跟踪周期变长，而P较大时跟踪效果没有明显降低，因此通常P取值较大。</p></li></ul><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p><a href="https://github.com/rlabbe/filterpy">https://github.com/rlabbe/filterpy</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Kalman滤波器是多目标跟踪任务中一个经典的运动模型，接下来会从理论、发展和代码实践三个方面对其进行展开，本次主要以理论基础为主。 在这篇之前还有一个关于多目标背景任务的介绍，不过被知乎删了，可以去&lt;a href=&quot;https://huangpiao.tech&quot;&gt;我的博客&lt;/a&gt;看&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="Kalman" scheme="https://huangpiao.tech/tags/Kalman/"/>
    
  </entry>
  
  <entry>
    <title>多目标跟踪任务介绍与评价规则</title>
    <link href="https://huangpiao.tech/2020/02/28/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AF%84%E4%BB%B7%E4%BD%93%E7%B3%BB/"/>
    <id>https://huangpiao.tech/2020/02/28/多目标跟踪任务介绍与评价体系/</id>
    <published>2020-02-28T07:10:00.000Z</published>
    <updated>2020-02-28T07:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>由于最近忙着整理毕设以及小论文发表，所以有些内容我暂时不能分享出来，先从多目标跟踪领域的基础知识开始分享，后续也会陆续结合代码讲解。如果有机会，还可以分享我在MOTChallenge上刷到sota的经验。这次先结合一篇综述和我的理解进行一个背景介绍。</p></blockquote><a id="more"></a><h2 id="1-多目标跟踪简介"><a href="#1-多目标跟踪简介" class="headerlink" title="1. 多目标跟踪简介"></a>1. 多目标跟踪简介</h2><p>多目标跟踪可认为是多变量估计问题，即给定一个图像序列，$s_t^i$表示第 <em>t</em> 帧第 <em>i</em> 个目标的状态，${S_t} = \left( {s_t^1,s_t^2,...,s_t^{{M_t}}} \right)$表示第 <em>t</em> 帧所有目标的状态序列，$s_{{i_s}:{i_c}}^i = \left( {s_{{i_s}}^i,...,s_{{i_e}}^i} \right)$ 表示第 <em>i</em> 个目标的状态序列，其中${i_s}$和${i_e}$分别表示目标 <em>i</em> 出现的第一帧和最后一帧，${S_{1:t}} = \left( {{S_1},{S_2},...,{S_t}} \right)$表示所有目标从第一帧到第t帧的状态序列。这里的状态可以理解为目标对应图像中哪个位置，或者是否存在于此图像，这个是算法所求得的所有信息。</p><p>在多目标跟踪领域最常见的就是tracking-by-detection算法，通过匹配得到对应的观测目标${O_t} = \left( {o_t^1,o_t^2,...,o_t^{{M_t}}} \right)$ ，${O_{1:t}} = \left( {{O_1},{O_2},...,{O_t}} \right)$ 表示所有目标从第一帧到第t帧的观测目标序列，这里的目标都是我们观测到的位置。</p><p>那么多目标跟踪的目的就是找到所有目标的最佳状态序列，而在所有观测目标的状态序列上的条件分布上，可以通过最大化后验概率（MAP）得到。这里可以区分一下最大似然概率（MLE）和最大后验概率（MAP）：</p><ul><li><p><strong>最大似然概率</strong></p><p>要了解最大似然概率，首先要区分似然（likelihood）和概率（probability），这里提出函数$P\left( {x|\theta } \right)$。其中$x$表示某一数据，$\theta $表示模型参数。那么当$x$为变量，$\theta $为常量时，该函数为概率函数，表示在该模型下，出现不同$x$的概率；反之则为似然函数，表示对于不同的模型参数，出现该$x$的概率，所以概率函数更偏向于结果，似然函数更偏向于结构化。</p><p>了解了似然函数，顾名思义，最大化似然函数就是怎样设置模型参数能让一个预期的结果以最大概率出现。下面给出一个简单例子：</p><p>抛掷一枚硬币多次，假设硬币正面朝上的概率为$\theta $，那么出现事件${x_0}$=“反正正正正反正正正反”的似然函数就是：<br>$$ f\left( {{x_0},\theta } \right) = \left( {1 - \theta } \right) \times {\theta ^4} \times \left( {1 - \theta } \right) \times {\theta ^3} \times \left( {1 - \theta } \right) = {\theta ^7} \times {\left( {1 - \theta } \right)^3} $$<br>可以发现，当 $\theta $=0.7时该似然函数最大，也就是说当 $\theta $=0.7时，事件${x_0}$ 的出现概率最大，为0.22%。</p></li><li><p><strong>最大后验概率</strong></p><p>最大似然函数是最大化似然函数，那么最大化后验概率则是在此基础上考虑了模型参数 $\theta $，其认为该参数也有概率分布，即先验概率。所以最大化后验概率是在最大化下面的函数：<br><script type="math/tex">P\left( {{x_0}|\theta } \right)P\left( \theta  \right)  $$ 而由于${x_0}$ 已知，所以该函数可以通过贝叶斯公式转化为下面的后验概率：</script>P\left( {\theta |{x_0}} \right) = \frac{{P\left( {{x_0}|\theta } \right)P\left( \theta \right)}}{{P\left( {{x_0}} \right)}} $$<br>那么现在问题就比较抽象了，还是举个栗子说明吧。假设我们先验的认为 $\theta $=0.5的概率较高，那么不妨先假设$\theta $的先验概率分布为均值为0.5，方差为0.1的高斯分布。即硬币正面朝上概率的先验分布如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228151838907.png" alt="image-20200228151838907"></p><p>后验概率对应的分布为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228152055231.png" alt="image-20200228152055231"></p><p>这里的分布与后验概率分布趋势一致，只是纵坐标绝对值不同而已，可以看到，当 $\theta $ =0.558时，后验概率最大，由此可见与最大似然概率的不同。</p><p>回到多目标跟踪问题上，最大化后验概率在这里的体现就是在观测目标已知的前提下，选择最合理的状态序列分布，使得该观测目标序列出现的概率最大：<br>$$ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over S} _{1\;:\;t}} = \mathop {argmax}\limits_{{S_{1\;:\;t}}} P\left( {{S_{1\;:\;t}}|{O_{1\;:\;t}}} \right) $$</p></li></ul><h2 id="2分类"><a href="#2分类" class="headerlink" title="2分类"></a>2分类</h2><p>多目标跟踪问题由于比较复杂，所以基于不同假设前提的挑战赛有很多，由此作为多目标跟踪的分类依据。</p><h3 id="2-1初始化方法"><a href="#2-1初始化方法" class="headerlink" title="2.1初始化方法"></a>2.1初始化方法</h3><p>不同于单目标跟踪，多目标跟踪问题中并不是所有目标都会在第一帧出现，也并不是所有目标都会出现在每一帧。那么如何告知算法出现了新目标变成了一个问题，即不同的初始化方法。常见的初始化方法分为两大类，一个是Detection-Based-Tracking(DBT),一个是Detection-Free-Tracking(DFT)。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228153534440.png" alt="image-20200228153534440"></p><ul><li><p><strong>DBT</strong></p><p>可以看到，DBT的方式就是典型的tracking-by-detection模式，即先检测目标，然后将目标关联进入跟踪轨迹中。那么就存在两个问题，第一，该跟踪方式非常依赖目标检测器的性能，第二，目标检测的实质是分类和回归，即该跟踪方式只能针对特定的目标类型，如：行人、车辆、动物。</p></li><li><p><strong>DFT</strong></p><p>DFT是单目标跟踪领域的常用初始化方法，即每当新目标出现时，人为告诉算法新目标的位置，这样做的好处是target free，坏处就是过程比较麻烦，存在过多的交互，所以DBT相对来说更受欢迎。</p></li></ul><h3 id="2-2处理模式"><a href="#2-2处理模式" class="headerlink" title="2.2处理模式"></a>2.2处理模式</h3><p>同样地，MOT也存在着不同的处理模式，Online和Offline两大类，其主要区别在于是否用到了后面帧的信息。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228153918452.png" alt="image-20200228153918452"></p><ul><li><p><strong>Online Tracking</strong></p><p>Online Tracking是对视频帧逐帧进行处理，当前帧的跟踪仅利用过去的信息。</p></li><li><p><strong>Offline Tracking</strong></p><p>不同于Online Tracking，Offline Tracking会利用前后视频帧的信息对当前帧进行目标跟踪，这种方式只适用于视频，如果应用于摄像头，则会有滞后效应，通常采用时间窗方式进行处理，以节省内存和加速。</p></li></ul><h2 id="3外观模型"><a href="#3外观模型" class="headerlink" title="3外观模型"></a>3外观模型</h2><h3 id="3-1视觉表达"><a href="#3-1视觉表达" class="headerlink" title="3.1视觉表达"></a>3.1视觉表达</h3><ul><li><p><strong>全局颜色特征</strong></p><p>全局颜色特征除了灰度（即RGB2GRAY）之外，比较有名的就是单目标跟踪领域中的CN2特征了，该颜色特征将颜色空间划分为了黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄共11种，然后将其投影至10维子空间的标准正交基上。具体过程可见我之前关于KCF的一片博客。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228155056419.png" alt="image-20200228155056419"></p></li><li><p><strong>局部特征</strong></p><p>局部特征比较典型的就是光流法，其中比较典型的有KLT（Kanade-Lucas-Tomasi Tracking）算法，这个算法在CVPR94中经由来Jianbo Shi和Carlo Tomasi两人在《Good Features to Track》一文中提出shi-tomasi角点而改进，光流法的好处就是方便提取运动轨迹信息。其中在多目标跟踪领域利用光流特征比较出名的是NOMT算法：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228155334710.png" alt="image-20200228155334710" style="zoom:50%"></p><p>不过近几年深度学习领域发展迅速，基于深度光流框架（FlowNet系列等)的也有，比如最近刚出的《Multiple Object Tracking by Flowing and Fusing》，就是很粗暴地结合光流 和检测框架。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228155456818.png" alt="image-20200228155456818"></p></li><li><p><strong>区域特征</strong></p><p>和局部特征不同的是，区域特征是针对选定的区域进行特征提取的，以区域间的关系不同可分为三类：</p><ul><li>Zero-Order:这一层面一般指的是颜色特征，常用的有颜色直方图，亦或是类似于全图特征；</li><li>First-Order：这一层面的特征则是用到了梯度信息，最常见的就是HOG特征，还有不常见的水平集特征；</li><li>Up-to-Second-Order：这一层面用到了二阶梯度，一般采用区域协方差矩阵。</li></ul></li><li><p><strong>深度特征</strong></p><p>现在深度学习领域中关于backbone框架的各种变种层出不穷，比如ResNet，VGGNet等。特征的选择有很多，我们可以根据速度、精度、鲁棒性三个方向进行不同的选择，当然也能针对特定的场景进行选择。比如颜色直方图经常使用，然而其忽略了目标区域的空间分布。局部特征是高效的，但是对遮挡和平面外旋转敏感，基于梯度的特征例如HOG可以描述目标的形状并且对光照有适应性，但它不能很好地处理遮挡和变形。区域协方差矩阵相对来说比较鲁棒，因为它们使用了较多的信息，但同时带来了较高的计算复杂度。</p><p>目标跟踪领域的Martin大神在其之前的论文UPDT中对比分析了深度特征和浅层特征：</p><ul><li><p><strong>深度特征</strong>：主要是CNN的高层激活，典型VGGNet的layer 5。优点是包含高层语义，对旋转和变形等外观变化具有不变性，何时何地都能找到目标，即鲁棒性很强；缺点是空间分辨率低，对平移和尺度都有不变性，无法精确定位目标，会造成目标漂移和跟踪失败，即准确性很差。</p></li><li><p><strong>浅层特征</strong>：主要是手工特征如RGB raw-pixel, HOG, CN，和CNN的低层激活，典型VGGNet的Layer 1。优点是主要包含纹理和颜色信息，空间分辨率高，适合高精度定位目标，即准确性很强；缺点是不变性很差，目标稍微形变就不认识了，尤其是旋转，一转就傻，即鲁棒性很差。</p></li></ul><p>不过近几年由于行人重识别Re-ID领域的迅速发展，深度特征逐渐从基于图像分类任务的backbone框架转变到了基于Re-ID的识别类任务，这样更容易得到端到端的特征训练框架。尤其是在DBT框架中检测质量参差不齐，而且遮挡严重，训练框架需要对前景目标具有一定的捕捉能力，一个精心设计的端到端训练框架尤为重要。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228160222361.png" alt="image-20200228160222361"></p></li></ul><h3 id="3-2统计测量"><a href="#3-2统计测量" class="headerlink" title="3.2统计测量"></a>3.2统计测量</h3><p>统计测量即相似性度量，这里我们分为单线索和多线索两种方法：</p><ul><li><p><strong>单线索</strong></p><p>使用单线索进行外观建模时要么是将距离转换为相似性，要么直接计算相似性，比如说归一化互相关，这一点有点像相关滤波，即对两个相同大小的区域进行相关运算，亦或是利用巴氏距离计算两直方图的距离，并将其转化为相似性，或者加入高斯分布进行转化。</p></li><li><p><strong>多线索</strong></p><p>多线索，顾名思义就是结合不同的线索进行特征融合，有以下这些方法：</p><p><strong>Boosting</strong>：从特征池中选择一部分特征进行基于boosting的算法，例如对于颜色直方图、HOG、协方差矩阵，分别采用Adaboost、RealBoost和 HybirdBoost来区分不同目标各自的轨迹；</p><p><strong>Concatenation</strong>：直接进行特征拼接</p><p><strong>Summation</strong>：对不同的特征得到的相似度加权求和；</p><p><strong>Product</strong>：如果说不同特征间独立，则可以将相似度相乘；</p><p><strong>Cascading</strong>：使用不同的方法级联计算，例如考虑不同粗细粒度。</p></li></ul><h2 id="4运动与假设模型"><a href="#4运动与假设模型" class="headerlink" title="4运动与假设模型"></a>4运动与假设模型</h2><p>为了简化现实生活中的多目标跟踪，我们可以引入一些假设来人为简化求解过程，分别有运动模型、交互模型、社会力模型、排斥模型和遮挡模型。</p><h3 id="4-1运动模型"><a href="#4-1运动模型" class="headerlink" title="4.1运动模型"></a>4.1运动模型</h3><p>运动模型捕捉目标的动态行为，它估计目标在未来帧中的潜在位置，从而减少搜索空间。在大多数情况下，假设目标在现实中是平缓运动的，那么在图像空间也是如此。对于行人的运动，大致可分为线性和非线性两种运动：</p><ul><li><p><strong>线性运动</strong>：线性运动模型是目前最主流的模型，即假设目标的运动属性平稳（速度，加速度，位置）；</p></li><li><p><strong>非线性运动</strong>：虽然线性运动模型比较常用，但由于存在它解决不了的问题，非线性运动模型随之诞生。它可以使tracklets间运动相似度计算得更加准确。</p></li></ul><p>当然，对于存在相机运动的场景也需要考虑相机运动，不过这就涉及到了一些相机几何知识了，我以后再开专题讲解。</p><h3 id="4-2交互模型"><a href="#4-2交互模型" class="headerlink" title="4.2交互模型"></a>4.2交互模型</h3><p>交互模型也称为相互运动模型，它捕捉目标对其他目标的影响。在拥挤场景中，目标会从其他的目标和物体中感受到“力”。例如，当一个行人在街上行走时，他会调整他的速度、方向和目的地，以避免与其他人碰撞。另一个例子是当一群人穿过街道时，他们每个人都跟着别人，同时引导其他人。</p><ul><li><strong>社会力模型也被称为群体模型</strong>。在这些模型中，每个目标都被认为依赖于其他目标和环境因素，这种信息可以缓解拥挤场景中跟踪性能的下降。在社会力模型中，目标会根据其他物体和环境的观察来确定它们自己的速度、加速度和目的地。更具体地说，在社会力模型中，目标行为可以由两方面建模而成：基于个体力和群体力。</li><li><strong>人群运动模型</strong>。通常这类模型适用于目标密度非常高的超密集场景，这时目标都比较小，那些外观、个人运动模式线索就会受到极大干扰，所以人群运动模式就相对比较适合，类似于元胞自动机或者有限元分析。该类模式又分结构化模式和非结构化模式，结构化模式主要得到集体的空间结构而非结构化模式主要得到不同个体运动的模式。通常来说，运动模式由不同方法学习得到，甚至考虑场景结构，然后运动模式可作为先验知识辅助目标跟踪。</li></ul><h3 id="4-3遮挡模型"><a href="#4-3遮挡模型" class="headerlink" title="4.3遮挡模型"></a>4.3遮挡模型</h3><p>一种比较流行的方法是将全局目标（类似一个跟踪框,bounding box）分割成几个部分，然后对每个部分计算相似度，具体来说就是当发生遮挡时，被遮挡的那些部分的相似度权重降低，而提高没被遮挡部分的相似性权重。至于如何进行分割，有将目标均匀地切分成一个个格子的，也有以某种形态例如人来切分目标的，还有由DPM检测器得到的部分。利用重构误差判断某个部分是否被遮挡，外观模型只根据可见部分进行更新，对于两个轨迹的相似度可利用各部分相似度加权求得。</p><p>而缓冲模型则是在发生遮挡前记录目标状态并且将发生遮挡时的观测目标存入缓冲区中，当遮挡结束后，目标状态基于缓冲区的观测目标和之前记录的状态恢复出来。当发生遮挡时，保持最多15帧的trajectory，然后推断发生遮挡时潜在的轨迹。当目标重新出现时，重新进行跟踪并且ID也维持不变。当跟踪状态因为遮挡而产生歧义时观测模式就会启动，只要有足够的观测目标，就会产生假设来解释观测目标。以上就是”buffer-and-recover”策略。</p><h2 id="5概率预测型模型"><a href="#5概率预测型模型" class="headerlink" title="5概率预测型模型"></a>5概率预测型模型</h2><p>概率预测模型大多都是基于两部迭代的方式，假设目标的状态转移服从一阶马尔科夫模型：</p><p>$$ \begin{array}{l} Predict:P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}}) = \int {P(\left. {{S_t}} \right|{S_{t - 1}})P(\left. {{S_{t - 1}}} \right|{O_{1\;:\;t - 1}})d{S_{t - 1}}} \\ Update:P(\left. {{S_t}} \right|{O_{1\;:\;t}}){\rm{ = }}\frac{{P(\left. {{O_t}} \right|{S_t})P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}})}}{{\int {P(\left. {{O_t}} \right|{S_t})P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}})d{S_t}} }} \end{array} $$<br>其中预测阶段是动态模型，更新阶段为观测模型。</p><p>常见的概率预测型算法大致可分为以下几类：</p><ul><li><p><strong>传统概率模型</strong>。以Kalman滤波器和粒子滤波为主，其中Kalman滤波器依旧在MOT领域活跃，比如我们熟知的Sort系列；</p></li><li><p><strong>联合概率数据关联</strong>。 联合概率数据互联JPDA是数据关联算法之一，它的基本思想是对应于观测数据落入跟踪门相交区域的情况，这些观测数据可能来源于多个目标。JPDA的目的在于计算观测数据与每一个目标之间的关联概率，且认为所有的有效回波都可能源于每个特定目标，只是它们源于不同目标的概率不同。JPDA算法的优点在于它不需要任何关于目标和杂波的先验信息，是在杂波环境中对多目标进行跟踪的较好方法之一。然而当目标和量测数目增多时，JPDA算法的计算量将出现组合爆炸现象，从而造成计算复杂。算法分成联合事件生成和关联概率计算两部分。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228161521089.png" alt="image-20200228161521089"></p><ul><li><p><strong>多假设跟踪</strong>。多假设跟踪MHT是数据关联另一种算法。它的基本思想是：与JPDA不同的是，MHT算法保留真实目标的所有假设，并让其继续传递，从后续的观测数据中来消除当前扫描周期的不确定性。在理想条件下，MHT是处理数据关联的最优算法，它能检测出目标的终结和新目标的生成。但是当杂波密度增大时，计算复杂度成指数增长，在实际应用中，要想实现目标与测量的配对也是比较困难的。在ICCV2015和CVPR2017都有相关的工作。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200226/image-20200228161656805.png" alt="image-20200228161656805"></p></li><li><p><strong>随机有限集</strong>。国内在这一块的研究都处于起步阶段，其优势在于无需考虑数据关联，不用先假设各种组合关系，不用担心出现“组合爆炸”情况，直接估测目标的个数和状态，这样就可以不需要目标检测的结果了，当然有了更好。即RFS直接将目标整体看做一个目标，因此多目标跟踪就变成了一个单目标跟踪问题，计算复杂度也是随着目标数量线性增长的，不过其理论比较挑战我的理论基础，有一本专门介绍概率型单/多目标跟踪的书籍《advances in statistical multisource-multitarget information fusion》，1000来页，1页一块钱，不过我最近发现了中文版《多源多目标统计信息融合》，才100多¥，数学hold住的可以看看。目前这块都需要基于一定的分布建模，典型的有PHD滤波和伯努利滤波。</p></li></ul></li></ul><h2 id="6数据关联"><a href="#6数据关联" class="headerlink" title="6数据关联"></a>6数据关联</h2><p>大多数DBT框架都避免不了数据关联过程，数据关联分为以下几类：</p><ul><li><strong>偶图匹配</strong>。即在相邻两帧之间对跟踪轨迹和观测进行数据关联，常用的有IOU Matching(贪婪)、匈牙利算法/KM算法，也有少数人用MinCostFlow；</li><li><strong>图论</strong>。这类算法大多用于离线跟踪的建模，比如MinCostFlow、最大流/最小割、超图等；</li><li><strong>马尔可夫随机场和条件随机场等</strong>。</li></ul><h2 id="7深度学习模型"><a href="#7深度学习模型" class="headerlink" title="7深度学习模型"></a>7深度学习模型</h2><p>近两年，深度学习算法开始在MOT领域发展，一般分为这么几类：</p><ul><li>以Re-ID为主的表观特征提取网络，如《Aggregate Tracklet Appearance Features for Multi-Object Tracking》；</li><li>基于单目标跟踪领域中成熟的Siam类框架构建的多目标跟踪框架，如《Multi-object tracking with multiple cues and switcher-aware classification》；</li><li>联合目标检测框架和单目标跟踪框架的多任务框架，如《Detect to track and track to detect》；</li><li>端到端的数据关联类算法，如《DeepMOT: A Differentiable Framework for Training Multiple Object Trackers》；</li><li>联合运动、表观和数据观联的集成框架，如《FAMNet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking》；</li><li>基于LSTM类算法实现的运动估计、表观特征选择和融合等等算法。</li></ul><h2 id="8MOTChallenge评价体系"><a href="#8MOTChallenge评价体系" class="headerlink" title="8MOTChallenge评价体系"></a>8MOTChallenge评价体系</h2><p>由于MOTChallenge是最主流的MOT数据集，所以我这里就以它为例进行介绍。其包含有MOT15~17三个数据集，其中MOT15提供了3D的坐标信息，包含5500帧训练集和5783帧测试集，提供了基于ACF检测器的观测结果。而MOT16和MOT17则包含5316帧训练集和5919帧测试集，其中MOT16仅提供了基于DPM检测器的观测，而MOT17则提供了SDP、FasterRcnn、DPM三种检测结果。</p><p>MOT提供的目标检测结果标注格式为： frame_id 、 target_id、 bb_left 、bb_top、 bb_width、bb_height 、confidence 、x 、y 、z 。即视频帧序号、目标编号（由于暂时未定，所以均为-1）、目标框左上角坐标和宽高、检测置信度（不一定是0~1）、三维坐标（2D数据集中默认为-1）.</p><p>那么我们所需要提供的跟踪结果格式也是同上面一致的，不过需要我们填写对应的target_id和对应的目标框信息，而confidence,x,y,z均任意，保持默认即可。</p><p>相应地，官方所采用的跟踪groudtruth格式则为： frame_id、 target_id 、bb_left、bb_top、bb_width bb_height 、is_active、label_id、visibility_ratio 。其中is_active代表此目标是否考虑，label_id表示该目标所属类别，visibility_ratio表示目标的可视程度，目标类别分类如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">类别</th><th style="text-align:center">标签</th><th style="text-align:center">类别</th><th style="text-align:center">标签</th></tr></thead><tbody><tr><td style="text-align:center">Pedestrian</td><td style="text-align:center">1</td><td style="text-align:center">Static Person</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center">Person on vehicle</td><td style="text-align:center">2</td><td style="text-align:center">Distractor</td><td style="text-align:center">8</td></tr><tr><td style="text-align:center">Car</td><td style="text-align:center">3</td><td style="text-align:center">Occluder</td><td style="text-align:center">9</td></tr><tr><td style="text-align:center">Bicycle</td><td style="text-align:center">4</td><td style="text-align:center">Occluder on the ground</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center">Motorbike</td><td style="text-align:center">5</td><td style="text-align:center">Occluder full</td><td style="text-align:center">11</td></tr><tr><td style="text-align:center">Non motorized vehicle</td><td style="text-align:center">6</td><td style="text-align:center">Reflection</td><td style="text-align:center">12</td></tr></tbody></table></div><p>最后，数据集还提供了各个视频的视频信息seqinfo.ini，主要包括视频名称、视频集路径、帧率fps、视频长度、图像宽高、图像格式等。</p><p>根据MOT官方工具箱中的评价工具，可分析如下的评价规则：</p><p><strong>Step1</strong> <strong>数据清洗</strong></p><p>对于跟踪结果进行简单的格式转换，这个主要是方便计算，意义不大，其中根据官方提供的跟踪groundtruth，<strong>只保留is_active = 1的目标</strong>（根据观察，只考虑了类别为1，即处于运动状态的无遮挡的行人）。另外将groudtruth中完全没有跟踪结果的目标清除，并保持groudtruth中的视频帧序号与视频帧数一一对应。为了统一跟踪结果和groudtruth的目标ID，首先建立目标的映射表，即将跟踪结果中离散的目标ID按照从1开始的数字ID替代。</p><p><strong>Step2</strong> <strong>数据匹配</strong></p><p>将跟踪结果和groundtruth中同属一帧的目标取出来，并计算两两之间的IOU，并将其转换为cost矩阵（可理解为距离矩阵，假定Thresh=0.5）。利用cost矩阵，通过匈牙利算法（Hungarian）建立匹配矩阵，从而将跟踪结果中的目标和groundtruth中的目标一一对应起来。</p><p><strong>Step3</strong> <strong>数据分析</strong></p><p>对视频每一帧进行分析，利用每一帧中的跟踪目标和groudtruth目标之间的匹配关系，可作出以下几个设定：</p><ul><li><p>对于当前帧检测到但未匹配的目标轨迹记作falsepositive；</p></li><li><p>对于当前帧groudtruth中未匹配的目标轨迹记作missed；</p></li><li><p>对于groudtruth中的某一目标，如果与之匹配的跟踪目标ID前后不一致，则记作IDswitch；</p></li><li><p>对于已匹配的轨迹记作covered，总轨迹为gt。</p></li></ul><p>其中，对于匹配和未匹配到的目标都有各自的评价依据，评价指标很多，这里就不细讲了，网上都有。</p><p>利用cost矩阵，通过匈牙利算法（Hungarian）建立匹配矩阵，从而将跟踪结果中的目标和groundtruth中的目标一一对应起来。</p><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p>[1] LUO W, XING J, MILAN A, et al. Multiple object tracking: A literature review[J]. arXiv preprint arXiv:1409.7618, 2014.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;由于最近忙着整理毕设以及小论文发表，所以有些内容我暂时不能分享出来，先从多目标跟踪领域的基础知识开始分享，后续也会陆续结合代码讲解。如果有机会，还可以分享我在MOTChallenge上刷到sota的经验。这次先结合一篇综述和我的理解进行一个背景介绍。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="MOT" scheme="https://huangpiao.tech/tags/MOT/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
  </entry>
  
  <entry>
    <title>多目标跟踪MOT相关论文和代码资源列表</title>
    <link href="https://huangpiao.tech/2020/02/23/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AAMOT%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E5%92%8C%E4%BB%A3%E7%A0%81%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8/"/>
    <id>https://huangpiao.tech/2020/02/23/多目标跟踪MOT相关论文和代码资源列表/</id>
    <published>2020-02-23T11:55:00.000Z</published>
    <updated>2020-02-23T11:55:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>这段时间在整理毕设，所以这里结合了<a href="https://github.com/SpyderXu/multi-object-tracking-paper-list">SpyderXu</a>分享的内容把多目标跟踪相关的文献资源共享一下，由于文章很多，所以我这里只整理3年以内的，对于年限久远的，这里只取提供了代码的和比较经典的。并且尽可能注释了相关算法在MOT数据集上的名称。各自算法的性能比较可以看论文以及MOT官网。</p></blockquote><a id="more"></a><p>在线跟踪（Online)</p><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Source</th><th style="text-align:center">Publication</th><th style="text-align:center">Notes</th></tr></thead><tbody><tr><td style="text-align:center">Tracking Objects as Points</td><td style="text-align:center"><a href="http://arxiv.org/abs/2004.01177">[pdf]</a> <a href="https://github.com/xingyizhou/CenterTrack">[code]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">CenterTrack</td></tr><tr><td style="text-align:center">Refinements in Motion and Appearance for Online Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/2003.07177">[pdf]</a> <a href="https://github.com/nightmaredimple/libmot">[code]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">MIFT</td></tr><tr><td style="text-align:center">Multiple Object Tracking by Flowing and Fusing</td><td style="text-align:center"><a href="https://arxiv.org/abs/2001.11180">[pdf]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">FFT</td></tr><tr><td style="text-align:center">A Unified Object Motion and Affinity Model for Online Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/2003.11291">[pdf]</a><a href="https://github.com/yinjunbo/UMA-MOT">[code]</a></td><td style="text-align:center">CVPR2020</td><td style="text-align:center">UMA</td></tr><tr><td style="text-align:center">DeepMOT:A Differentiable Framework for Training Multiple Object Trackers</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1906.06618.pdf">[pdf]</a> <a href="https://gitlab.inria.fr/yixu/deepmot">[code]</a></td><td style="text-align:center">CVPR2020</td><td style="text-align:center">DeepMOT</td></tr><tr><td style="text-align:center">Online multiple pedestrian tracking using deep temporal appearance matching association</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1906.06618.pdf">[pdf]</a> <a href="https://gitlab.inria.fr/yixu/deepmot">[code]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">DD_TAMA19</td></tr><tr><td style="text-align:center">Spatial-temporal relation networks for multi-object tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Spatial-Temporal_Relation_Networks_for_Multi-Object_Tracking_ICCV_2019_paper.pdf">[pdf]</a></td><td style="text-align:center">ICCV2019</td><td style="text-align:center">STRN</td></tr><tr><td style="text-align:center">Towards Real-Time Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1909.12605v1.pdf">[pdf]</a> <a href="https://github.com/Zhongdao/Towards-Realtime-MOT">[code]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">JDE(private)</td></tr><tr><td style="text-align:center">Multi-object tracking with multiple cues and switcher-aware classification</td><td style="text-align:center"><a href="https://arxiv.org/abs/1901.06129">[pdf]</a></td><td style="text-align:center">arXiv(2019)</td><td style="text-align:center">LSST</td></tr><tr><td style="text-align:center">FAMNet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdf">[pdf]</a></td><td style="text-align:center">ICCV2019</td><td style="text-align:center">FAMNet</td></tr><tr><td style="text-align:center">Online multi-object tracking with instance-aware tracker and dynamic model refreshment</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1902.08231">[pdf]</a></td><td style="text-align:center">WACV2019</td><td style="text-align:center">KCF</td></tr><tr><td style="text-align:center">Tracking without bells and whistles</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1903.05625.pdf">[pdf]</a> <a href="https://github.com/phil-bergmann/tracking_wo_bnw">[code]</a></td><td style="text-align:center">ICCV2019</td><td style="text-align:center">Tracktor</td></tr><tr><td style="text-align:center">MOTS: Multi-Object Tracking and Segmentation</td><td style="text-align:center"><a href="https://www.vision.rwth-aachen.de/media/papers/mots-multi-object-tracking-and-segmentation/MOTS.pdf">[pdf]</a> <a href="https://github.com/VisualComputingInstitute/TrackR-CNN/tree/master">[code]</a></td><td style="text-align:center">CVPR2019</td><td style="text-align:center">Track R-CNN</td></tr><tr><td style="text-align:center">Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Maksai_Eliminating_Exposure_Bias_and_Metric_Mismatch_in_Multiple_Object_Tracking_CVPR_2019_paper.pdf">[pdf]</a> <a href="https://github.com/VisualComputingInstitute/TrackR-CNN/tree/master">[code]</a></td><td style="text-align:center">CVPR2019</td><td style="text-align:center">SAS_MOT17</td></tr><tr><td style="text-align:center">Recurrent autoregressive networks for online multi-object tracking</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1711.02741">[pdf]</a></td><td style="text-align:center">WACV2018</td><td style="text-align:center">RAN</td></tr><tr><td style="text-align:center">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</td><td style="text-align:center"><a href="https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification">[pdf]</a> <a href="https://github.com/longcw/MOTDT">[code]</a></td><td style="text-align:center">ICME2018</td><td style="text-align:center">MOTDT</td></tr><tr><td style="text-align:center">Online multi-object tracking with dual matching attention networks</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper.pdf">[pdf]</a> <a href="https://github.com/jizhu1023/DMAN_MOT">[code]</a></td><td style="text-align:center">ECCV2018</td><td style="text-align:center">DMAN</td></tr><tr><td style="text-align:center">Extending IOU Based Multi-Object Tracking by Visual Information</td><td style="text-align:center"><a href="http://elvera.nue.tu-berlin.de/typo3/files/1547Bochinski2018.pdf">[pdf]</a> <a href="https://github.com/bochinski/iou-tracker">[code]</a></td><td style="text-align:center">AVSS2018</td><td style="text-align:center">V-IOU</td></tr><tr><td style="text-align:center">Online Multi-target Tracking using Recurrent Neural Networks</td><td style="text-align:center"><a href="http://www.milanton.de/files/aaai2017/aaai2017-anton-rnntracking.pdf">[pdf]</a> <a href="https://bitbucket.org/amilan/rnntracking">[code]</a></td><td style="text-align:center">AAAI2017</td><td style="text-align:center">MOT-RNN</td></tr><tr><td style="text-align:center">Detect to Track and Track to Detect</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Feichtenhofer_Detect_to_Track_ICCV_2017_paper.pdf">[pdf]</a> <a href="https://github.com/feichtenhofer/Detect-Track">[code]</a></td><td style="text-align:center">ICCV2017</td><td style="text-align:center">D&amp;T(private)</td></tr><tr><td style="text-align:center">Online multi-object tracking using CNN-based single object tracker with spatial-temporal attention mechanism</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chu_Online_Multi-Object_Tracking_ICCV_2017_paper.pdf">[pdf]</a></td><td style="text-align:center">ICCV2017</td><td style="text-align:center">STAM</td></tr><tr><td style="text-align:center">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</td><td style="text-align:center"><a href="https://arxiv.org/abs/1701.01909">[pdf]</a></td><td style="text-align:center">ICCV2017</td><td style="text-align:center">AMIR</td></tr><tr><td style="text-align:center">Simple online and realtime tracking with a deep association metric</td><td style="text-align:center"><a href="https://arxiv.org/abs/1703.07402">[pdf]</a> <a href="https://github.com/nwojke/deep_sort">[code]</a></td><td style="text-align:center">ICIP2017</td><td style="text-align:center">DeepSort</td></tr><tr><td style="text-align:center">High-speed tracking-by-detection without using image information</td><td style="text-align:center"><a href="http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf">[pdf]</a> <a href="https://github.com/bochinski/iou-tracker">[code]</a></td><td style="text-align:center">AVSS2017</td><td style="text-align:center">IOU Tracker</td></tr><tr><td style="text-align:center">Simple online and realtime tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/1602.00763">[pdf]</a> <a href="https://github.com/abewley/sort">[code]</a></td><td style="text-align:center">ICIP2016</td><td style="text-align:center">Sort</td></tr><tr><td style="text-align:center">Temporal dynamic appearance modeling for online multi-person tracking</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1510.02906v1">[pdf]</a></td><td style="text-align:center">CVIU(2016)</td><td style="text-align:center">TDAM</td></tr><tr><td style="text-align:center">Online multi-object tracking via structural constraint event aggregation</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Yoon_Online_Multi-Object_Tracking_CVPR_2016_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2016</td><td style="text-align:center">SCEA</td></tr><tr><td style="text-align:center">Online Multi-Object Tracking Via Robust Collaborative Model and Sample Selection</td><td style="text-align:center"><a href="https://users.encs.concordia.ca/~rcmss/include/Papers/CVIU2016.pdf">[pdf]</a> <a href="https://users.encs.concordia.ca/~rcmss/">[code]</a></td><td style="text-align:center">CVIU2016</td><td style="text-align:center">RCMSS</td></tr><tr><td style="text-align:center">Learning to Track: Online Multi-Object Tracking by Decision Making</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf">[pdf]</a> <a href="http://cvgl.stanford.edu/projects/MDP_tracking/">[code]</a></td><td style="text-align:center">ICCV2015</td><td style="text-align:center">MDP</td></tr><tr><td style="text-align:center">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</td><td style="text-align:center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Bae_Robust_Online_Multi-Object_2014_CVPR_paper.pdf">[pdf]</a> <a href="https://cvl.gist.ac.kr/project/cmot.html">[code]</a></td><td style="text-align:center">CVPR2014</td><td style="text-align:center">CMOT</td></tr><tr><td style="text-align:center">The Way They Move: Tracking Targets with Similar Appearance</td><td style="text-align:center"><a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Dicle_The_Way_They_2013_ICCV_paper.pdf">[pdf]</a> <a href="https://bitbucket.org/cdicle/smot">[code]</a></td><td style="text-align:center">ICCV2013</td><td style="text-align:center">SMOT</td></tr><tr><td style="text-align:center">Online Multi-Person Tracking by Tracker Hierarchy</td><td style="text-align:center"><a href="https://github.com/SpyderXu/multi-object-tracking-paper-list/blob/master">[pdf]</a> <a href="http://cs-people.bu.edu/jmzhang/tracker_hierarchy/Tracker_Hierarchy.htm">[code]</a></td><td style="text-align:center">AVSS2012</td><td style="text-align:center">OMPTTH</td></tr></tbody></table></div><h2 id="离线跟踪（Batch"><a href="#离线跟踪（Batch" class="headerlink" title="离线跟踪（Batch)"></a>离线跟踪（Batch)</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Source</th><th style="text-align:center">Publication</th><th style="text-align:center">Notes</th></tr></thead><tbody><tr><td style="text-align:center">Learning non-uniform hypergraph for multi-object tracking</td><td style="text-align:center"><a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/download/4928/4801">[pdf]</a></td><td style="text-align:center">AAAI2019</td><td style="text-align:center">NT</td></tr><tr><td style="text-align:center">Learning a Neural Solver for Multiple Object Tracking</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1912.07515">[pdf]</a> <a href="https://github.com/selflein/GraphNN-Multi-Object-Tracking">[code]</a></td><td style="text-align:center">CVPR2020</td><td style="text-align:center">MPNTracker</td></tr><tr><td style="text-align:center">Deep learning of graph matching</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Deep_Learning_of_CVPR_2018_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2018</td><td style="text-align:center">深度图匹配</td></tr><tr><td style="text-align:center">muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking</td><td style="text-align:center"><a href="http://papers.nips.cc/paper/8334-mussp-efficient-min-cost-flow-algorithm-for-multi-object-tracking">[pdf]</a> <a href="https://github.com/yu-lab-vt/muSSP">[code]</a></td><td style="text-align:center">NIPS(2019)</td><td style="text-align:center">muSSP</td></tr><tr><td style="text-align:center">Exploit the connectivity: Multi-object tracking with trackletnet</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1909.12605v1.pdf">[pdf]</a> <a href="https://github.com/GaoangW/TNT">[code]</a></td><td style="text-align:center">ACM mm 2019</td><td style="text-align:center">TNT(eTC)</td></tr><tr><td style="text-align:center">Deep affinity network for multiple object tracking</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1810.11780">[pdf]</a> <a href="https://github.com/shijieS/SST">[code]</a></td><td style="text-align:center">PAMI(2019)</td><td style="text-align:center">DAN</td></tr><tr><td style="text-align:center">Multiple people tracking using body and joint detections</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/BMTT/Henschel_Multiple_People_Tracking_Using_Body_and_Joint_Detections_CVPRW_2019_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPRW2019</td><td style="text-align:center">JBNOT</td></tr><tr><td style="text-align:center">Aggregate Tracklet Appearance Features for Multi-Object Tracking</td><td style="text-align:center"><a href="https://www.sci-hub.shop/10.1109/lsp.2019.2940922">[pdf]</a></td><td style="text-align:center">SPL(2019)</td><td style="text-align:center">NOTA</td></tr><tr><td style="text-align:center">Customized multi-person tracker</td><td style="text-align:center"><a href="https://is.mpg.de/uploads_file/attachment/attachment/469/0509.pdf">[pdf]</a></td><td style="text-align:center">ACCV2018</td><td style="text-align:center">HCC</td></tr><tr><td style="text-align:center">Multi-object tracking with neural gating using bilinear lstm</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1810.11780">[pdf]</a></td><td style="text-align:center">ECCV2018</td><td style="text-align:center">MHT_bLSTM</td></tr><tr><td style="text-align:center">Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/1804.04555">[pdf]</a></td><td style="text-align:center">ICME2018</td><td style="text-align:center">GCRE</td></tr><tr><td style="text-align:center">Multiple People Tracking with Lifted Multicut and Person Re-identification</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2017</td><td style="text-align:center">LMP</td></tr><tr><td style="text-align:center">Deep network flow for multi-object tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Schulter_Deep_Network_Flow_CVPR_2017_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2017</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Non-markovian globally consistent multi-object tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Maksai_Non-Markovian_Globally_Consistent_ICCV_2017_paper.pdf">[pdf]</a> <a href="https://github.com/maksay/ptrack_cpp">[code]</a></td><td style="text-align:center">ICCV2017</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Multi-Object Tracking with Quadruplet Convolutional Neural Networks</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Son_Multi-Object_Tracking_With_CVPR_2017_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2017</td><td style="text-align:center">Quad-CNN</td></tr><tr><td style="text-align:center">Enhancing detection model for multiple hypothesis tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w34/papers/Chen_Enhancing_Detection_Model_CVPR_2017_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPRW2017</td><td style="text-align:center">EDMT</td></tr><tr><td style="text-align:center">POI: Multiple Object Tracking with High Performance Detection and Appearance Feature</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1610.06136">[pdf]</a></td><td style="text-align:center">ECCV2016</td><td style="text-align:center">KNDT</td></tr><tr><td style="text-align:center">Multiple hypothesis tracking revisited</td><td style="text-align:center"><a href="https://www.cc.gatech.edu/~ckim314/papers/MHTR_ICCV2015.pdf">[pdf]</a> <a href="http://rehg.org/mht/">[code]</a></td><td style="text-align:center">ICCV2015</td><td style="text-align:center">MHT-DAM</td></tr><tr><td style="text-align:center">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Choi_Near-Online_Multi-Target_Tracking_ICCV_2015_paper.pdf">[pdf]</a></td><td style="text-align:center">ICCV2015</td><td style="text-align:center">NOMT</td></tr><tr><td style="text-align:center">Learning to Divide and Conquer for Online Multi-Target Tracking</td><td style="text-align:center"><a href="http://ieeexplore.ieee.org/document/7410854/">[pdf]</a> <a href="https://github.com/francescosolera/LDCT">[code]</a></td><td style="text-align:center">ICCV2015</td><td style="text-align:center">LDCT</td></tr><tr><td style="text-align:center">On Pairwise Costs for Network Flow Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/1408.3304">[pdf]</a> <a href="http://www.di.ens.fr/willow/research/flowtrack/">[code]</a></td><td style="text-align:center">CVPR2015</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Multiple Target Tracking Based on Undirected Hierarchical Relation Hypergraph</td><td style="text-align:center"><a href="http://www.cbsr.ia.ac.cn/users/lywen/papers/CVPR2014_HyperGraphMultiTargetsTracker.pdf">[pdf]</a> <a href="http://www.cbsr.ia.ac.cn/users/lywen/">[code]</a></td><td style="text-align:center">CVPR2014</td><td style="text-align:center">H2T</td></tr><tr><td style="text-align:center">Continuous Energy Minimization for Multi-Target Tracking</td><td style="text-align:center"><a href="http://www.milanton.de/files/pami2014/pami2014-anton.pdf">[pdf]</a> <a href="http://www.milanton.de/contracking/">[code]</a></td><td style="text-align:center">CVPR2014</td><td style="text-align:center">CEM</td></tr><tr><td style="text-align:center">GMCP-Tracker: Global Multi-object Tracking Using Generalized Minimum Clique Graphs</td><td style="text-align:center"><a href="http://crcv.ucf.edu/papers/eccv2012/GMCP-Tracker_ECCV12.pdf">[pdf]</a> <a href="http://crcv.ucf.edu/projects/GMCP-Tracker/">[code]</a></td><td style="text-align:center">ECCV2012</td><td style="text-align:center">GMCP</td></tr><tr><td style="text-align:center">Multiple Object Tracking using K-Shortest Paths Optimization</td><td style="text-align:center"><a href="https://cvlab.epfl.ch/files/content/sites/cvlab2/files/publications/publications/2011/BerclazFTF11.pdf">[pdf]</a> <a href="https://cvlab.epfl.ch/software/ksp">[code]</a></td><td style="text-align:center">PAMI2011</td><td style="text-align:center">KSP</td></tr><tr><td style="text-align:center">Global data association for multi-object tracking using network flows</td><td style="text-align:center"><a href="https://inc.ucsd.edu/mplab/wordpress/wp-content/uploads/CVPR2008/Conference/data/papers/244.pdf">[pdf]</a> <a href="https://github.com/nwojke/mcf">[code]</a></td><td style="text-align:center">CVPR2008</td><td style="text-align:center">-</td></tr></tbody></table></div><h2 id="跨摄像头跟踪（MTMC）"><a href="#跨摄像头跟踪（MTMC）" class="headerlink" title="跨摄像头跟踪（MTMC）"></a>跨摄像头跟踪（MTMC）</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Source</th><th style="text-align:center">Publication</th><th style="text-align:center">Notes</th></tr></thead><tbody><tr><td style="text-align:center">CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Tang_CityFlow_A_City-Scale_Benchmark_for_Multi-Target_Multi-Camera_Vehicle_Tracking_and_CVPR_2019_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2019</td><td style="text-align:center">CityFlow</td></tr><tr><td style="text-align:center">Features for multi-target multi-camera tracking and re-identification</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1803.10859.pdf">[pdf]</a> <a href="https://github.com/SamvitJ/Duke-DeepCC">[code]</a></td><td style="text-align:center">CVPR2018</td><td style="text-align:center">DeepCC(MTMC)</td></tr><tr><td style="text-align:center">Rolling Shutter and Radial Distortion Are Features for High Frame Rate Multi-Camera Tracking</td><td style="text-align:center"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bapat_Rolling_Shutter_and_CVPR_2018_paper.pdf">[pdf]</a></td><td style="text-align:center">CVPR2018</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Towards a Principled Integration of Multi-Camera Re-Identification andTracking through Optimal Bayes Filters</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1705.04608.pdf">[pdf]</a> <a href="https://github.com/VisualComputingInstitute/towards-reid-tracking">[code]</a></td><td style="text-align:center">CVPR2017</td><td style="text-align:center">towards-reid-tracking</td></tr></tbody></table></div><h2 id="3D-amp-多模态跟踪"><a href="#3D-amp-多模态跟踪" class="headerlink" title="3D&amp;多模态跟踪"></a>3D&amp;多模态跟踪</h2><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Source</th><th style="text-align:center">Publication</th><th style="text-align:center">Notes</th></tr></thead><tbody><tr><td style="text-align:center">Robust Multi-Modality Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.org/abs/1909.03850">[pdf]</a> <a href="https://github.com/ZwwWayne/mmMOT#">[code]</a></td><td style="text-align:center">ICCV2019</td><td style="text-align:center">mmMOT</td></tr><tr><td style="text-align:center">A baseline for 3D Multi-Object Tracking</td><td style="text-align:center"><a href="https://arxiv.xilesou.top/pdf/1907.03961">[pdf]</a> <a href="https://github.com/xinshuoweng/AB3DMOT">[code]</a></td><td style="text-align:center">arXiv</td><td style="text-align:center">-</td></tr></tbody></table></div><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p><a href="http://pdfs.semanticscholar.org/3dff/acda086689c1bcb01a8dad4557a4e92b8205.pdf">Multiple Object Tracking: A Literature Review</a></p><p><a href="https://arxiv.xilesou.top/pdf/1802.06897">Machine Learning Methods for Solving Assignment Problems in Multi-Target Tracking</a></p><p><a href="https://arxiv.xilesou.top/pdf/1907.12740">Deep Learning in Video Multi-Object Tracking_ A Survey</a></p><p><a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/greedy_fahim_albert.pdf">Globally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects</a></p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><a href="https://motchallenge.net/">MOT</a>：包含2D MOT2015、3D MOT2015、MOT16、MOT17和MOT17Det等多个子数据集，提供了ACF、DPM、Faster RCNN、SDP等多个检测器输入。包含不同的相机视角、相机运动、场景和时间变化以及密集场景。</p><p><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php">KITTI</a>：提供了汽车和行人的标注，场景较稀疏。</p><p><a href="https://www.d2.mpi-inf.mpg.de/node/428">TUD Stadtmitte</a>：包含3D人体姿态识别、多视角行人检测和朝向检测、以及行人跟踪的标注，相机视角很低，数据集不大。</p><p><a href="https://data.vision.ee.ethz.ch/cvl/aess/dataset/">ETHZ</a>：由手机拍摄的多人跟踪数据集，包含三个场景。</p><p><a href="https://cvlab.epfl.ch/data/data-pom-index-php/#terrace">EPFL</a>：多摄像头采集的行人检测和跟踪数据集，每隔摄像头离地2米，实验人员就是一个实验室的，分为实验室、校园、平台、通道、篮球场这5个场景，每个场景下都有多个摄像头，每个摄像头拍摄2分钟左右。</p><p><a href="https://creativecommons.org/licenses/by/3.0/">KIT AIS</a>：空中拍摄的，只有行人的头</p><p><a href="https://motchallenge.net/data/PETS2017/">PETS</a>：比较早期的视频，有各式各样的行人运动。</p><p><a href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC</a>：多摄像头多行人跟踪。</p><p><a href="https://www.vision.rwth-aachen.de/page/mots">MOTS</a>：多目标跟踪与分割。</p><h2 id="评价体系"><a href="#评价体系" class="headerlink" title="评价体系"></a>评价体系</h2><p><a href="https://cvhci.anthropomatik.kit.edu/images/stories/msmmi/papers/eurasip2008.pdf">ClearMOT</a></p><p><a href="https://users.cs.duke.edu/~ristani/bmtt2016/ristani2016MTMC.pdf">IDF1</a></p><p>Code: <a href="https://github.com/cheind/py-motmetrics">python</a>、<a href="https://motchallenge.net/devkit/">matlab</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;这段时间在整理毕设，所以这里结合了&lt;a href=&quot;https://github.com/SpyderXu/multi-object-tracking-paper-list&quot;&gt;SpyderXu&lt;/a&gt;分享的内容把多目标跟踪相关的文献资源共享一下，由于文章很多，所以我这里只整理3年以内的，对于年限久远的，这里只取提供了代码的和比较经典的。并且尽可能注释了相关算法在MOT数据集上的名称。各自算法的性能比较可以看论文以及MOT官网。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="MOT" scheme="https://huangpiao.tech/tags/MOT/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
  </entry>
  
  <entry>
    <title>矩阵求导</title>
    <link href="https://huangpiao.tech/2019/02/15/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    <id>https://huangpiao.tech/2019/02/15/矩阵求导/</id>
    <published>2019-02-15T10:00:00.000Z</published>
    <updated>2019-02-15T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>无论是机器学习还是最近大热的深度学习算法，很多情况下都需要用到矩阵求导方面的知识，比如拉格朗日乘子法、梯度下降法等等，但是矩阵求导不同于传统的标量求导法则，存在很多冲突。网上以及各种论文中所推导的梯度计算方法，标准都不一样，容易把人搞迷糊。所以我这里理一下矩阵/向量求导相关的原理，并且确定一套属于自己的求导标准，方便自己研究。</p></blockquote><a id="more"></a><h2 id="1-规则说明"><a href="#1-规则说明" class="headerlink" title="1.规则说明"></a>1.规则说明</h2><p>整体来看，矩阵/向量求导涉及到<code>标量</code>，<code>向量</code>和<code>矩阵</code>三者两两之间的求导，那么这其中就涉及到了如下两种规则，这也是众多论文、网站博客让人迷惑的地方：</p><ul><li><strong>布局(layout)</strong>：求导后的向量/矩阵形状；</li><li><strong>链式顺序</strong>：比如f(g(x))求导问题中f’(g(x))和g’(x)的顺序排列问题；</li><li><strong>数据优先级</strong>：列优先或者行优先，其中列优先时默认为列向量，对于矩阵的向量展开顺序为逐列展开。</li></ul><p>最有意思的地方在于，我们平时所碰见的机器学习和深度学习相关的指标计算，常常会利用范数进行刻画：</p><p>$$ \left\{ \begin{array}{l} {\left\| X \right\|_1}{\rm{ = }}{{{\rm{\vec 1}}}^T} \times vec\left( X \right)\\ {\left\| X \right\|_2}{\rm{ = }}tr\left( {{X^T}X} \right)\\ {\left\| X \right\|_\infty }{\rm{ = }}max\left( {\left| X \right|} \right) = {{\vec k}^T} \times vec\left( X \right) \end{array} \right. $$<br>其中的<code>k</code>向量是一个除了最大值位置处为1，其他位置都为0的矩阵的向量展开。从上面可以看到，如果不考虑函数链式法则（即每次都从结果处开始求导)，那么基本上所有的一阶导数问题都能归结为标量对于向量和矩阵的导数问题。但是如果是分段式的求导或者计算二阶导数则需要用到向量和矩阵之间的相互求导。</p><p>这里我先确定一些统一规则：</p><ul><li><p><strong>符号说明</strong>：标量用小写字母(a,b,c,…)表示，向量用加粗小写字母(<strong>x</strong>,<strong>y</strong>,<strong>z</strong>,<strong>w</strong>,…)表示，矩阵用加粗的大写字母(<strong>A</strong>,<strong>B</strong>,<strong>C</strong>…)表示；</p></li><li><p><strong>列优先</strong>，即所有向量都是列向量，矩阵的向量化也是逐列展开；</p></li><li><p><strong>分母布局</strong>，即求导后的变量形状与分母对齐，至少保证行数一致，这一点是为了方便待求变量的更新，比如：<br>$$ \begin{array}{*{20}{l}} {\frac{{\partial y}}{{\partial {\bf{x}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial y}}{{\partial {x_1}}}}\\ {\frac{{\partial y}}{{\partial {x_2}}}}\\ \vdots \\ {\frac{{\partial y}}{{\partial {x_n}}}} \end{array}} \right].}\\ {\frac{{\partial {\bf{y}}}}{{\partial x}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial {y_1}}}{{\partial x}}}&{\frac{{\partial {y_2}}}{{\partial x}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial x}}} \end{array}} \right].}\\ {\frac{{\partial {\bf{y}}}}{{\partial {\bf{x}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial {y_1}}}{{\partial {x_1}}}}&{\frac{{\partial {y_2}}}{{\partial {x_1}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_1}}}}\\ {\frac{{\partial {y_1}}}{{\partial {x_2}}}}&{\frac{{\partial {y_2}}}{{\partial {x_2}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_2}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {\frac{{\partial {y_1}}}{{\partial {x_n}}}}&{\frac{{\partial {y_2}}}{{\partial {x_n}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_n}}}} \end{array}} \right].}\\ {\frac{{\partial y}}{{\partial {\bf{X}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial y}}{{\partial {x_{11}}}}}&{\frac{{\partial y}}{{\partial {x_{12}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{1q}}}}}\\ {\frac{{\partial y}}{{\partial {x_{21}}}}}&{\frac{{\partial y}}{{\partial {x_{22}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{2q}}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {\frac{{\partial y}}{{\partial {x_{p1}}}}}&{\frac{{\partial y}}{{\partial {x_{p2}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{pq}}}}} \end{array}} \right].} \end{array} $$</p></li></ul><p>可以看到上面两个向量之间的求导得到的是一个矩阵，这个是符合求导规则的，但是机器学习中的参数更新我们会利用梯度进行更新，这一点会在以后讲解梯度下降法、牛顿法、共轭梯度下降法的时候介绍。既然要更新，则肯定需要保证导数和参数形状一模一样，那么我们根据梯度的累加性得到：</p><p>$$ \vec x = \vec x - \eta \frac{{\partial \vec y}}{{\partial \vec x}} \times \vec 1 $$</p><p>这样做还有一个好处，那就是待求参数的转置不影响求导结果，即：</p><p>$$ \frac{{\partial \left( {Ax} \right)}}{{\partial x}} = \frac{{\partial \left( {{x^T}{A^T}} \right)}}{{\partial x}}={A^T} $$</p><ul><li><p><strong>链式法则</strong>和<strong>运算法则</strong>对于分子的组成很敏感，如果详细推导会很麻烦，当分子为标量时，可以用第2章的内容进行推导，当分子为向量时，有如下规律：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190622\1561216200833.png" alt="1561216200833"></p></li></ul><h2 id="2-矩阵求导"><a href="#2-矩阵求导" class="headerlink" title="2.矩阵求导"></a>2.矩阵求导</h2><h3 id="2-1-迹的引入"><a href="#2-1-迹的引入" class="headerlink" title="2.1  迹的引入"></a>2.1 迹的引入</h3><p>上面所介绍的是标量和向量相关的求导法则，但是涉及到矩阵问题的时候，会出现不一样的地方。由于向量或者矩阵求导的本质是相应元素标量之间的求导，所以对于一个<code>m x n</code>的矩阵<strong>A</strong>和<code>p x q</code>的矩阵<strong>B</strong>之间导数，必定是一个<code>mp x nq</code>的大型矩阵，这个操作利用简单Hadamard矩阵乘法是做不到的，我们也无法利用上面的规则。所以对于这类问题可以利用方阵的迹进行求解，其中标量也可以视为是<code>1 x 1</code>大小的方阵，下面所介绍的规则同样适用于标量对向量的求导。</p><p>首先我们来研究一下一元微积分和多元微积分微分的原始定义：</p><p>$$ \left\{ \begin{array}{l} df = f'\left( x \right)dx\\ df = \sum\limits_{i = 1}^n {\frac{{\partial f}}{{\partial {x_i}}}d{x_i}} \end{array} \right. \Rightarrow df = {\left( {\frac{{\partial f}}{{\partial x}}} \right)^T}dx $$<br>可以发现可以用导数的转置向量与微分向量的内积来表示导数，所以矩阵的微分也可以定义为：</p><p>$$ df = \sum\limits_{i = 1}^m {\sum\limits_{j = 1}^n {\frac{{\partial f}}{{\partial {X_{ij}}}}} } d{X_{ij}} = tr\left( {{{\frac{{\partial f}}{{\partial X}}}^T}dX} \right) $$<br>，其中<code>tr</code>表示矩阵的迹，即矩阵对角线元素之和。不同于上面的求导法则，下面我们针对分子为<code>标量</code>或者<code>方阵</code>的矩阵/向量求导设置新的运算法则：</p><ul><li><strong>微分法则</strong>，其中$\odot$表示矩阵点乘，$\sigma$表示元素级函数：</li></ul><p>$$ \left\{ {\begin{array}{*{20}{l}} {d\left( {X \pm Y} \right) = dX \pm dY}\\ \begin{array}{l} d\left( {XY} \right) = \left( {dX} \right)Y + XdY\\ d\left( {X \odot Y} \right) = dX \odot Y + X \odot dY\\ d\left( {\sigma \left( X \right)} \right) = \sigma '\left( X \right) \odot dX \end{array} \end{array}} \right. $$</p><ul><li><p><strong>微分与迹</strong>：<br>$$ \left\{ \begin{array}{l} d\left( {tr(X)} \right) = tr\left( {dX} \right)\\ dx = tr\left( {dx} \right)\\ d\left( {{X^T}} \right) = {\left( {dX} \right)^T}\\ tr\left( {{X^T}} \right) = tr\left( X \right) \end{array} \right. $$</p></li><li><p><strong>迹的运算法则</strong>：<br>$$ \left\{ \begin{array}{l} tr\left( {A \pm B} \right) = tr\left( A \right) \pm tr\left( B \right)\\ tr\left( {ABC} \right) = tr\left( {CAB} \right) = tr\left( {BCA} \right)\\ tr\left( {{A^T}\left( {B \odot C} \right)} \right) = tr\left( {{{\left( {A \odot B} \right)}^T}C} \right) \end{array} \right. $$<br>其中第二项并不是说矩阵的迹具有交换律，而是轮换不变性，即向左向右移位。</p></li><li><p><strong>特殊运算</strong>：<br>$$ \left\{ \begin{array}{l} d\left( {X{X^{ - 1}}} \right) = dX{X^{ - 1}} + Xd{X^{ - 1}} = dI = 0 \Rightarrow d{X^{ - 1}} = - {X^{ - 1}}dX{X^{ - 1}}\\ d\left| X \right| = tr\left( {{X_{adj}}dX} \right)\mathop \to \limits^{inversable} \left| X \right|tr\left( {{X^{ - 1}}dX} \right) \end{array} \right. $$</p></li></ul><h3 id="2-2-算例说明"><a href="#2-2-算例说明" class="headerlink" title="2.2 算例说明"></a>2.2 算例说明</h3><p>由于绝大部分机器学习算法中涉及到的求导都是标量对向量和矩阵的，所以这里用迹来求解非常方便，我们还是保证列优先：</p><ul><li><p>eg1: 假设<code>y=AXB</code>是标量，那么其关于<code>X</code>的导数为：<br>$$ \begin{array}{l} dy = tr\left( {dy} \right)\\ = tr\left( {d\left( {AXB} \right)} \right)\\ = tr\left( {dAXB + AdXB + AXdB} \right)\\ = tr\left( {AdXB} \right)\\ = tr\left( {BAdX} \right)\\ = tr\left( {{{\left( {{A^T}{B^T}} \right)}^T}dX} \right)\\ \Rightarrow \frac{{\partial y}}{{\partial X}} = {\left( {{A^T}{B^T}} \right)^T} \end{array} $$</p></li><li><p>eg2:<br>$$ \begin{array}{l} f = tr\left( {{Y^T}MY} \right),Y = \sigma \left( {WX} \right)\\ \Rightarrow df\\ = tr\left( {d{Y^T}MY + {Y^T}MdY} \right)\\ = tr\left( {{Y^T}{M^T}dY + {Y^T}MdY} \right)\\ = tr\left( {{Y^T}\left( {{M^T} + M} \right)dY} \right)\\ \Rightarrow \frac{{\partial f}}{{\partial Y}} = \left( {M + {M^T}} \right)Y\\ \because dY = \sigma '\left( {WX} \right) \odot \left( {WdX} \right)\\ \therefore df = tr\left( {{{\frac{{\partial f}}{{\partial Y}}}^T}dY} \right)\\ = tr\left( {{{\frac{{\partial f}}{{\partial Y}}}^T}\left( {\sigma '\left( {WX} \right) \odot \left( {WdX} \right)} \right)} \right)\\ = tr\left( {\left( {{{\frac{{\partial f}}{{\partial Y}}}^T} \odot \sigma '\left( {WX} \right)} \right)WdX} \right)\\ \Rightarrow \frac{{\partial f}}{{\partial X}} = {W^T}\left( {\frac{{\partial f}}{{\partial Y}} \odot \sigma '{{\left( {WX} \right)}^T}} \right) \end{array} $$</p></li><li><p>eg3: 岭回归模型，<code>X</code>为<code>m x n</code>的矩阵，<code>w</code>为<code>n x 1</code>的向量，<code>y</code>为<code>m x 1</code>的向量<br>$$ \begin{array}{l} l = {\left\| {Xw - y} \right\|^2} + \lambda \left\| w \right\|\\ = {\left( {Xw - y} \right)^T}\left( {Xw - y} \right) + {w^T}w\\ \Rightarrow dl = tr\left( {dl} \right)\\ = tr\left( {d{{\left( {Xw - y} \right)}^T}\left( {Xw - y} \right) + {{\left( {Xw - y} \right)}^T}d\left( {Xw - y} \right) + d{w^T}w + {w^T}dw} \right)\\ \because d\left( {Xw} \right) = dXw + Xdw = Xdw\\ \therefore dl = tr\left( {{{\left( {Xdw} \right)}^T}\left( {Xw - y} \right) + {{\left( {Xw - y} \right)}^T}Xdw + {w^T}dw + {w^T}dw} \right)\\ = tr\left( {2{{\left( {Xw - y} \right)}^T}Xdw + 2{w^T}dw} \right)\\ = tr\left( {2{{\left( {{X^T}\left( {Xw - y} \right)} \right)}^T}dw + 2{w^T}dw} \right)\\ \therefore \frac{{\partial l}}{{\partial w}} = 2\left( {{X^T}\left( {Xw - y} \right) + w} \right) = 0\\ \therefore \left( {{X^T}X + I} \right)w = {X^T}y\\ \therefore w = {\left( {{X^T}X + I} \right)^{ - 1}}{X^T}y \end{array} $$</p></li><li><p>eg4: 神经网络(交叉熵损失函数+softmax激活函数+2层神经网络)，<code>y</code>是一个除一个元素为1之外，其他元素都为0的<code>m x 1</code>向量，<code>W2</code>为<code>m x p</code>矩阵，<code>W1</code>为<code>p x n</code>矩阵，<code>x</code>是<code>n x 1</code>向量：<br>$$ \begin{array}{l} l = - {y^T}\log softmax\left( {{W_2}\sigma \left( {{W_1}x} \right)} \right)\\ \Rightarrow \left\{ \begin{array}{l} l = - {y^T}\log {h_2}\\ {h_2} = softmax\left( {{a_2}} \right) = \frac{{{e^{{a_2}}}}}{{{{\vec 1}^T}{e^{{a_2}}}}}\\ {a_2} = {W_2}{h_1}\\ {h_1} = \sigma \left( {{a_1}} \right) = \frac{1}{{1 + {e^{ - {a_1}}}}}\\ {a_1} = {W_1}x \end{array} \right.\\ \therefore dl = tr\left( {dl} \right) = tr\left( { - {y^T}\left( {\frac{1}{{{h_2}}} \odot d{h_2}} \right)} \right)\\ \left( 1 \right)if\;i = j\\ \frac{{\partial softmax\left( {a_{_2}^i} \right)}}{{\partial a_{_2}^j}} = \left( {\frac{{{e^{a_{_2}^i}}}}{{k + {e^{a_{_2}^i}}}}} \right)'\\ = \left( {1 - \frac{k}{{k + {e^{a_{_2}^i}}}}} \right)'\\ = \frac{{k{e^{a_{_2}^i}}}}{{{{\left( {k + {e^{a_{_2}^i}}} \right)}^2}}}\\ = \frac{{{e^{a_{_2}^i}}}}{{k + {e^{a_{_2}^i}}}}\frac{k}{{k + {e^{a_{_2}^i}}}}\\ = softmax\left( {a_{_2}^i} \right)\left( {1 - softmax\left( {a_{_2}^i} \right)} \right)\\ = h_2^i\left( {1 - h_2^i} \right)\\ \left( 2 \right)if\;i \ne j:\\ \frac{{\partial softmax\left( {a_{_2}^i} \right)}}{{\partial a_{_2}^j}} = \left( {\frac{{{e^{a_{_2}^i}}}}{{n + {e^{a_{_2}^j}}}}} \right)'\\ = \left( { - \frac{{{e^{a_{_2}^i}}{e^{a_{_2}^j}}}}{{n + {e^{a_{_2}^j}}}}} \right)\\ = - \frac{{{e^{a_{_2}^i}}{e^{a_{_2}^j}}}}{{{{\left( {n + {e^{a_{_2}^j}}} \right)}^2}}}\\ = - h_2^ih_2^j\\ \therefore 利用向量对向量求导的定义可得：\\ \therefore d{h_2} = (diag\left( {{h_2}} \right) - {h_2}h_2^T)d{a_2}\\ dl = tr\left( { - {y^T}\left( {\frac{1}{{{h_2}}} \odot \left( {diag\left( {{h_2}} \right) - {h_2}h_2^T} \right)d{a_2}} \right)} \right)\\ = tr\left( { - {{\left( {y \odot \frac{1}{{{h_2}}}} \right)}^T}\left( {diag\left( {{h_2}} \right) - {h_2}h_2^T} \right)d{a_2}} \right)\\ = tr\left( {\left( { - {y^T} + h_2^T} \right)d{a_2}} \right)\\ = tr\left( {{{\left( {{h_2} - y} \right)}^T}d{a_2}} \right)\\ \because d{a_2} = d{W_2}{h_1} = {W_2}d{h_1}\\ \therefore dl = tr\left( {{{\left( {{h_2} - y} \right)}^T}d{W_2}{h_1}} \right) = tr\left( {{h_1}{{\left( {{h_2} - y} \right)}^T}d{W_2}} \right)\\ \Rightarrow \frac{{\partial l}}{{\partial {W_2}}} = \left( {{h_2} - y} \right)h_1^T\\ \because d{h_1} = \sigma '\left( {{a_1}} \right) \odot d{a_1} = {h_1} \odot \left( {1 - {h_1}} \right) \odot d{a_1}\\ d{a_1} = d{W_1}x\\ \therefore dl = tr\left( {{{\left( {{h_2} - y} \right)}^T}{W_2}\left( {{h_1} \odot \left( {1 - {h_1}} \right) \odot d{W_1}x} \right)} \right)\\ = tr\left( {{{\left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right)}^T}d{W_1}x} \right)\\ = tr\left( {x{{\left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right)}^T}d{W_1}} \right)\\ \Rightarrow \frac{{\partial l}}{{\partial {W_1}}} = \left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right){x^T} \end{array} $$</p></li></ul><p>这里主要是为了强调这个过程，实际上<code>log</code>和<code>softmax</code>在一起时，可以将乘除法转化为加减法。</p><h3 id="2-3-矩阵拓展"><a href="#2-3-矩阵拓展" class="headerlink" title="2.3 矩阵拓展"></a>2.3 矩阵拓展</h3><p>上面只介绍了<code>向量与标量</code>、<code>向量与向量</code>和<code>矩阵与标量</code>的求导规则，但是对于<code>矩阵与矩阵</code>和<code>矩阵与向量</code>的求导尚未说明，这里只简单说明。</p><p>我们在之前讨论过，矩阵求导本质上是矩阵对矩阵每一个元素的求导，那么为了更方便整体求导，不妨先将矩阵转化为向量，即将矩阵拉伸为列向量，然后再求导，其同样满足：</p><p>$$ \begin{array}{l} vec\left( {dF} \right) = {\frac{{\partial F}}{{\partial X}}^T}vec\left( {dX} \right)\\ \left\{ \begin{array}{l} vec\left( {AXB} \right) = \left( {{B^T} \otimes A} \right)vec\left( X \right)\\ vec\left( {{A^T}} \right) = {K_{mn}}vec\left( A \right)\\ vec\left( {A \odot X} \right) = diag\left( A \right)vec\left( X \right) \end{array} \right. \end{array} $$<br>其中$\otimes$表示Kronecker积，${K_{mn}}$表示转换矩阵，其实就是转换前后的仿射矩阵，关于这两个算子有如下规律：</p><p>$$ \left\{ \begin{array}{l} {\left( {A \otimes B} \right)^T} = {A^T} \otimes {B^T}\\ vec\left( {a{b^T}} \right) = b \otimes a\\ \left( {A \otimes B} \right)\left( {C \otimes D} \right) = \left( {AC} \right) \otimes \left( {BD} \right)\\ {K_{mn}} = K_{nm}^T\\ {K_{mn}}K_{nm}^T = I\\ {K_{pm}}\left( {A \otimes B} \right){K_{nq}} = B \otimes A \end{array} \right. $$<br>如果想利用上面的方式求导，在参数更新的时候，比如<code>AXB</code>，其中<code>A(m x n)</code>,<code>X(n x p)</code>,<code>B(p x q)</code>，需要对导数(<code>pn x qm</code>)乘以一个<code>qm x 1</code>的全1向量，用来将梯度累加，然后重塑为矩阵<code>n x p</code>。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://en.wikipedia.org/wiki/Matrix_calculus">https://en.wikipedia.org/wiki/Matrix_calculus</a></li><li><a href="https://zhuanlan.zhihu.com/p/24709748">https://zhuanlan.zhihu.com/p/24709748</a></li><li><a href="https://zhuanlan.zhihu.com/p/24863977">https://zhuanlan.zhihu.com/p/24863977</a></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;无论是机器学习还是最近大热的深度学习算法，很多情况下都需要用到矩阵求导方面的知识，比如拉格朗日乘子法、梯度下降法等等，但是矩阵求导不同于传统的标量求导法则，存在很多冲突。网上以及各种论文中所推导的梯度计算方法，标准都不一样，容易把人搞迷糊。所以我这里理一下矩阵/向量求导相关的原理，并且确定一套属于自己的求导标准，方便自己研究。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数学基础" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="矩阵求导" scheme="https://huangpiao.tech/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>详解YOLO系列</title>
    <link href="https://huangpiao.tech/2019/02/09/%E8%AF%A6%E8%A7%A3YOLO%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95/"/>
    <id>https://huangpiao.tech/2019/02/09/详解YOLO系列算法/</id>
    <published>2019-02-09T15:50:00.000Z</published>
    <updated>2019-02-09T15:50:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:21 GMT+0800 (中国标准时间) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。</p></blockquote><a id="more"></a><h2 id="1-目标检测简介"><a href="#1-目标检测简介" class="headerlink" title="1.目标检测简介"></a>1.目标检测简介</h2><p><a href="https://pjreddie.com/darknet/yolo/">YOLO</a>(You Only Look Once)的作者非常萌，无论是写作风格、表情包还是Github风格，都表现出他是一个有趣的人。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/sayit.jpg" alt="img"></p><p>好了，言归正传，众所众知，目标检测算法的核心在于：</p><ul><li><p><strong>候选区域/框/角点等的确定</strong>。神经网络/深度学习本质是分类，那么对于目标检测问题，我们需要将其转化为分类问题，因此许多研究者发现需要先确定候选位置，然后对候选位置进行分类判断。这里，候选区域的选取从最初的滑窗方式。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220845247.png" alt="img"></p><p>慢慢演变到以<code>Selective Search</code>(过分割+分层聚类)为主的RCNN算法，为了更高效的生成候选区域，我们又利用卷积和池化过程近似滑窗从而有了Fast RCNN算法。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220928555.png" alt="img"></p><p>再演变至以<code>anchor box</code>为代表的Faster RCNN、SSD和YOLO等系列算法，其原理在于可以对每一个ROI区域的中心，给定一个假设的长宽比，由此作为候选区域，再在后面利用回归层精修回归框。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549555635485.png" alt="1549555635485"></p><p>最后到现在的<code>Corner</code>为代表的CornerNet算法，不断地提升候选框提取效率、候选框有效率、候选框精准度以及与分类框架的融合。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549556050439.png" alt="1549556050439"></p></li><li><p><strong>判断目标属于什么类别</strong>。有了候选区域，那么就可以利用很简单的级联全连接层判定每个候选区域属于前景/背景的概率，以及属于各个目标类别的概率。</p></li><li><p><strong>目标精定位</strong>。目标框的描述包括目标的中心/角点位置和宽高，这些仅仅依赖候选区域是不够精确的，那么就需要合理的设计损失函数，利用多个全连接层进行进一步的回归，得到目标框的精修位置。当然CornerNet里面采用骨骼关键点检测里面的Hourglass结构作为backbone，再加上Corner Pooling层预测角点位置，不存在精修。</p></li></ul><h2 id="2-Darknet"><a href="#2-Darknet" class="headerlink" title="2.Darknet"></a>2.Darknet</h2><h3 id="2-1-Darknet网络框架"><a href="#2-1-Darknet网络框架" class="headerlink" title="2.1 Darknet网络框架"></a>2.1 Darknet网络框架</h3><p>目前来说，无论是在目标检测、目标识别还是目标分割、姿态分析等领域，都会用到各种各样的backbone网络，最常用的就是基于图像分类的backbone网络，因为深度学习本质是分类，而绝大多数分类网络都会在ImageNet竞赛中进行测试，我们从AlexNet，VGGNet到GoogleNet，再到ResNet/DenseNet等，已经见过很多优秀的骨干网络结构了，其中很多优秀的子模块也被用于其他网络结构，如：卷积+池化+BatchNorm+Relu的组合、Inception各个版本结构、残差模块、1x1卷积核等等。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/822124-20160902160437324-793316644.png" alt="img"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180114183212429.png" alt="img"></p><p>而Darknet实际上也是YOLO作者实现的一个backbone网络，其改进对象主要是GoogleNet，将GoogleNet中的Inception结构改成了串行的结构，从而使得网络速度更快，而效果仅仅损失了一点。可以看到下面的网络结构有24个卷积层，外加2个全连接层。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221342432.png" alt="img"></p><h3 id="2-2-Darknet训练框架"><a href="#2-2-Darknet训练框架" class="headerlink" title="2.2 Darknet训练框架"></a>2.2 Darknet训练框架</h3><p>除此之外呢，<a href="https://github.com/pjreddie/darknet">Darknet</a>也是一个深度学习框架，其框架设计与<a href="https://github.com/BVLC/caffe">caffe</a>基本一致，只不过是用C语言写的，其整体框架十分简洁，所以编译速度非常快。如果学习过caffe的话，应该很容易上手darknet。其中<code>darknet.h</code>就类似于caffe中的<code>caffe.proto</code>，定义了所有数据结构，而网络的构建是利用了<code>cfg</code>格式文件，即利用<code>key=value</code>方式搭建网络，这种方式的问题在于对于复杂网络的设计非常复杂，很难写。另外，由于darknet是纯C框架，所以要想增加自定义层的话会比较麻烦，主要是因为没有好的设计模式和面向对象设计，导致使用者需要完全读懂整个框架，而且很难实现共享内存和逐层不同学习率。</p><p>当然，darknet框架的安装也是很简单的，除开显卡驱动和CUDA、cudnn等配置之外，只需要从git上面clone下来源码，然后<code>make</code>即可，这里我们不考虑Windows版本的，github上面有相应的<a href="https://github.com/AlexeyAB/darknet">教程</a>。不仅可以利用原始的C接口，还能利用将其编译为动态链接库供C++接口调用，见<a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp">这里</a>，不过我主要是利用python接口调用，这里呢就存在一个问题，即原始darknet数据结构是<code>image</code>，如果利用<code>ctypes</code>进行C/Python混合编程的话，需要设计到numpy数据结构与image数据结构的交互，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> darknet <span class="keyword">as</span> dn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMAGE</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"w"</span>, c_int),</span><br><span class="line">                (<span class="string">"h"</span>, c_int),</span><br><span class="line">                (<span class="string">"c"</span>, c_int),</span><br><span class="line">                (<span class="string">"data"</span>, POINTER(c_float))]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETECTION</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"bbox"</span>, BOX),</span><br><span class="line">                (<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"prob"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"mask"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"objectness"</span>, c_float),</span><br><span class="line">                (<span class="string">"sort_class"</span>, c_int)]</span><br><span class="line">    </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">array_to_image</span><span class="params">(arr)</span>:</span></span><br><span class="line">    arr = arr.transpose(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    c = arr.shape[<span class="number">0</span>]</span><br><span class="line">    h = arr.shape[<span class="number">1</span>]</span><br><span class="line">    w = arr.shape[<span class="number">2</span>]</span><br><span class="line">    arr = (arr/<span class="number">255.0</span>).flatten()</span><br><span class="line">    data = dn.c_array(dn.c_float, arr)</span><br><span class="line">    im = dn.IMAGE(w,h,c,data)</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><p>上面这种利用图像数据结构转化的方式，会占用很多时间，所以我们可以利用numpy的c接口实现数据结构转换，具体如下：</p><p>先在<code>src/image.c</code> line 558左右添加：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> NUMPY</span></span><br><span class="line"><span class="function">image <span class="title">ndarray_to_image</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span>* src, <span class="keyword">long</span>* shape, <span class="keyword">long</span>* strides)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h = shape[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> w = shape[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> c = shape[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span> step_h = strides[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> step_w = strides[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> step_c = strides[<span class="number">2</span>];</span><br><span class="line">    image im = make_image(w, h, c);</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">int</span> index1, index2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h; ++i)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k= <span class="number">0</span>; k &lt; c; ++k)&#123;</span><br><span class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; w; ++j)&#123;</span><br><span class="line"></span><br><span class="line">                    index1 = k*w*h + i*w + j;</span><br><span class="line">                    index2 = step_h*i + step_w*j + step_c*k;</span><br><span class="line">                    im.data[index1] = src[index2]/<span class="number">255.</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    rgbgr_image(im);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> im;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>然后在<code>src/image.h</code>19行左右添加：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> NUMPY</span></span><br><span class="line"><span class="function">image <span class="title">ndarray_to_image</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span>* src, <span class="keyword">long</span>* shape, <span class="keyword">long</span>* strides)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>再在<code>MakeFile</code>中加入：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(NUMPY)</span>, 1) </span><br><span class="line">COMMON+= -DNUMPY -I/usr/<span class="keyword">include</span>/python2.7/ -I/usr/lib/python2.7/dist-packages/numpy/core/<span class="keyword">include</span>/numpy/</span><br><span class="line">CFLAGS+= -DNUMPY</span><br><span class="line"><span class="keyword">endif</span></span><br></pre></td></tr></table></figure><p>并设置<code>Makefile</code></p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU=1</span><br><span class="line">CUDNN=1</span><br><span class="line">OPENCV=1</span><br><span class="line">OPENMP=0</span><br><span class="line">NUMPY=1</span><br><span class="line">DEBUG=0</span><br></pre></td></tr></table></figure><p>最后<code>python</code>接口为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nparray_to_image</span><span class="params">(self,img)</span>:</span></span><br><span class="line">    data = img.ctypes.data_as(POINTER(c_ubyte))</span><br><span class="line">    image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><p>这样的话，数据转换的速度大大提升,下面我附上我写的<code>darknet.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">c_array</span><span class="params">(ctype, values)</span>:</span></span><br><span class="line">    arr = (ctype*len(values))()</span><br><span class="line">    arr[:] = values</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BOX</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"x"</span>, c_float),</span><br><span class="line">                (<span class="string">"y"</span>, c_float),</span><br><span class="line">                (<span class="string">"w"</span>, c_float),</span><br><span class="line">                (<span class="string">"h"</span>, c_float)]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETECTION</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"bbox"</span>, BOX),</span><br><span class="line">                (<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"prob"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"mask"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"objectness"</span>, c_float),</span><br><span class="line">                (<span class="string">"sort_class"</span>, c_int)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMAGE</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"w"</span>, c_int),</span><br><span class="line">                (<span class="string">"h"</span>, c_int),</span><br><span class="line">                (<span class="string">"c"</span>, c_int),</span><br><span class="line">                (<span class="string">"data"</span>, POINTER(c_float))]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">METADATA</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"names"</span>, POINTER(c_char_p))]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObjectDetect</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,cfg_path = None, wight_path = None, meta_path = None, ctx = None)</span>:</span></span><br><span class="line">        lib = CDLL(os.path.dirname(os.path.realpath(__file__))+<span class="string">"/libdarknet.so"</span>, RTLD_GLOBAL)</span><br><span class="line"></span><br><span class="line">        lib.network_width.argtypes = [c_void_p]</span><br><span class="line">        lib.network_width.restype = c_int</span><br><span class="line">        lib.network_height.argtypes = [c_void_p]</span><br><span class="line">        lib.network_height.restype = c_int</span><br><span class="line"></span><br><span class="line">        predict = lib.network_predict</span><br><span class="line">        predict.argtypes = [c_void_p, POINTER(c_float)]</span><br><span class="line">        predict.restype = POINTER(c_float)</span><br><span class="line"></span><br><span class="line">        self.set_gpu = lib.cuda_set_device</span><br><span class="line">        self.set_gpu.argtypes = [c_int]</span><br><span class="line">        <span class="keyword">if</span> ctx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.set_gpu(ctx)</span><br><span class="line"></span><br><span class="line">        make_image = lib.make_image</span><br><span class="line">        make_image.argtypes = [c_int, c_int, c_int]</span><br><span class="line">        make_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.get_network_boxes = lib.get_network_boxes</span><br><span class="line">        self.get_network_boxes.argtypes = [c_void_p, c_int, c_int, c_float, c_float, POINTER(c_int), c_int, POINTER(c_int)]</span><br><span class="line">        self.get_network_boxes.restype = POINTER(DETECTION)</span><br><span class="line"></span><br><span class="line">        make_network_boxes = lib.make_network_boxes</span><br><span class="line">        make_network_boxes.argtypes = [c_void_p]</span><br><span class="line">        make_network_boxes.restype = POINTER(DETECTION)</span><br><span class="line"></span><br><span class="line">        self.free_detections = lib.free_detections</span><br><span class="line">        self.free_detections.argtypes = [POINTER(DETECTION), c_int]</span><br><span class="line"></span><br><span class="line">        free_ptrs = lib.free_ptrs</span><br><span class="line">        free_ptrs.argtypes = [POINTER(c_void_p), c_int]</span><br><span class="line"></span><br><span class="line">        network_predict = lib.network_predict</span><br><span class="line">        network_predict.argtypes = [c_void_p, POINTER(c_float)]</span><br><span class="line"></span><br><span class="line">        self.load_net = lib.load_network</span><br><span class="line">        self.load_net.argtypes = [c_char_p, c_char_p, c_int]</span><br><span class="line">        self.load_net.restype = c_void_p</span><br><span class="line">        self.net = self.load_net(cfg_path, weight_path, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.do_nms_obj = lib.do_nms_obj</span><br><span class="line">        self.do_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]</span><br><span class="line"></span><br><span class="line">        do_nms_sort = lib.do_nms_sort</span><br><span class="line">        do_nms_sort.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]</span><br><span class="line"></span><br><span class="line">        self.free_image = lib.free_image</span><br><span class="line">        self.free_image.argtypes = [IMAGE]</span><br><span class="line"></span><br><span class="line">        letterbox_image = lib.letterbox_image</span><br><span class="line">        letterbox_image.argtypes = [IMAGE, c_int, c_int]</span><br><span class="line">        letterbox_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.load_meta = lib.get_metadata</span><br><span class="line">        lib.get_metadata.argtypes = [c_char_p]</span><br><span class="line">        lib.get_metadata.restype = METADATA</span><br><span class="line">        self.meta = self.load_meta(meta_path)</span><br><span class="line"></span><br><span class="line">        load_image = lib.load_image_color</span><br><span class="line">        load_image.argtypes = [c_char_p, c_int, c_int]</span><br><span class="line">        load_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        rgbgr_image = lib.rgbgr_image</span><br><span class="line">        rgbgr_image.argtypes = [IMAGE]</span><br><span class="line"></span><br><span class="line">        self.ndarray_image = lib.ndarray_to_image</span><br><span class="line">        self.ndarray_image.argtypes = [POINTER(c_ubyte), POINTER(c_long), POINTER(c_long)]</span><br><span class="line">        self.ndarray_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.predict_image = lib.network_predict_image</span><br><span class="line">        self.predict_image.argtypes = [c_void_p, IMAGE]</span><br><span class="line">        self.predict_image.restype = POINTER(c_float)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nparray_to_image</span><span class="params">(self,img)</span>:</span></span><br><span class="line">        data = img.ctypes.data_as(POINTER(c_ubyte))</span><br><span class="line">        image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides)</span><br><span class="line">        <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detect</span><span class="params">(self,img, thresh=<span class="number">.5</span>, hier_thresh=<span class="number">.5</span>, nms=<span class="number">.45</span>)</span>:</span></span><br><span class="line">        im = self.nparray_to_image(img)</span><br><span class="line">        num = c_int(<span class="number">0</span>)</span><br><span class="line">        pnum = pointer(num)</span><br><span class="line">        self.predict_image(self.net, im)</span><br><span class="line">        dets = self.get_network_boxes(self.net, im.w, im.h, thresh, hier_thresh, <span class="keyword">None</span>, <span class="number">0</span>, pnum)</span><br><span class="line">        num = pnum[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> (nms): self.do_nms_obj(dets, num, self.meta.classes, nms);</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(meta.classes):</span><br><span class="line">                <span class="keyword">if</span> dets[j].prob[i] &gt; <span class="number">0</span>:</span><br><span class="line">                    b = dets[j].bbox</span><br><span class="line">                    res.append((self.meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))</span><br><span class="line">        res = sorted(res, key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line">        self.free_image(im)</span><br><span class="line">        self.free_detections(dets, num)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model = ObjectDetect(<span class="string">"cfg/yolo.cfg"</span>, <span class="string">"yolo.weights"</span>,<span class="string">"cfg/coco.data"</span>)</span><br><span class="line">    cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">    ret, img = cap.read()</span><br><span class="line">    r = model.detect(img)</span><br><span class="line">    print(r)</span><br></pre></td></tr></table></figure><h2 id="3-YOLOv1"><a href="#3-YOLOv1" class="headerlink" title="3.YOLOv1"></a>3.YOLOv1</h2><h3 id="3-1-YOLO网络框架"><a href="#3-1-YOLO网络框架" class="headerlink" title="3.1 YOLO网络框架"></a>3.1 YOLO网络框架</h3><p>作者为了让backbone网络具有更好的性能，取了上面提到的darknet版本的前20个卷积层，然后利用一个全局池化层和一个全连接层，搭建了一个预训练网络，其中全局池化层是指的将一个通道内的所有元素平均，这一点在YOLO系列版本中都有体现：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549636582826.png" alt="1549636582826"></p><p>可以看到YOLOv1才用的darknet版本(Darknet Reference)效果最差，其余的我们后面再说。有了预训练模型之后，我们再以上面提到的Darknet的完整框架（24卷积层+2全连接层）进行训练，其中网络输入大小固定为448x448。然后将最后一层的输出形状改为7x7x30。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221649338.jpg" alt="img"></p><p>对于网络的输出，我们可以这样理解，30=20+2+4x2，其中20指的是VOC数据集的类别数，即20个类别的概率，2指的是两个目标框的置信度，然后每个通道预测2个目标框，所以就是两个(x,y,w,h)即8个元素。那么作者在论文中所提到的网格划分是怎么体现的呢，这里要通过最后一个卷积层的输出来看，即7x7x1024，可以看到特征图的尺寸是7x7，那么根据卷积网络的特点，每一层的输出特征图上的每一个像素点都会对应着输入特征图的一个区域，也就是这个像素点的<strong>感受野</strong>，那么在最后一层卷积层输出特征图上，也是如此，所以我们可以认为是<strong>将原图划分成了7X7的网格区域，每个网格预测20个类别的概率，目标框置信度以及两个目标框信息，其中每个目标的中心位置都会转换至网格区域内</strong>。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221547656.png" alt="img"></p><p>另外要说明的是，我在最新的github版本中发现，最后两个全连接层被替换成了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[local]</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=256</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[dropout]</span><br><span class="line">probability=.5</span><br><span class="line"></span><br><span class="line">[connected]</span><br><span class="line">output= 1715</span><br><span class="line">activation=linear</span><br></pre></td></tr></table></figure><p>其中的<code>connected</code>不用多说，就是全连接层，只不过节点数变成了1714,即7x7x35，那么这个35则说明每个网格区域会输出：20个类别概率，3个目标框置信度，3个目标框信息包含(x,y,w,h)。对于每个框所包含的物体判别方式则是采用了贝叶斯公式，将上述各个类别的概率作为条件概率。因此每个类别的真实置信度计算方式如下：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}Confidenc{e_{object}} = P\left( {object} \right) \times IOU_{truth}^{pred}\\Confidenc{e_{clas{s_i}}} = Confidenc{e_{object}} \times P\left( {\left. {clas{s_i}} \right|object} \right)\end{array} \right.</script><p>也就是说<strong>每个目标框所输出的边框置信度，本身就包含了先验概率和IOU的乘积</strong>，这一点在YOLOv2论文中有所体现。另外我们还发现YOLOv1中是直接输出目标框的，而不是采用<code>anchor boxes</code>方式。</p><p>而<code>local</code>是用的<code>Locally Connected Layers</code>结构，这种结构跟1x1卷积方式不同，1x1卷积核是利用很多个1x1大小的卷积核遍历整个特征图，而<code>Locally Connected Layers</code>结构则是一个介于全连接和卷积网络之间的一个结构：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4-lc.png" alt="img"></p><p>可以看到，它也是利用卷积的方式进行计算的，不同的地方在于随着卷积的不断遍历，每个遍历位置的卷积核都不一样，即没有了卷积层所特有的共享内存。其好处在于更多的利用了空间相对区域特征以及整体特征，从而提升了一点效果。</p><h3 id="3-2-数据准备"><a href="#3-2-数据准备" class="headerlink" title="3.2 数据准备"></a>3.2 数据准备</h3><p>目前最常用的两个目标检测数据集分别是VOC和COCO，其中VOC数据集中的目标多为大目标,分为20个类别，而COCO数据集中有很多小而密集的目标，更加贴近实际，共80个类别，有几十万幅图像，几百万个目标实例。</p><p>对于YOLO的训练，我们需要将每个目标的信息进行转化，其中一个文件中包含所有目标信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class_label,center_x,center_y,width,height&gt;</span><br></pre></td></tr></table></figure><p>其中目标框信息都需要归一化，即除以对应的图像宽高，另一个文件中则是包含对应图像的地址。</p><h3 id="3-3-数据增强"><a href="#3-3-数据增强" class="headerlink" title="3.3 数据增强"></a>3.3 数据增强</h3><p>作者在训练中主要采用了 jittering 和 HSV 空间扰动两种数据增强方式，详细的过程比较复杂，我用 matlab 把过程复现了：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all;</span><br><span class="line"><span class="comment">%% 参数设置</span></span><br><span class="line">jitter = <span class="number">0.3</span>;<span class="comment">%抖动幅度</span></span><br><span class="line">hue = <span class="number">0.1</span>;<span class="comment">%色调变化幅度</span></span><br><span class="line">saturation = <span class="number">1.5</span>;<span class="comment">%饱和度变化幅度</span></span><br><span class="line">exposure = <span class="number">1.5</span>;<span class="comment">%曝光率变化幅度</span></span><br><span class="line">w = <span class="number">416</span>;<span class="comment">%网络输入的宽</span></span><br><span class="line">h = <span class="number">416</span>;<span class="comment">%网络输入的高</span></span><br><span class="line">filepath = <span class="string">'E:\201709\val\val\000e4adcf3a3a5a246351fd4a3e18ae9ac4d44a9.jpg'</span>;<span class="comment">%图片地址</span></span><br><span class="line"><span class="comment">%% jittering+resize</span></span><br><span class="line">I = imread(filepath);<span class="comment">%读取图像</span></span><br><span class="line">[oh,ow,~] = <span class="built_in">size</span>(I);<span class="comment">%读取原图宽高</span></span><br><span class="line">[dw,dh] = deal(<span class="built_in">floor</span>(ow*jitter),<span class="built_in">floor</span>(oh*jitter));<span class="comment">%计算抖动值上限</span></span><br><span class="line">pleft = <span class="built_in">floor</span>(-dw + <span class="number">2</span>*<span class="built_in">rand</span>*dw);<span class="comment">%随机化 left 抖动值</span></span><br><span class="line">pright = <span class="built_in">floor</span>(-dw + <span class="number">2</span>*<span class="built_in">rand</span>*dw);<span class="comment">%随机化 right 抖动值</span></span><br><span class="line">ptop = <span class="built_in">floor</span>(-dh + <span class="number">2</span>*<span class="built_in">rand</span>*dh);<span class="comment">%随机化 top 抖动值</span></span><br><span class="line">pbot = <span class="built_in">floor</span>(-dh + <span class="number">2</span>*<span class="built_in">rand</span>*dh);<span class="comment">%随机化 bot 抖动值</span></span><br><span class="line">swidth = ow - pleft - pright;<span class="comment">%计算抖动后的图像宽度</span></span><br><span class="line">sheight = oh - ptop - pbot;<span class="comment">%计算抖动后的图像高度</span></span><br><span class="line">sx = swidth/ow;<span class="comment">%计算 jittering 后图像宽度与原图的比例</span></span><br><span class="line">sy = sheight/oh;<span class="comment">%计算 jittering 后图像高度与原图的比例</span></span><br><span class="line"><span class="comment">%各向同性 crop</span></span><br><span class="line">crop_image = uint8(<span class="built_in">zeros</span>(sheight,swidth,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : sheight</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : swidth</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line">            r = <span class="built_in">max</span>(<span class="built_in">i</span> + ptop,<span class="number">1</span>);</span><br><span class="line">            r = <span class="built_in">min</span>(r,oh);</span><br><span class="line">            c = <span class="built_in">max</span>(<span class="built_in">j</span> + pleft,<span class="number">1</span>);</span><br><span class="line">            c = <span class="built_in">min</span>(c,ow);</span><br><span class="line">            crop_image(<span class="built_in">i</span>,<span class="built_in">j</span>,k) = I(r,c,k);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%图像大小调整 resize,双线性插值</span></span><br><span class="line">w_scale = (swidth<span class="number">-1</span>)/w;<span class="comment">%待变换宽尺度</span></span><br><span class="line">h_scale = (sheight<span class="number">-1</span>)/h;<span class="comment">%待变换高尺度</span></span><br><span class="line">resized_image = uint8(<span class="built_in">zeros</span>(w,h,<span class="number">3</span>));</span><br><span class="line">part = uint8(<span class="built_in">zeros</span>(w,sheight,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : sheight</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line"><span class="keyword">if</span> k == w</span><br><span class="line">part(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = crop_image(<span class="built_in">j</span>,swidth,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">sx = k*w_scale;</span><br><span class="line">ix = <span class="built_in">floor</span>(sx);</span><br><span class="line">                  dx = sx-ix;</span><br><span class="line">                  part(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = (<span class="number">1</span>-dx)*crop_image(<span class="built_in">j</span>,ix,<span class="built_in">i</span>)+dx*crop_image(<span class="built_in">j</span>,ix+<span class="number">1</span>,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> :<span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : h</span><br><span class="line">        sy = <span class="built_in">j</span>*h_scale;</span><br><span class="line">        iy = <span class="built_in">floor</span>(sy);</span><br><span class="line">        dy = sy - iy;</span><br><span class="line">        <span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line">        resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = (<span class="number">1</span>-dy)*part(iy,k,<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">j</span> &lt; h</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line">resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>)+dy*part(iy+<span class="number">1</span>,k,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%% 方式二 jittering+resizeing(随机)</span></span><br><span class="line">I = imread(filepath);<span class="comment">%读取图像</span></span><br><span class="line">[oh,ow,~] = <span class="built_in">size</span>(I);<span class="comment">%读取原图宽高</span></span><br><span class="line">[dw,dh] = deal(ow*jitter,oh*jitter);<span class="comment">%计算抖动值上限</span></span><br><span class="line">aspect_ratio = (ow-dw+<span class="number">2</span>*dw*<span class="built_in">rand</span>)/(oh-dh+<span class="number">2</span>*dh*<span class="built_in">rand</span>);<span class="comment">%计算 jittering 后的长宽比</span></span><br><span class="line">scale = <span class="number">0.25</span> + <span class="number">1.75</span>*<span class="built_in">rand</span>;<span class="comment">%对标准输入大小进行随机放缩，然后保证放缩后长宽比</span></span><br><span class="line"><span class="keyword">if</span> aspect_ratio &lt; <span class="number">1</span></span><br><span class="line">    nh = <span class="built_in">floor</span>(scale*h);</span><br><span class="line">    nw = <span class="built_in">floor</span>(nh*aspect_ratio);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    nw = <span class="built_in">floor</span>(scale*w);</span><br><span class="line">    nh = <span class="built_in">floor</span>(nw/aspect_ratio);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">[Dx,Dy] = deal(<span class="built_in">floor</span>((w-nw)*<span class="built_in">rand</span>),<span class="built_in">floor</span>((h-nh)*<span class="built_in">rand</span>));</span><br><span class="line">resized_image2 = uint8(<span class="number">0.5</span>*<span class="built_in">ones</span>(h,w,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> c = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> y = <span class="number">1</span> : nh</span><br><span class="line">        <span class="keyword">for</span> x = <span class="number">1</span> : nw</span><br><span class="line">            frx = x/nw*ow;</span><br><span class="line">            fry = y/nh*oh;</span><br><span class="line">            rx = <span class="built_in">floor</span>(frx);</span><br><span class="line">            ry = <span class="built_in">floor</span>(fry);</span><br><span class="line">            dx = frx - rx;</span><br><span class="line">            dy = fry - ry;</span><br><span class="line">            val = (<span class="number">1</span>-dy)*(<span class="number">1</span>-dx)*get_pixel(I,ry,rx,c)+dy*(<span class="number">1</span>-dx)*get_pixel(I,ry+<span class="number">1</span>,rx,c)+... (<span class="number">1</span>-dy)*dx*get_pixel(I,ry,rx+<span class="number">1</span>,c)+dy*dx*get_pixel(I,ry+<span class="number">1</span>,rx+<span class="number">1</span>,c);</span><br><span class="line"><span class="keyword">if</span> x+Dx&gt;<span class="number">0</span>&amp;&amp;x+Dx&lt;=w&amp;&amp;y+Dy&gt;<span class="number">0</span>&amp;&amp;y+Dy&lt;=h</span><br><span class="line">resized_image2(y+Dy,x+Dx,c) = val;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">1</span>)</span><br><span class="line">subplot(<span class="number">221</span>);imshow(I);title(<span class="string">'原图'</span>);</span><br><span class="line">subplot(<span class="number">222</span>);imshow(crop_image);title(<span class="string">'方式一 jittering'</span>);</span><br><span class="line">subplot(<span class="number">223</span>);imshow(resized_image);title(<span class="string">'方式一各向异性 resize'</span>);</span><br><span class="line">subplot(<span class="number">224</span>);imshow(resized_image2);title(<span class="string">'方式二 jittering+各向同性 resize'</span>);</span><br><span class="line"><span class="comment">%% 翻转</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">J = <span class="built_in">fliplr</span>(resized_image2);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">2</span>)</span><br><span class="line">imshow(J);</span><br><span class="line"><span class="comment">%% HSV 空间扰动</span></span><br><span class="line">dhue = -hue+<span class="number">2</span>*<span class="built_in">rand</span>*hue;<span class="comment">%随机化色调偏差</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">dsat = <span class="number">1</span> + <span class="built_in">rand</span>*(saturation<span class="number">-1</span>);<span class="comment">%随机化饱和度偏差</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">dsat = <span class="number">1</span>/(<span class="number">1</span> + <span class="built_in">rand</span>*(saturation<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">dexp = <span class="number">1</span> + <span class="built_in">rand</span>*(exposure<span class="number">-1</span>);<span class="comment">%随机化曝光率偏差</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">dexp = <span class="number">1</span>/(<span class="number">1</span> + <span class="built_in">rand</span>*(exposure<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">J = <span class="built_in">rgb2hsv</span>(J);<span class="comment">%将 RGB 空间转换到 HSV 空间</span></span><br><span class="line">temp = J(:,:,<span class="number">1</span>)+dhue;<span class="comment">%调整色调</span></span><br><span class="line">temp(temp&gt;<span class="number">1</span>) = temp(temp&gt;<span class="number">1</span>)<span class="number">-1</span>;</span><br><span class="line">temp(temp&lt;<span class="number">0</span>) = temp(temp&lt;<span class="number">0</span>)+<span class="number">1</span>;</span><br><span class="line">J(:,:,<span class="number">1</span>) = temp;</span><br><span class="line">J(:,:,<span class="number">2</span>) = J(:,:,<span class="number">2</span>)*dsat;<span class="comment">%调整饱和度</span></span><br><span class="line">J(:,:,<span class="number">3</span>) = J(:,:,<span class="number">3</span>)*dexp;<span class="comment">%调整曝光率/亮度</span></span><br><span class="line">J = <span class="built_in">hsv2rgb</span>(J);<span class="comment">%返回 RGB 空间</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">3</span>)</span><br><span class="line">imshow(J);title(<span class="string">'HSV 空间扰动'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">pixel</span> = <span class="title">get_pixel</span><span class="params">(image,i,j,c)</span></span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">i</span> &lt; <span class="number">1</span>||<span class="built_in">i</span>&gt;<span class="built_in">size</span>(image,<span class="number">1</span>)||<span class="built_in">j</span>&lt;<span class="number">1</span>||<span class="built_in">j</span>&gt;<span class="built_in">size</span>(image,<span class="number">2</span>)</span><br><span class="line">pixel = uint8(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">pixel = image(<span class="built_in">i</span>,<span class="built_in">j</span>,c);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">%Matlab 中使用工具箱函数 imresize 会更快：</span></span><br><span class="line"><span class="comment">%imresize(I,[w,h],’bilinear’)%双线性插值</span></span><br><span class="line"><span class="comment">%imresize(I,[w,h],’bicubic’)%双三次线性插值</span></span><br></pre></td></tr></table></figure><p>作者在 YOLO 和 YOLOv2 分别用了两种实现方法，第一种将 jittering 和 resize 分开了，采用双线性插值的方式，第二种则是将二者结合了，先在原图获取一定比例的亚像素值，然后再进行随机双线性插值，效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645871959.png" alt="1549645871959"></p><p>可以看到，方式一的 jittering 会将边界像素进行复制扩充，并且不会内部像素会进行重排，所以是各向异性 resize。而方式二就好像在保证原始图像比例的前提下，通过填充 0 像素达到规定尺寸，所以是各向同性 resize。然后随机将图像进行左右翻转，最后就是 HSV 空间扰动，具体原理还是直接看代码，由于每次随机的值都不一样，所以下面的图可能与上面的不是一致的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645895056.png" alt="1549645895056"></p><h3 id="3-4-训练技巧"><a href="#3-4-训练技巧" class="headerlink" title="3.4 训练技巧"></a>3.4 训练技巧</h3><p>YOLO训练过程中采用了很多技巧，具体如下：</p><ul><li><p><strong>采用Leaky Relu激活函数</strong></p><script type="math/tex;mode=display">leaky(x) = \left\{ \begin{array}{l}  x,\;\;\;\;\;x > 0\\  0.1x,x \le 0  \end{array} \right.</script></li><li><p><strong>损失函数</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/3.png" alt="img"></p><p>损失函数整体分为定位误差和分类误差（图中的中心位置x,y部分有错误），其中</p><p>(1)第一部分表示当区域内存在目标，且也检测到了匹配目标的前提下，计算目标框中心的均方误差，定位权重为5；</p><p>(2)第二部分就是当区域内存在目标，且也检测到了匹配目标的前提下，计算的目标框宽高的均方误差，定位权重为5，实际上作者训练的时候输出的就是宽高开平方后的结果。这里注意用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大。举个例子，原来 w=10，h=20，预测出来 w=8，h=22，跟原来 w=3，h=5，预测出来其实前者的误差要比后者下，但是如果不加开根号，那么损失都是一样：4+4=8，但是加上根号后，变成 0.15和 0.7；</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4.png" alt="img"></p><p>(3)第三部分是分别计算当区域内真实目标和与之匹配的预测目标同时存在和不同时存在的情况下，边框置信度的均方误差，其中${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over C} _i}$表示的是真实目标框与预测目标框的IOU值。然而，大部分边界框都没有物体，积少成多，造成loss的不平衡，所以同时存在状态下的分类权重为1，不同时存在状态下的分类权重为0.5；</p><p>(4)第四部分计算的是当该区域存在目标时，计算目标类别均方误差。对于每个格子而言，作者设计只能包含同种物体。若格子中包含物体，我们希望希望预测正确的类别的概率越接近于1越好，而错误类别的概率越接近于0越好。</p><p><strong>其中对于预测目标和groundtruth的匹配，作者除了利用IOU进行匹配之外，对于无匹配对象的ground，则是取与其(x,y,w,h)均方误差最小的目标框作为匹配对象</strong></p></li><li><p><strong>非极大值抑制</strong></p><p>非极大值抑制(NMS)算法是目标检测领域中不可或缺的一个算法，其主要功能在于目标去重。而主要依据在于目标框之间的IOU以及每个目标框的置信度，即保证在IOU较大的目标群中选择置信度最高的目标框作为该目标群唯一的预测输出。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549648009461.png" alt="1549648009461"></p><p>YOLO所采用的NMS算法流程如下：</p><ul><li><p>Step1 在网络输出结果之后，会得到7x7x2=98个目标框，首先会根据阈值将prob不合格(大概率属于背景)的目标框置信度置为0；</p></li><li><p>Step2 对于每一个类别分开处理，先根据每个目标框在该类别下的prob置信度进行从大到小排序；</p></li><li><p>Step3 对于排序好的第一个置信度不为0的目标框，依次计算与其他置信度不为0的目标框的IOU，如果IOU大于阈值，则将该目标框置信度置为0，且其对应的所有类别prob都置为0；</p></li><li><p>Step4 转移至下一个置信度不为0的目标框，重复Step3，直到下一步没有置信度非0的目标框为止；</p></li><li><p>Step5 重复Step2</p></li><li><p>Step5 输出所有置信度非0的目标框，并根据阈值筛选有效目标。</p></li></ul></li></ul><p>部分C代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">nms_comparator</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *pa, <span class="keyword">const</span> <span class="keyword">void</span> *pb)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    detection a = *(detection *)pa;</span><br><span class="line">    detection b = *(detection *)pb;</span><br><span class="line">    <span class="keyword">float</span> diff = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(b.sort_class &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        diff = a.prob[b.sort_class] - b.prob[b.sort_class];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        diff = a.objectness - b.objectness;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(diff &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(diff &gt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_nms_sort</span><span class="params">(detection *dets, <span class="keyword">int</span> total, <span class="keyword">int</span> classes, <span class="keyword">float</span> thresh)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    k = total<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= k; ++i)&#123;</span><br><span class="line">        <span class="keyword">if</span>(dets[i].objectness == <span class="number">0</span>)&#123;</span><br><span class="line">            detection swap = dets[i];</span><br><span class="line">            dets[i] = dets[k];</span><br><span class="line">            dets[k] = swap;</span><br><span class="line">            --k;</span><br><span class="line">            --i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    total = k+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; classes; ++k)&#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; total; ++i)&#123;</span><br><span class="line">            dets[i].sort_class = k;</span><br><span class="line">        &#125;</span><br><span class="line">        qsort(dets, total, <span class="keyword">sizeof</span>(detection), nms_comparator);</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; total; ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(dets[i].prob[k] == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            box a = dets[i].bbox;</span><br><span class="line">            <span class="keyword">for</span>(j = i+<span class="number">1</span>; j &lt; total; ++j)&#123;</span><br><span class="line">                box b = dets[j].bbox;</span><br><span class="line">                <span class="keyword">if</span> (box_iou(a, b) &gt; thresh)&#123;</span><br><span class="line">                    dets[j].prob[k] = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549650071906.png" alt="1549650071906"></p><p>这里要注意的是YOLOv1中并没有用到softmax，所以可能同一个目标框的多个类别的置信度都很高。</p><ul><li><p><strong>dropout</strong></p><p>为了减少过拟合概率，YOLOv1中采用的是dropout方式，即在第一个全连接层/局部连接层后利用dropout随机将特征图中的一部分特征丢失。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549695294511.png" alt="1549695294511"></p><p>关于dropout的实现，其主要依据的是<code>drop_probability</code>，即丢失比例/概率，不同框架的实现方式不同，例如caffe：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span>* mask = rand_vec_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;phase_ == TRAIN) &#123;</span><br><span class="line">    <span class="comment">// Create random numbers</span></span><br><span class="line">    caffe_rng_bernoulli(count, <span class="number">1.</span> - threshold_, mask);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">      top_data[i] = bottom_data[i] * mask[i] * scale_;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    caffe_copy(bottom[<span class="number">0</span>]-&gt;count(), bottom_data, top_data);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其在测试环节直接将输入映射到输出，而在训练环节则是利用伯努利分布按照<code>1-drop_probability</code>的比例选取特征图中的特征，得到一个mask，然后乘以一定比例：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}mask \sim Bernoulli\left( {1 - drop\_probability,N} \right)\\{f_i} = {f_i} \times mas{k_i} \times \frac{1}{{1 - drop\_probability}}\end{array} \right.</script><p>而在Darknet中，其实现方式是：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_dropout_layer</span><span class="params">(dropout_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">if</span> (!net.train) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch * l.inputs; ++i)&#123;</span><br><span class="line">        <span class="keyword">float</span> r = rand_uniform(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        l.rand[i] = r;</span><br><span class="line">        <span class="keyword">if</span>(r &lt; l.probability) net.input[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> net.input[i] *= l.scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现原理是一样的，不过其实现方式更加清晰，缺点在于需要重复生成很多无效随机数，以及没有向量加速。</p></li><li><p><strong>学习率</strong></p><p>YOLOv1采用的是<code>multistep</code>变化方式，即在特定的迭代次数更新学习率，论文中的YOLOv1采用的是学习率逐渐衰减的方式，但有意思的是最新的YOLOv1中学习率变化过程不同于传统的逐渐衰减方式，而是类似于当前新兴的<code>warmup/warmrestart</code>变化方式。我们知道传统的训练模式下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/u=212751988,185815275&amp;fm=173&amp;s=0942F5122D5ED5CE18C595DA000050B3&amp;w=459&amp;h=414&amp;img.JPEG" alt="img"></p><p>不断衰减的学习率可以稳定收敛，但是现在发现在某些模型训练过程中收敛效果并不好，所以就有研究者提出了“热重启”策略，当然也有类似的“循环”策略：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697404615.png" alt="1549697404615"></p><p>那么在YOLOv1中的学习率设定是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=0.0005</span><br><span class="line">policy=steps</span><br><span class="line">steps=200,400,600,20000,30000</span><br><span class="line">scales=2.5,2,2,.1,.1</span><br></pre></td></tr></table></figure><p>可以看到，初始学习率只有0.0005，先慢慢增大，然后逐渐变小。</p></li></ul><h3 id="3-5-测试效果"><a href="#3-5-测试效果" class="headerlink" title="3.5 测试效果"></a>3.5 测试效果</h3><p>先放一张原论文中的实验效果图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697673711.png" alt="1549697673711"></p><p>我们可以看到YOLOv1这个典型的<code>one-stage</code>目标检测算法，在速度大幅领先的前提下，只损失了7%的精度，并且值得注意的是YOLO相对于Fast RCNN将目标误识别为背景的概率小很多，所以作者做了一个小尝试，即以YOLO本身预测为主，逐个对比Fast RCNN和YOLO预测的目标框，根据置信度进行选择，也就是下面<code>Fast R-CNN + YOLO</code>的embedding组合。那么对于VOC各个类别的定位精度：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697855804.png" alt="1549697855804"></p><p>其对于稀疏大目标的定位效果还是能接受的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698204272.png" alt="1549698204272"></p><h3 id="3-6-优缺点"><a href="#3-6-优缺点" class="headerlink" title="3.6 优缺点"></a>3.6 优缺点</h3><p>YOLO的优点不用多说，其作为当时最快的实时目标检测算法横空出世，SSD是在其后出来的，正式打开了<code>one-stage</code>目标检测算法的大门，虽然精度仍然不如Faster RCNN等，但是其速度很高，适合工程界研究改造。</p><p>当然其缺点也有很多：</p><ul><li><p>1.YOLOv1采用了7x7的网格划分模式，每个网格只能预测两个同类别的目标框，那么就无法预测密集场景下的目标位置，如：拥挤人群；</p></li><li><p>2.YOLOv1的网格划分方式会影响每个目标的边界定位准确度，因为目标一般是跨网格区域的，如果目标只有一小部分在某个网格，那么可能就会被忽略；</p></li><li><p>3.NMS本身漏洞，NMS会将相邻的目标框去重，那么就会出现下面的情况：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698728695.png" alt="1549698728695"></p><p>另外，由于置信度和IOU并不是强相关的，那么对于下面的情况，则不得不选择更差的目标框：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698809190.png" alt="1549698809190"></p></li><li><p>4.固定分辨率，由于YOLOv1中存在全连接层，所以输入的分辨率必须固定，那么对于YOLOv1所固定的448x448大小分辨率，很多大分辨率图像中的目标会变得很小，另外许多非正方形分辨率的图像目标会失真。</p></li></ul><h2 id="4-YOLOv2"><a href="#4-YOLOv2" class="headerlink" title="4.YOLOv2"></a>4.YOLOv2</h2><p>YOLOv2也被称作YOLO9000，其相对于YOLOv1提升了很多，正如作者所说：<code>Better、Faster、Stronger</code>。其行文很容易理解，我们直接通过创新点来了解其与YOLOv1的区别。</p><h3 id="4-1-网络结构改变"><a href="#4-1-网络结构改变" class="headerlink" title="4.1 网络结构改变"></a>4.1 网络结构改变</h3><p>YOLOv2的网络结构做了较大的改变，主要有:</p><ul><li><p><strong>Training for classfication—Darknet19</strong></p><p>YOLOv2在YOLOv1中的backbone网络基础上借鉴了VGG网络中的卷积方式，利用多个小卷积核替代大卷积核，并且利用1x1卷积核代替全连接层，这样做的好处的特征图每个位置共享参数，然后利用卷积核个数弥补参数组合多样性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   256 x 256 x   3   -&gt;   256 x 256 x  32  0.113 BFLOPs</span><br><span class="line">    1 max          2 x 2 / 2   256 x 256 x  32   -&gt;   128 x 128 x  32</span><br><span class="line">    2 conv     64  3 x 3 / 1   128 x 128 x  32   -&gt;   128 x 128 x  64  0.604 BFLOPs</span><br><span class="line">    3 max          2 x 2 / 2   128 x 128 x  64   -&gt;    64 x  64 x  64</span><br><span class="line">    4 conv    128  3 x 3 / 1    64 x  64 x  64   -&gt;    64 x  64 x 128  0.604 BFLOPs</span><br><span class="line">    5 conv     64  1 x 1 / 1    64 x  64 x 128   -&gt;    64 x  64 x  64  0.067 BFLOPs</span><br><span class="line">    6 conv    128  3 x 3 / 1    64 x  64 x  64   -&gt;    64 x  64 x 128  0.604 BFLOPs</span><br><span class="line">    7 max          2 x 2 / 2    64 x  64 x 128   -&gt;    32 x  32 x 128</span><br><span class="line">    8 conv    256  3 x 3 / 1    32 x  32 x 128   -&gt;    32 x  32 x 256  0.604 BFLOPs</span><br><span class="line">    9 conv    128  1 x 1 / 1    32 x  32 x 256   -&gt;    32 x  32 x 128  0.067 BFLOPs</span><br><span class="line">   10 conv    256  3 x 3 / 1    32 x  32 x 128   -&gt;    32 x  32 x 256  0.604 BFLOPs</span><br><span class="line">   11 max          2 x 2 / 2    32 x  32 x 256   -&gt;    16 x  16 x 256</span><br><span class="line">   12 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   13 conv    256  1 x 1 / 1    16 x  16 x 512   -&gt;    16 x  16 x 256  0.067 BFLOPs</span><br><span class="line">   14 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   15 conv    256  1 x 1 / 1    16 x  16 x 512   -&gt;    16 x  16 x 256  0.067 BFLOPs</span><br><span class="line">   16 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   17 max          2 x 2 / 2    16 x  16 x 512   -&gt;     8 x   8 x 512</span><br><span class="line">   18 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   19 conv    512  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x 512  0.067 BFLOPs</span><br><span class="line">   20 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   21 conv    512  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x 512  0.067 BFLOPs</span><br><span class="line">   22 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   23 conv   1000  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x1000  0.131 BFLOPs</span><br><span class="line">   24 avg                        8 x   8 x1000   -&gt;  1000</span><br><span class="line">   25 softmax                                        1000</span><br></pre></td></tr></table></figure><p>我们可以看到其中有19个卷积层和5个<code>max pooling</code>层，所以称其为<code>Darknet19</code>。作者利用该网络重新再Imagenet上训练了，相对于yolov1中的backbone网络，参数量更少，计算速度更快，效果更好。</p></li><li><p><strong>Training for Detection</strong></p><p>有了backbone骨干网络之后，作者剔除了<code>Darknet19</code>的最后一个卷积层，然后额外添加了几个卷积层：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers=-9</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=64</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[reorg]</span><br><span class="line">stride=2</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers=-1,-4</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=425</span><br><span class="line">activation=linear</span><br></pre></td></tr></table></figure><p>我们可以看到其中出现了两个新层<code>reorg</code>和<code>route</code>，这两个层的意义在于：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702344040.png" alt="1549702344040"></p><p>可以也就是说，<code>route</code>的层的作用就是将选定层按照通道拼接在一起，而<code>reorg</code>层的作用就是将特征图均匀划分为 4 份，从而使得两组特征图可以拼接。大致原理如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702478831.png" alt="1549702478831"></p><p>最终的网络结构为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   416 x 416 x   3   -&gt;   416 x 416 x  32</span><br><span class="line">    1 max          2 x 2 / 2   416 x 416 x  32   -&gt;   208 x 208 x  32</span><br><span class="line">    2 conv     64  3 x 3 / 1   208 x 208 x  32   -&gt;   208 x 208 x  64</span><br><span class="line">    3 max          2 x 2 / 2   208 x 208 x  64   -&gt;   104 x 104 x  64</span><br><span class="line">    4 conv    128  3 x 3 / 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    5 conv     64  1 x 1 / 1   104 x 104 x 128   -&gt;   104 x 104 x  64</span><br><span class="line">    6 conv    128  3 x 3 / 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    7 max          2 x 2 / 2   104 x 104 x 128   -&gt;    52 x  52 x 128</span><br><span class="line">    8 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">    9 conv    128  1 x 1 / 1    52 x  52 x 256   -&gt;    52 x  52 x 128</span><br><span class="line">   10 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">   11 max          2 x 2 / 2    52 x  52 x 256   -&gt;    26 x  26 x 256</span><br><span class="line">   12 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   13 conv    256  1 x 1 / 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   14 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   15 conv    256  1 x 1 / 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   16 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   17 max          2 x 2 / 2    26 x  26 x 512   -&gt;    13 x  13 x 512</span><br><span class="line">   18 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   19 conv    512  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   20 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   21 conv    512  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   22 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   23 conv   1024  3 x 3 / 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   24 conv   1024  3 x 3 / 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   25 route  16                                                             </span><br><span class="line">   26 reorg              / 2    26 x  26 x 512   -&gt;    13 x  13 x2048       </span><br><span class="line">   27 route  26 24                                                      </span><br><span class="line">   28 conv   1024  3 x 3 / 1    13 x  13 x3072   -&gt;    13 x  13 x1024</span><br><span class="line">   29 conv    425  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 425</span><br><span class="line">   30 detection</span><br></pre></td></tr></table></figure><p><strong>这里输出的425指的是：每个网格输出5个目标框，每个目标框包含COCO的80个类别概率，一个边框置信度，以及(tx,ty,tw,th)，即5x(80+1+4)。</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181226152903109.png" alt="img"></p></li><li><p><strong>全卷积网络</strong></p><p>综上可知，YOLOv2是一个全卷积网络，与YOLOv1相同，利用感受野的概念，我们可以认为是将原图划分为了多个网格区域，且区域半径为32，所以说输入大小必须是32的倍数。另外全卷积网络的好处在于可以有任意分辨率的输入，因为全连接层参数依赖前后两层的尺寸，而卷积层参数只有卷积核，与前后层尺寸无关，所以更为方便了。</p></li></ul><h3 id="4-2-batch-normalization"><a href="#4-2-batch-normalization" class="headerlink" title="4.2 batch normalization"></a>4.2 batch normalization</h3><p>相对于YOLOv1，YOLOv2将<code>dropout</code>替换成了效果更好的<code>batch normalization</code>，在每个卷积层计算之前利用<code>batch normalization</code>进行批归一化：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^k} = \frac{{{x^k} - E\left[ {{x^k}} \right]}}{{\sqrt {Var\left[ {{x^k}} \right]} }}\\{y^k} = {\gamma ^k}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^k} + {\beta ^k}\end{array} \right.</script><h3 id="4-3-Multi-Scale-Training"><a href="#4-3-Multi-Scale-Training" class="headerlink" title="4.3 Multi-Scale Training"></a>4.3 Multi-Scale Training</h3><p>为了让网络能适应不同分辨率的输入，在训练过程中，每个10个batches会随机选择一种分辨率输入，即利用图像插值对图像进行放缩，由于训练速度的要求以及分辨率必须是32倍数，所以训练过程中选择的分辨率分别为：320, 352, …, 608。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549703794254.png" alt="1549703794254"></p><h3 id="4-4-Anchor-boxes"><a href="#4-4-Anchor-boxes" class="headerlink" title="4.4 Anchor boxes"></a>4.4 Anchor boxes</h3><p>YOLOv2相对于YOLOv1的定位架构最大的改变在于剔除了<code>anchor boxes</code>概念，具体见第一章，从直接预测目标相对网格区域的偏移量到预测<code>anchor box</code>的修正量，有了先验长宽比的约束，可以减少很多不规则的目标定位。</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}x = {t_x} \times {w_a} + {x_a}\\y = {t_y} \times {h_a} + {y_a}\\w = {t_w} \times img\_width\\h = {t_h} \times img\_height\end{array} \right. \Rightarrow \left\{ \begin{array}{l}{b_x} = \sigma \left( {{t_x}} \right) + {c_x}\\{b_y} = \sigma \left( {{t_y}} \right) + {c_y}\\{b_w} = {p_w}{e^{{t_w}}}\\{b_h} = {p_h}{e^{{t_h}}}\\Pr\left( {object} \right) * IOU\left( {b,object} \right) = \sigma \left( {{t_o}} \right)\end{array} \right.</script><p>其中，<code>p</code>代表的是<code>anchor box</code>的先验值，<code>c</code>表示每个网格区域的左上角顶点，<code>t</code>表示网络输出的目标框的5个参数(tx,ty,tw,th,to),<code>b</code>表示真实预测定位信息。而最后一个关于先验概率的等价关系我们可以知道，与YOLOv1相同，这里目标预测的边框置信度包含了IOU先验值。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549705872475.png" alt="1549705872475"></p><p>其中对于部分输出进行了logistic转换：</p><script type="math/tex;mode=display">\sigma \left( x \right) = \frac{1}{{1 + x}}</script><p>对于先验<code>anchor boxes</code>的确定，作者通过 K-means 的方法对 VOC 和 COCO 数据集所有框的标签进行聚类，最后发现anchor box 在仅有 5 种 aspect ratio 的情况下就能达到足够的效果，当然，作者也试着将 K 提升到 9 个，发现效果更好。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549704037718.png" alt="1549704037718"></p><h3 id="4-5-其他训练技巧"><a href="#4-5-其他训练技巧" class="headerlink" title="4.5 其他训练技巧"></a>4.5 其他训练技巧</h3><ul><li><p><strong>softmax</strong></p><p>YOLOv2中对于每个类别的概率输出进行了softmax归一化。</p></li><li><p><strong>学习率</strong></p><p>YOLOv2的学习率变化方式与YOLOv1类似：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=0.001</span><br><span class="line">burn_in=1000</span><br><span class="line">max_batches = 500200</span><br><span class="line">policy=steps</span><br><span class="line">steps=400000,450000</span><br><span class="line">scales=.1,.1</span><br></pre></td></tr></table></figure><p>只不过多了一个<code>burn_in</code>参数，那么上面的参数设置对应的变化方式为：</p><script type="math/tex;mode=display">learning\_rat{e_{iter}} = \left\{ \begin{array}{l}base\_learningrate \times {\left( {iter/burn\_in} \right)^{power}},if\;iter < burn\_in\\learningrat{e_{iter - 1}} \times scal{e_j},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;elif\;iter = step{s_j}\\learningrat{e_{iter - 1}},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;otherwise\end{array} \right.</script></li><li><p><strong>损失函数</strong></p><p>YOLOv2的损失函数类似于YOLOv1，也有所不同，我阅读源码之后总结如下：</p><script type="math/tex;mode=display">\begin{array}{l}Loss=\\{\lambda _{prior}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{iter < 12800}\left[ {{{\left( {\sigma ({t_{{x_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{y_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{w_{ij}}}}) - {p_{{w_k}}}} \right)}^2} + {{\left( {\sigma ({t_{{h_{ij}}}}) - {p_{{h_k}}}} \right)}^2}} \right]} } } \\ + {\lambda _{coord}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{obj}{{\left( {{x_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_{ijk}}} \right)}^2} + {{\left( {{y_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }_{ijk}}} \right)}^2} + {{\left( {{w_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over w} }_{ijk}}} \right)}^2} + {{\left( {{h_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over h} }_{ijk}}} \right)}^2}} } } \\ + {\lambda _{obj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over C} }_{ij}}} \right)^2} + {\lambda _{noobj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{noobj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over C} }_{ij}}} \right)^2}\\ + {\lambda _{class}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}\sum\limits_{c \in classes} {{{\left( {{p_{ij}}\left( c \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over p} }_{ij}}\left( c \right)} \right)}^2}} } } \end{array}</script><p>可以发现只有定位误差部分的损失函数变化了，其中${\lambda <em>{prior}=0.01},{\lambda </em>{coord}=1},{\lambda <em>{class}=1},{\lambda </em>{obj}=5,{\lambda _{noobj}=1}}$，其中第一部分的意思是当训练的batch数不超过12800个时，尽量让预测目标框靠近每个网格中心，且尺寸与先验anchor box相同。</p></li></ul><h3 id="4-6-联合训练分类和检测"><a href="#4-6-联合训练分类和检测" class="headerlink" title="4.6 联合训练分类和检测"></a>4.6 联合训练分类和检测</h3><p>作者在论文最后提出，可以将分类和检测数据集放在一起训练网络，在遇到分类问题时，就只调用分类部分损失函数，否则调用检测分布损失函数。而对于两类数据集中存在的，类别相互包含的情况，作者则是剔除了<code>Word Tree</code>的概念：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549723273927.png" alt="1549723273927"></p><p>其原理实际上就是，预先构建好所有类别的关系树，然后利用联合概率分布和条件概率等，进行组合，相当于YOLO中对于分类概率和目标置信度的关系。</p><h3 id="4-7-测试效果"><a href="#4-7-测试效果" class="headerlink" title="4.7 测试效果"></a>4.7 测试效果</h3><p>YOLOv2在VOC和COCO上的测试结果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721649962.png" alt="1549721649962"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721663939.png" alt="1549721663939"></p><p>我们可以看到，此时的YOLOv2的效果已经与Faster RCNN以及新出现的<code>one-stage</code>算法<code>SSD</code>持平，不过YOLOv2依旧保持着遥遥领先的速度优势。</p><h3 id="4-8-优缺点"><a href="#4-8-优缺点" class="headerlink" title="4.8 优缺点"></a>4.8 优缺点</h3><p>YOLOv2相对来说在每个网格内预测了更多的目标框，并且每个目标框可以不用为同一类，而每个目标都有着属于自己的分类概率，这些使得预测结果更加丰富。另外，由于<code>anchor box</code>的加入，使得YOLOv2的定位精度更加准确。不过，其对于YOLOv1的许多问题依旧没有解决，当然那些也是很多目标检测算法的通病。那么随着<code>anchor box</code>的加入所带来的新问题是：</p><ul><li><p><code>anchor box</code>的个数以及参数都属于超参数，因此会影响训练结果；</p></li><li><p>由于<code>anchor box</code>在每个网格内都需要计算一次损失函数，然而每个正确预测的目标框才能匹配一个比较好的先验anchor，也就是说，对于YOLOv2中的5种<code>anchor box</code>，相当于强行引入了4倍多的负样本，在本来就样本不均衡的情况下，加重了不均衡程度，从而使得训练难度增大；</p></li><li><p>由于IOU和NMS的存在，会出现下面的情况：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549724233482.png" alt="1549724233482"></p><p>我们可以看到，当两个人很靠近或重叠时，检测框变成了中间的矩形框，其原因在于对于两个候选框（红，绿），其中红色框可能更加容易受到目标1的影响，而绿色框会同时收到目标1和目标2的影响，从而导致最终定位在中间。然后由于NMS存在，其他的相邻的框则会被剔除。要想避免这种情况，就应该在损失函数中加入相关的判定。</p></li></ul><h2 id="5-YOLOv3"><a href="#5-YOLOv3" class="headerlink" title="5.YOLOv3"></a>5.YOLOv3</h2><h3 id="5-1-Darknet-53"><a href="#5-1-Darknet-53" class="headerlink" title="5.1 Darknet-53"></a>5.1 Darknet-53</h3><p>YOLOv3中又提出了一种新的backbone网络——Darknet-53，其效果还是见第二章的表格图片。其架构如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725186270.png" alt="1549725186270"></p><p>可以看到，新增了<code>Residual</code>模块，不同于原本的Resnet中的残差模块：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/2018071722042637.png" alt="img"></p><p>我怎么感觉作者就是为了加深网络，所以才不得不引入残差模块的…可以看到明显的效果变化：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725539157.png" alt="1549725539157"></p><h3 id="5-2-网络多尺度输出"><a href="#5-2-网络多尺度输出" class="headerlink" title="5.2 网络多尺度输出"></a>5.2 网络多尺度输出</h3><p>YOLOv3增加了top down 的多级预测，解决了yolo颗粒度粗，对小目标无力的问题。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/606386-20180327004340505-1572852891.png" alt="img"></p><p>可以看到，不仅在不同的感受野范围输出了三种尺度的预测结果，每种预测结果中每个网格包含3个目标框，一共是9个目标框。而且，相邻尺度的网络还存在着级联：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181213163753559.png" alt="img"></p><p><strong>DBL</strong>: conv+BN+Leaky relu。</p><p><strong>resn</strong>：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。</p><p><strong>concat</strong>：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</p><p><strong>upsample</strong>：终于把原来的<code>reorg</code>改成了<code>upsample</code>，这里的upsample很暴力，很像克罗内克积，即：</p><script type="math/tex;mode=display">\left[ {\begin{array}{*{20}{c}}1&2\\3&4\end{array}} \right] \Rightarrow \left[ {\begin{array}{*{20}{c}}1&1&2&2\\1&1&2&2\\3&3&4&4\\3&3&4&4\end{array}} \right]</script><p>可以看到每个输出的深度都是255，即3x(80+5)。这种多尺度预测的方式应该是参考的FPN算法。</p><h3 id="5-3-Anchor-Boxes改进"><a href="#5-3-Anchor-Boxes改进" class="headerlink" title="5.3 Anchor Boxes改进"></a>5.3 Anchor Boxes改进</h3><p>YOLOv2中是直接预测了目标框相对网格点左上角的偏移，以及<code>anchor box</code>的修正量，而在YOLOv3中同样是利用K-means聚类得到了9组<code>anchor box</code>，只不过YOLOv2中用的是相对比例，而YOLOv3中用的是绝对大小。那么鉴于我们之前提到的<code>anchor box</code>带来的样本不平衡问题，以及绝对大小可能会出现超出图像边界的情况，作者加入了新的判断条件，即对于每个目标预测结果只选择与groundtruth的IOU最大/超过0.5的<code>anchor</code>，不考虑其他的<code>anchor</code>，从而大大减少了样本不均衡情况。</p><h3 id="5-4-分类函数"><a href="#5-4-分类函数" class="headerlink" title="5.4 分类函数"></a>5.4 分类函数</h3><p>YOLOv3中取消了对于分类概率的联合分布softmax，而是采用了logistic函数，因为有一些数据集的中的目标存在多标签，而softmax函数会让各个标签相互抑制。</p><h3 id="5-5-测试效果"><a href="#5-5-测试效果" class="headerlink" title="5.5 测试效果"></a>5.5 测试效果</h3><p>YOLOv3的泛化性能更好了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-fbf72eec750dfd743a15f511872a0974_hd.jpg" alt="img"></p><p>在加入了多尺度预测之后，小尺度目标检测效果更好：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-400c7fed8265bf7e2a5b42a866c6f2a1_hd.jpg" alt="img"></p><p>与其他算法的对比效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549729080541.png" alt="1549729080541"></p><h3 id="5-6-展望"><a href="#5-6-展望" class="headerlink" title="5.6 展望"></a>5.6 展望</h3><p>感觉YOLOv3的提升已经很大了，不过一些固有问题还是没有解决，有意思的是，在加入了多尺度预测后，拥挤场景下的目标检测效果更好了。不过基于<code>anchor box</code>的目标检测算法始终都有着瓶颈，寻求更好的出路才是最好的。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://lanbing510.info/2017/08/28/YOLO-SSD.html">http://lanbing510.info/2017/08/28/YOLO-SSD.html</a></li><li><a href="https://pjreddie.com/darknet/">https://pjreddie.com/darknet/</a></li><li><a href="https://blog.csdn.net/m0_37192554/article/details/81092514">https://blog.csdn.net/m0_37192554/article/details/81092514</a></li><li><a href="https://blog.csdn.net/leviopku/article/details/82660381">https://blog.csdn.net/leviopku/article/details/82660381</a></li><li><a href="https://github.com/pjreddie/darknet">https://github.com/pjreddie/darknet</a></li><li>Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</li><li>Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. 2017.</li><li>Redmon J, Farhadi A. YOLOv3: An Incremental Improvement[J]. 2018.</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:21 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="YOLO" scheme="https://huangpiao.tech/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>剖析KCF</title>
    <link href="https://huangpiao.tech/2019/02/04/%E5%89%96%E6%9E%90KCF/"/>
    <id>https://huangpiao.tech/2019/02/04/剖析KCF/</id>
    <published>2019-02-04T14:00:00.000Z</published>
    <updated>2019-02-04T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>核相关滤波算法是单目标跟踪领域一个举足轻重的算法，而kernelized correlation filters(KCF)是其原始形态，下面我以一个小白的角度慢慢揭开其神秘面纱。</p></blockquote><a id="more"></a><h2 id="1-岭回归理论推导"><a href="#1-岭回归理论推导" class="headerlink" title="1.岭回归理论推导"></a>1.岭回归理论推导</h2><p>岭回归的理论比较简单，类似于一个单层神经网络加上一个正则项，不同于支撑向量机中的结构风险最小化，岭回归更像是一个逻辑回归，是在保证误差风险最小的情况下尽量使得结构风险小。另外支撑向量机对于高维数据的训练比较快，因为它只取对分类有影响的 support 向量。不过在此处 KCF 的训练样本也不多，所以二者其实都可以，再加上 KCF 中岭回归还引用了对偶空间、傅里叶变换以及核函数，二者的差别就比较小了。<br>​ 岭回归的算法形式如下：</p><script type="math/tex;mode=display">\mathop {\min }\limits_w \sum\limits_i {{{\left( {f\left( {{x_i}} \right) - {y_i}} \right)}^2} + \lambda {{\left\| w \right\|}^2}}  \Leftrightarrow \mathop {\min }\limits_w \sum\limits_i {{{\left( {Xw - y} \right)}^2} + \lambda {{\left\| w \right\|}^2}}\tag{1-1}</script><p>其中<em>X</em> 为特征矩阵，<em>w</em> 为权值，<em>y</em> 为样本标签/响应，其中每一项都采用了<em>L</em>2 范数的平方,即矩阵内所有元素的平方和。因此，该优化的关键在于求最优的 <em>w</em>，求解方法则是使用了最直接的拉格朗日乘子法：</p><script type="math/tex;mode=display">\begin{array}{l}\because L = {\left( {Xw - y} \right)^T}\left( {Xw - y} \right) + \lambda {w^T}w\\\;\;\;\;\;\; = \left( {{w^T}{X^T} - {y^T}} \right)\left( {Xw - y} \right) + \lambda {w^T}w\\\;\;\;\;\;\; = {w^T}{X^T}Xw - {y^T}Xw - {w^T}{X^T}y + {y^T}y + \lambda {w^T}w\\\therefore \frac{{\partial L}}{{\partial w}} = 2{X^T}Xw - {X^T}y - {X^T}y + 2\lambda w\\\;\;\;\;\;\;\;\; = 2\left( {{X^T}X + \lambda I} \right)w - 2{X^T}y\\\;\;\;\;\;\;\;\; = 0\\\therefore w = {\left( {{X^T}X + \lambda I} \right)^{ - 1}}{X^T}y\end{array}</script><p>我们<strong>假设当前的权重W和输出y都是一维向量</strong>，则矩阵的求导公式满足:</p><script type="math/tex;mode=display">\begin{array}{l}\frac{{d\left( {XA} \right)}}{{dX}} = A,\frac{{d\left( {AX} \right)}}{{dX}} = {A^T},\frac{{d\left( {{X^T}A} \right)}}{{dX}} = A,\frac{{d\left( {{X^T}AX} \right)}}{{dX}} = \left( {A + {A^T}} \right)X,\\\frac{{d\left( {{A^T}XB} \right)}}{{dX}} = A{B^T},\frac{{d\left( {{A^T}{X^T}B} \right)}}{{dX}} = B{A^T},\frac{{d\left( {{A^T}{X^T}XA} \right)}}{{dX}} = 2XA{A^T}\end{array}</script><p>不过，由于后面要引入复频域空间，所以我们这里做一些微调:</p><script type="math/tex;mode=display">w = {\left( {{X^H}X + \lambda I} \right)^{ - 1}}{X^H}y\tag{1-2}</script><p>其中，H 代表共轭转置，即在转置的同时将矩阵内所有元素变为其共轭形式，原因很简单：</p><script type="math/tex;mode=display">\left( {a + bi} \right)\left( {a - bi} \right) = {a^2} + {b^2}\tag{1-3}</script><h2 id="2-循环矩阵"><a href="#2-循环矩阵" class="headerlink" title="2. 循环矩阵"></a>2. 循环矩阵</h2><h3 id="2-1-循环矩阵的引入"><a href="#2-1-循环矩阵的引入" class="headerlink" title="2.1 循环矩阵的引入"></a>2.1 循环矩阵的引入</h3><p>由于在目标跟踪中定位目标时如果采用循环移位的方式定位其中心，则需要采用循环的方式逐步判断，这样做太耗时，因此作者引入了循环矩阵。这样做的话，我们的待选目标框不用移动，直接将原图像矩阵循环移位。以一维矩阵为例：</p><script type="math/tex;mode=display">K = \left[ {\begin{array}{*{20}{c}}0&1&0& \ldots &0\\0&0&1& \ldots &0\\0&0& \ldots &1&0\\ \vdots & \vdots & \vdots & \ddots & \vdots \\1&0&0& \ldots &0\end{array}} \right]\tag{2-1}</script><p>矩阵的每一行相对上一行都向右移动了一位，这里举这个矩阵例子是有用意的，通过该矩阵的n阶形式，我们可以轻松的实现任意矩阵 <em>X</em> 的右移$XK^T$或者下移$KX$ ，如：</p><script type="math/tex;mode=display">X = C\left( x \right) = \left[ {\begin{array}{*{20}{c}}{{x_1}}&{{x_2}}& \ldots &{{x_n}}\\{{x_n}}&{{x_1}}& \ldots &{{x_{n - 1}}}\\ \vdots & \vdots & \ddots & \vdots \\{{x_2}}&{{x_3}}& \ldots &{{x_1}}\end{array}} \right]\tag{2-2}</script><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549029068783.png" alt="1549029068783"></p><h3 id="2-2-循环矩阵的转换"><a href="#2-2-循环矩阵的转换" class="headerlink" title="2.2 循环矩阵的转换"></a>2.2 循环矩阵的转换</h3><p>循环矩阵本身是将循环移位的结果整合到了一个矩阵中，虽然可以将循环计算过程优化为矩阵运算，但对于图像这类二维矩阵，则会生成一个很大的循环矩阵，从而耗费内存。这里作者巧妙地引入了离散傅里叶变换(DFT)，将循环矩阵X等价为：</p><script type="math/tex;mode=display">X = Fdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){F^H}\tag{2-3}</script><p>其中 <em>F</em> 与离散傅里叶变换中的矩阵有所差异，${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over x} }​$就是原矩阵的傅里叶变换，<em>diag</em>是将矩阵变为对角形式，后面会详细解释。</p><p>先以一维矩阵为例来证明：</p><ul><li><p><strong>Step1</strong> 定义循环矩阵 <em>X</em> 的多项式函数为：</p><script type="math/tex;mode=display">X = {f_X}\left( K \right) = {x_0}I + {x_1}K + {x_2}{K^2} + ... + {x_{n - 1}}{K^{n - 1}}\tag{2-4}</script><p>这里先说明一下，单位矩阵 <em>I</em> 其实也是一个循环矩阵，而 $P^n $其实就是将矩阵<em>I</em>所有元素右移 <em>n</em> 个单位。</p></li><li><p><strong>Step2</strong> 求矩阵<em>K</em>的特征值和特征向量：</p><script type="math/tex;mode=display">\begin{array}{l}\because \left| {\lambda I - K} \right| = \left| {\begin{array}{*{20}{c}}\lambda &{ - 1}& \ldots &0\\0&\lambda & \ldots &0\\ \vdots & \vdots & \ddots & \vdots \\{ - 1}&0& \cdots &\lambda \end{array}} \right| = {\lambda ^n} + {\left( { - 1} \right)^{n + 1}} \times {\left( { - 1} \right)^n} = {\lambda ^n} - 1\\\therefore {\lambda _k} = \cos \frac{{2\pi k}}{n} + i\sin \frac{{2\pi k}}{n} = {e^{\frac{{2\pi k}}{n}}},0 \le k \le n - 1\\\because \left[ {\begin{array}{*{20}{c}}{{\lambda _k}}&{ - 1}& \ldots &0\\0&{{\lambda _k}}& \ldots &0\\ \vdots & \vdots & \ddots & \vdots \\{ - 1}&0& \cdots &{{\lambda _k}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\\ \vdots \\{{x_n}}\end{array}} \right] = 0\\\therefore {\lambda _k} = \frac{{{x_1}}}{{{x_0}}} = \frac{{{x_2}}}{{{x_1}}} = ... = \frac{{{x_n}}}{{{x_{n - 1}}}}\\\therefore {x_n} = \lambda _k^{n - 1}{x_0}\\\therefore {D_k} = {\left[ {\begin{array}{*{20}{c}}{\lambda _k^0}&{\lambda _k^1}& \cdots &{\lambda _k^{n - 1}}\end{array}} \right]^T} = {\left[ {\begin{array}{*{20}{c}}{{e^0}}&{{e^{\frac{{2\pi k}}{n}}}}& \cdots &{{e^{\frac{{2\pi k\left( {n - 1} \right)}}{n}}}}\end{array}} \right]^T}\\\therefore 基础解系为： {D} = \left[ {\begin{array}{*{20}{c}}{{e^0}}&{{e^0}}& \cdots &{{e^0}}\\{{e^0}}&{{e^{\frac{{2\pi }}{n}}}}& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}\\ \vdots & \vdots & \ddots & \vdots \\{{e^0}}&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}}\end{array}} \right]\end{array}</script><p>可以发现矩阵 <em>K</em> 的特征矩阵与 <em>DFT</em> 的变换矩阵 <em>W</em> 一致，再利用多项式矩阵的性质可知,循环矩阵 <em>X</em> 的特征值为${f_X}\left( {diag\left( {{\lambda _k}} \right)} \right)​$, 利用矩阵与其特征值矩阵相似的特点, 可以很容易的证明该性质。</p></li><li><p><strong>Step3</strong> 求循环矩阵 <em>X</em> 的特征值和特征向量：</p><script type="math/tex;mode=display">\begin{array}{l}\because diag\left( {{\lambda _x}} \right)\\ = {f_X}\left( {diag\left( {{\lambda _k}} \right)} \right)\\ = {x_0} + {x_1}\left[ {\begin{array}{*{20}{c}}{{e^0}}&0& \cdots &0\\0&{{e^{\frac{{2\pi }}{n}}}}& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\0&0& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}\end{array}} \right] + ... + {x_{n - 1}}\left[ {\begin{array}{*{20}{c}}{{e^0}}&0& \cdots &0\\0&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\0&0& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}}\end{array}} \right]\\ = \left[ {\begin{array}{*{20}{c}}{\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi 0}}{n}i}}{x_i}} }&0& \cdots &0\\0&{\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi 1}}{n}i}}{x_i}} }& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\0&0& \cdots &{\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}i}}{x_i}} }\end{array}} \right]\\\therefore {\lambda _x} = \left[ {\begin{array}{*{20}{c}}{{e^0}}&{{e^0}}& \cdots &{{e^0}}\\{{e^0}}&{{e^{\frac{{2\pi }}{n}}}}& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}\\ \vdots & \vdots & \ddots & \vdots \\{{e^0}}&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{x_0}}\\{{x_1}}\\ \vdots \\{{x_{n - 1}}}\end{array}} \right] = Dx\end{array}</script><p>可以发现循环矩阵 <em>X</em> 的特征值就是其原矩阵 <em>x</em> 的离散傅里叶变换，对于循环矩阵的特征向量，推导过程如下:</p><script type="math/tex;mode=display">\begin{array}{l}\because {K^{n - 1}}{D_k} = {\lambda _k}{K^{n - 2}}{D_k} = ... = {\lambda _k}^{n - 1}{D_k}\\\therefore {K^{n - 1}}的特征值是{\lambda _k}^{n - 1}，而特征向量不变\\\therefore X的特征向量同样是{D_k}\end{array}</script><p>在这里我们将 $D_k $替换为 <em>DFT</em> 变换矩阵<em>W</em> ，利用矩阵对角化可知：</p><script type="math/tex;mode=display">X = Wdiag\left( {{\lambda _x}} \right){W^{ - 1}} = Wdiag\left( {Wx} \right){W^{ - 1}} = Wdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){W^{ - 1}}\tag{2-5}</script></li><li><p><strong>Step4</strong> 利用 <em>DFT</em> 变换矩阵 <em>W</em> 的性质修正 <em>X</em> :</p><p>通过观察可知 <em>W</em> 为对称矩阵，另外也可以轻松证明${W^H}W = W{W^H} = nI​$，在这里呢，我们可以对 <em>W</em> 进行适当地变换：</p><script type="math/tex;mode=display">F = \frac{1}{{\sqrt n }}W\tag{2-6}</script><p>因此${F^H}F = F{F^H} = I​$，则 <em>F</em> 为酉矩阵，同时它也满足 ${F^H} = {F^{-1}}​$，如果我们将之前的 <em>W</em> 替换为 <em>F</em> ，那么：</p><script type="math/tex;mode=display">X = \frac{1}{{\sqrt n }}Wdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right)\sqrt n {W^{ - 1}} = X\tag{2-7}</script><p>所以整体来看， <em>X</em> 保持不变，综上可得公式(2-3)。</p></li></ul><h3 id="2-3-二维循环矩阵"><a href="#2-3-二维循环矩阵" class="headerlink" title="2.3 二维循环矩阵"></a>2.3 二维循环矩阵</h3><p>上述推导都是基于一维矩阵进行的，那么对于二维矩阵的循环矩阵则是要进行两次一维的循环矩阵变换，下面简要介绍一下方法，完整的我不会推导~</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549042686678.png" alt="1549042686678"></p><p>对于 <em>m×n</em> 的矩阵 <em>x</em>，其循环矩阵是将其当作块矩阵，矩阵每一行的元素都是前一个元素向下平移得来的，每一列都是向右平移得来的。因此其循环矩阵大小为 <em>mn×mn</em>，这主要是为了将循环矩阵 <em>X</em> 变为方阵，这样才能求其特征值和特征向量。当然，如果原矩阵 <em>x</em> 本身就是方阵，那么则不必这样做，可以将行向量改为右移，列向量改为下移，这样就符合观察习惯，这也是论文代码所采用的方式。</p><p>先将 <em>n×n</em> 大小的矩阵 <em>x</em> 每一行向量单独看作一个 <em>1×n</em> 的块矩阵，按照一维循环矩阵$Kx​$的逻辑去做，不断下移，可得一个 $n^2×n​$的块矩阵。然后再将每一列向量单独看作一个$ n^2×1​$ 的块矩阵,按照 $xK^T​$的方式，不断右移，最后可得一个 ${n^2} × {n^2}​$的块矩阵。其中二维的 DFT 变换方式为 $WXW^H​$ 。</p><h2 id="3-循环矩阵与岭回归算法的结合"><a href="#3-循环矩阵与岭回归算法的结合" class="headerlink" title="3. 循环矩阵与岭回归算法的结合"></a>3. 循环矩阵与岭回归算法的结合</h2><p>建立了循环矩阵 X 之后，如果判定其第(i, j)处的块矩阵处响应最大，即目标框相对前一个目标框向下偏移 i-1 个单位, 向右偏移 j-1 个单位。那么标签 y的大小也就是 n×n。将其与岭回归算法结合之后可以得到:</p><script type="math/tex;mode=display">\begin{array}{l}w = {\left( {{X^H}X + \lambda I} \right)^{ - 1}}{X^H}y\\\;\;\; = {\left( {Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H} \cdot Fdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){F^H} + \lambda Fdiag\left( \delta  \right){F^H}} \right)^{ - 1}}{X^H}y\\\;\;\; = Fdiag\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  + \lambda \delta }}} \right){F^H}Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H}y\\\;\;\; = Fdiag\left( {\frac{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  + \lambda \delta }}} \right){F^H}y\end{array}</script><p>其中，${{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}$表示 <em>x</em> 经 DFT 变换之后的共轭形式，$\delta $表示全 1 向量，其等同于单位矩阵的特征向量，$\odot $表示矩阵元素点乘。</p><p>根据DFT时域卷积的性质：</p><script type="math/tex;mode=display">x\left( n \right) \otimes y\left( n \right) \leftrightarrow F\left( x \right)F\left( y \right)\tag{3-1}</script><p>而时域卷积常用的是循环卷积，即将原序列看作一个周期，通过验证可以得到：</p><script type="math/tex;mode=display">x\left( n \right) \otimes y\left( n \right) = C{\left( x \right)^T}y\tag{3-2}</script><p>可以发现${X^H} = C{\left( x \right)^H} = Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H}​$,因此可得：</p><script type="math/tex;mode=display">F\left( {C{{\left( x \right)}^T}y} \right) = \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} \tag{3-3}</script><p>利用上面的结论可以继续转换w为：</p><script type="math/tex;mode=display">Fw = \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over w}  = F{F^H}{\left( {\frac{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  + \lambda \delta }}} \right)^*} \odot Fy = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}  + \lambda \delta }}\tag{3-4}</script><p>至此，权重矩阵 w 的求解在傅式空间变成了简单的点乘运算，运算复杂度大幅降低。</p><h2 id="4-对偶空间的引入"><a href="#4-对偶空间的引入" class="headerlink" title="4. 对偶空间的引入"></a>4. 对偶空间的引入</h2><p>对偶空间的具体意义我只有一个模糊的概念，之前在运筹学中学习的时候就感觉对偶空间像是从另外一个角度分析优化问题，比方说岭回归中的权值矩阵w，它的目的是完成 X 到 y 的映射，如果将循环矩阵拉伸为多个行向量，即$n^2$个 n×n 的样本，则更直接一点就是完成从$ n^2$维空间到 1 维空间的维度转换。那么对偶空间呢？对偶空间所要考虑的就是那 $n^2$ 个样本对于问题的影响，而这个影响因子，在优化问题中常常作为约束惩罚项的系数，然后分别权衡约束对于各个样本的影响。</p><h3 id="4-1-优化角度分析"><a href="#4-1-优化角度分析" class="headerlink" title="4.1 优化角度分析"></a>4.1 优化角度分析</h3><p>原优化问题为：</p><script type="math/tex;mode=display">\mathop {\min }\limits_w {\sum\limits_{i = 1}^n {\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}} } \right)} ^2} + \lambda \sum\limits_{j = 1}^p {w_j^2} \tag{4-1}</script><p>其等价为：</p><script type="math/tex;mode=display">\begin{array}{l}\mathop {\min }\limits_w \sum\limits_{i = 1}^n {{\xi _i}^2}  + \lambda \sum\limits_{j = 1}^p {w_j^2} \\s.t.\;\;{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}}  = {\xi _i}\end{array}\tag{4-2}</script><p>采用惩罚函数的方式是：</p><script type="math/tex;mode=display">\mathop {\min }\limits_w \sum\limits_{i = 1}^n {{\xi _i}^2}  + \lambda \sum\limits_{j = 1}^p {w_j^2}  + \sum\limits_{i = 1}^n {{\alpha _i}\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}}  - {\xi _i}} \right)} \tag{4-3}</script><p>可以发现，如果将${\xi _i}$ 看作${w_j}$经过一定线性变换之后在对偶空间的表现形式，则这里将其看作一个单独的变量，利用拉格朗日乘子法可得：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}\frac{{\partial L}}{{\partial {w_j}}} = 2\lambda {w_j} - \sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}}  = 0 \Rightarrow {w_j} = \frac{1}{{2\lambda }}\sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}} \\\frac{{\partial L}}{{\partial {\xi _i}}} = 2{\xi _i} - {\alpha _i} = 0 \Rightarrow {\xi _i} = \frac{1}{2}{\alpha _i}\end{array} \right.\tag{4-4}</script><p>将其带入原目标函数可得：</p><script type="math/tex;mode=display">\begin{array}{l}\mathop {\min }\limits_\alpha  \frac{1}{4}\sum\limits_{i = 1}^n {{\alpha _i}^2}  + \frac{1}{{4\lambda }}\sum\limits_{j = 1}^p {\left( {\sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}} } \right)\left( {\sum\limits_{k = 1}^n {{\alpha _k}{x_{kj}}} } \right)}  + \sum\limits_{i = 1}^n {{\alpha _i}\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}\frac{1}{{2\lambda }}\sum\limits_{k = 1}^n {{\alpha _k}{x_{kj}}}  - \frac{{{\alpha _i}}}{2}} } \right)} \\ \Leftrightarrow \mathop {\min }\limits_\alpha  \sum\limits_{i = 1}^n {{\alpha _i}{y_i}}  - \frac{1}{4}\sum\limits_{i = 1}^n {{\alpha _i}^2}  - \frac{1}{{4\lambda }}\sum\limits_{i = 1}^n {\sum\limits_{k = 1}^n {{\alpha _i}{\alpha _k}} } \sum\limits_{j = 1}^p {{x_{ij}}{x_{kj}}} \end{array}</script><p>由此可得原优化问题的对偶问题，将其转换成矩阵形式为：</p><script type="math/tex;mode=display">\mathop {\min }\limits_\alpha  {\alpha ^T}y - \frac{1}{4}{\alpha ^T}\alpha  - \frac{1}{{4\lambda }}{\alpha ^T}G\alpha \tag{4-5}</script><p>其中 G 表示$\left\langle {{x_i},{x_j}} \right\rangle = {x_{i \cdot }}^T{x_{k \cdot }}​$，利用拉格朗日乘子法可得：</p><script type="math/tex;mode=display">\frac{{\partial Q}}{{\partial \alpha }} = y - \frac{1}{2}\alpha  - \frac{1}{{2\lambda }}G\alpha  \Rightarrow \alpha  = 2\lambda {\left( {G + \lambda I} \right)^{ - 1}}y\tag{4-6}</script><p>再将此对偶空间的最优解带入公式（4-4）可得：</p><script type="math/tex;mode=display">w = \frac{1}{{2\lambda }}{X^T}\alpha  = {X^T}{\left( {G + \lambda I} \right)^{ - 1}}y\tag{4-7}</script><h3 id="4-2-矩阵变换角度"><a href="#4-2-矩阵变换角度" class="headerlink" title="4.2 矩阵变换角度"></a>4.2 矩阵变换角度</h3><p>上面的优化方法更侧重于从理论源头出发，而如果真的要用的话，可以直接用上面的理论，因此呢我们可以直接对矩阵进行变换：</p><script type="math/tex;mode=display">\begin{array}{l}\because\left( {{X^T}X + \lambda I} \right)w = {X^T}y\\\therefore w = {\lambda ^{ - 1}}\left( {{X^T}y - {X^T}Xw} \right) = {\lambda ^{ - 1}}{X^T}\left( {y - Xw} \right) - {X^T}\alpha \\\therefore \alpha  = {\lambda ^{ - 1}}\left( {y - Xw} \right)\\\therefore \lambda \alpha  = y - X{X^T}\alpha \\\therefore \alpha  = {\left( {{X^T}X + \lambda I} \right)^{ - 1}}y = {\left( {G + \lambda I} \right)^{ - 1}}y\end{array}</script><h3 id="4-3-新样本测试"><a href="#4-3-新样本测试" class="headerlink" title="4.3 新样本测试"></a>4.3 新样本测试</h3><p>训练好参数之后，当引入新样本时，可以直接利用 <em>wx</em> 的方式求出响应 y,根据公式（4-7）可得：</p><script type="math/tex;mode=display">f\left( z \right) = Z{X^T}{\left( {G + \lambda I} \right)^{ - 1}}y = {y^T}{\left( {G + \lambda I} \right)^{ - 1}}X{Z^T} = {y^T}{\left( {{G_{xx}} + \lambda I} \right)^{ - 1}}{G_{xz}}\tag{4-8}</script><h2 id="5-核函数的高维映射"><a href="#5-核函数的高维映射" class="headerlink" title="5.核函数的高维映射"></a>5.核函数的高维映射</h2><h3 id="5-1核函数的引入"><a href="#5-1核函数的引入" class="headerlink" title="5.1核函数的引入"></a>5.1核函数的引入</h3><p>核函数的引入主要是为了减少线性不可分问题的影响，因为岭回归跟神经网络或者深度学习不同，它是单层结构，需要从空间映射来着手。<br>常见的核函数有线性函数、多项式核函数和高斯核函数（RBF），而论文中则是采用了 RBF 核函数，理论上来讲是映射到了无穷维数的空间，可以通过展开其级数得知。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549204757681.png" alt="1549204757681"></p><p>如图所示，黑色和蓝色区域明显是一个线性不可分的，而利用一个二次函数却能完美分割，这就是核函数的意义。本文所使用的核函数是高斯核函数：</p><script type="math/tex;mode=display">K = {e^{ - \frac{{{{\left( {x - \mu } \right)}^2}}}{{2{\sigma ^2}}}}}\tag{5-1}</script><p>论文中将核函数引入样本的点积，即：</p><script type="math/tex;mode=display">f\left( z \right) = {y^T}{\left( {{K^{xx}} + \lambda I} \right)^{ - 1}}{K^{xz}} = {\alpha ^T}{K^{xz}}\tag{5-2}</script><h3 id="5-2-核函数与循环矩阵的结合"><a href="#5-2-核函数与循环矩阵的结合" class="headerlink" title="5.2 核函数与循环矩阵的结合"></a>5.2 核函数与循环矩阵的结合</h3><p>对于任何的循环矩阵 X，还是以一维的原矩阵 x 为例，可知：</p><script type="math/tex;mode=display">K_{ij}^{xx} = K\left( {{P^i}x,{P^j}x} \right) = K\left( {{P^{ - i}}{P^i}x,{P^{ - i}}{P^j}x} \right) = K\left( {x,{P^{j - i}}x} \right) = K\left( {x,{P^{\left( {j - i} \right)\;\bmod \;n}}x} \right)\tag{5-3}</script><p>其中由于 P 是循环矩阵的基础变换矩阵，也就是前面所提到的 K 矩阵，这里是为了与核函数区分开来，在矩阵点积中，两个矩阵的元素对应相乘，因此两个矩阵同时移位$ P^i​$，并不会影响结果。<br>由公式（5-3）可知，只要行号和列号的差值相同，其对应元素的值就相同，所以 $K^{xx}​$是循环矩阵。再利用循环矩阵的特性（2-3）可得：</p><script type="math/tex;mode=display">\begin{array}{l}\because \alpha  = {\left( {{K^{xx}} + \lambda I} \right)^{ - 1}}y\\\;\;\;\;\;\; = {\left( {Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}}} \right){F^H} + \lambda I} \right)^{ - 1}}y\\\;\;\;\;\;\; = Fdiag\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }}} \right){F^H}y\\\therefore F\alpha  = F{F^H}{\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }}} \right)^*} \odot Fy\\\therefore \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha }  = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{x{x^*}}} + \lambda }}\\\because k_i^{xx} = K\left( {{x_0},{x_i}} \right),k_{n - i}^{xx} = K\left( {{x_0},{x_{n - i}}} \right) = K\left( {{x_0},{x_i}} \right)\\\therefore {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{x{x^*}}} = {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}}\\\therefore \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha }  = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }}\end{array}</script><p>同理可得：</p><script type="math/tex;mode=display">\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over f} \left( z \right) = {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} ^{xz}} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } \tag{5-4}</script><blockquote><p>虽然原论文关于w的推导错误了，但是代码是根据$\alpha $来实现的，所以正确。</p></blockquote><h3 id="5-3-不同核函数的计算"><a href="#5-3-不同核函数的计算" class="headerlink" title="5.3 不同核函数的计算"></a>5.3 不同核函数的计算</h3><p>从最基础的内积出发，其核函数形式就是：</p><script type="math/tex;mode=display">{K^{xz}} = {X^T}z = C\left( x \right){z^T} = {F^{ - 1}}{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)^T}\tag{5-5}</script><p>对于多项式核，则有：</p><script type="math/tex;mode=display">{K^{xz}} = {\left( {{X^T}z + a} \right)^b} = \left[ {{F^{ - 1}}{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)}^T} + a} \right]\tag{5-6}</script><p>对于 RBF 径向基核，也就是常说的高斯核函数，有：</p><script type="math/tex;mode=display">\begin{array}{l}\because k_i^{xz} = h\left( {{{\left\| {{x_i} - z} \right\|}^2}} \right) = h\left( {{{\left\| {{x_i}} \right\|}^2} + {{\left\| z \right\|}^2} - 2x_i^Tz} \right)\\\therefore {k^{xz}} = h\left( {{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{X^T}z} \right)\\\;\;\;\;\;\;\;\; = h\left( {{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{F^{ - 1}}\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)} \right)\\\;\;\;\;\;\;\;\; = {e^{ - \frac{{{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{F^{ - 1}}\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)}}{{{\sigma ^2}}}}}\end{array}</script><h2 id="6-模板图像的获取"><a href="#6-模板图像的获取" class="headerlink" title="6. 模板图像的获取"></a>6. 模板图像的获取</h2><p>模板图像是基于第一帧图像目标框所得到的，其具体获取过程如下：</p><ul><li><p>Step1 保持初始目标框中心不变，将目标框的宽和高同时扩大相同倍数（论文中取 2.5 倍）；</p></li><li><p>Step2 设定模板图像尺寸为 96，计算扩展框与模板图像尺寸的比例：</p><script type="math/tex;mode=display">scal{e_z} = \frac{{max\left( {w,h} \right)}}{{template}}\tag{6-1}</script></li><li><p>Step3 然后将 scale 同时应用于宽和高，获取图像提取区域：</p><script type="math/tex;mode=display">ro{i_{w,h}} = \left( {\frac{w}{{scal{e_z}}},\frac{h}{{scal{e_z}}}} \right)\tag{6-2}</script></li><li><p>Step4 由于后面提取 hog 特征时会以 cell 单元的形式提取，另外由于需要将频域直流分量移动到图像中心，因此需保证图像大小为 cell大小的偶数倍，另外，在 hog 特征的降维的过程中是忽略边界 cell 的，所以还要再加上两倍的 cell 大小：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549265609508.png" alt="1549265609508"></p><script type="math/tex;mode=display">ro{i_{w,h}} = {\left[ {\frac{{ro{i_{w,h}}}}{{2cell\_size}}} \right]_{floor}} \cdot 2cell\_size + 2cell\_size\tag{6-3}</script></li><li><p>Step5 由于 roi 区域可能会超出原图像边界，因此超出边界的部分填充为原图像边界的像素；</p></li><li><p>Step6 最后利用线性插值的方式将 roi 区域采样为 template 大小。</p></li></ul><h2 id="7-特征提取"><a href="#7-特征提取" class="headerlink" title="7.特征提取"></a>7.特征提取</h2><h3 id="7-1-f-hog特征"><a href="#7-1-f-hog特征" class="headerlink" title="7.1 f-hog特征"></a>7.1 f-hog特征</h3><p>hog 特征又叫做方向梯度直方图，顾名思义，它所描述的是图像各像素点的方向梯度它对图像几何的和光学的形变都能保持很好的不变性。而论文中所用的hog 特征与常规的不同，具体的不同和实施细节我会在下文详细介绍：</p><ul><li><p><strong>Step1 梯度幅值计算。</strong></p><p>计算模板图像 RGB 三通道每个像素点的水平梯度 <em>dx</em> 和垂直梯度 <em>dy</em>，并计算各点的梯度幅值，以最大的梯度幅值所在通道为准：</p><script type="math/tex;mode=display">a = \mathop {max}\limits_{i \in \left( {r,g,b} \right)} \left( {\sqrt {{{\left( {d{x_i}} \right)}^2} + {{\left( {d{y_i}} \right)}^2}} } \right)\tag{7-1}</script></li><li><p><strong>Step2 梯度方向判定。</strong></p><p>如果像素点最大幅值所在通道为 m，则利用该通道的水平梯度和垂直梯度计算该点的梯度方向，论文中将[0,180)分为了 9 个方向，同时还将[0,360)分为了18 个方向，具体方向归属则是利用该像素点梯度在模板方向上的投影值确定：</p><script type="math/tex;mode=display">\theta  = \mathop {max}\limits_\theta  \left( {dx\cos \theta  + dy\sin \theta } \right)\tag{7-2}</script><p>分别通过以[0,180 ) 和[0,360 )为周期将个像素点的方向投影至这两个区间，从而每个像素点都有两种方向；</p></li><li><p><strong>Step3 cell的分割。</strong></p><p>确定 cell 单元尺寸（论文中设为 4），因此，水平方向有 sizeX=24,竖直方向也有 sizeY =24 个 cell，由于在计算像素点梯度时，边界像素点的梯度都是通过镜像幅值边界像素的方式计算的，因此论文中并未考虑边界点的梯度。每个 cell中的方向梯度直方图应该有 9+18 = 27 个方向。</p></li><li><p><strong>Step4 cell 内像素点梯度幅值加权方式。</strong></p><p>论文代码中关于 cell 的方向梯度直方图的求解很特别。对于每个 cell，根据其尺寸，设计了两个离散序列，因为比较有规律，所以当做两个函数 x 和 y：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549270523031.png" alt="1549270523031"></p><p>通过图像可以看到，随着 cell 内像素点横纵坐标的偏移，对应的点的 x,y 值一直在变化，且两个函数关于 0.5 对称，另外 x+y=1，利用这一特性，可以利用x 和 y 进行组合分解：</p><script type="math/tex;mode=display">1 = {\left( {x + y} \right)^2} = {x^2} + xy + yx + {y^2}\tag{7-3}</script><p>利用公式（7-3），论文代码中将每个 cell 等分为 4 部分（左上、右上、左下和右下），每一部分都是由包含该 cell 在内的相邻 4 个 cell 的同一部分加权平均得来的，其权重即为公式（7-3）所示的四个部分。具体组合方式如下（以 cell左上角部分为例）：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549270624282.png" alt="1549270624282"></p><p>可以发现，cell 左上部分的加权方式是以自身为左上角，然后取相邻的其他三个 cell 的左上部分组成一个新的虚 拟 cell，再利用上图 所示的权重分布将四个cell 的左上部分进行加权平均，权重正是 $x^2​$,xy,yx,$y^2​$。同理，cell 的其他三个部分也是一样的原理，比如 cell 的右下部分，则是将该 cell 的右下部分作为虚拟 cell的右下角，然后分别取该 cell 左上，正上，正左三个方向相邻的 cell 的右下部分组成新的虚拟 cell，再进行加权平均。<br>当然，对于边界 cell，则只选取不超过边界的部分 cell 进行不完整加权。</p><ul><li><p><strong>Step5 方向梯度直方图计算。</strong></p><p>对于 cell 内每个像素点，将其梯度幅值分别以[0,180 )和[0,360 ) 两种投影区间累加至对应梯度方向直方图中，在按照上一步中提到的加权方式计算完 cell特征之后，每个 cell 保留了 9+18 个方向的梯度。</p></li><li><p><strong>Step6 相对领域归一化及截断。</strong></p><p>对于每个 cell，分别取包含其在内的相邻四个 cell，如下图所示（好丑-_-||，为了区分四个 cell 的不同，尽力了。。。）</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549271000831.png" alt="1549271000831"></p><p>因此有四个组合方式，每种组合方式都取该组合方式内四个 cell 的方向梯度直方图的前 9 个方向梯度的 L2 范数 val，然后用该 cell 内 27 个方向的梯度直方图除以 val，即可得到规范化之后的 hog 特征。四个组合可以得到四组 hog 特征，即 9+9+9+9+18+18+18+18=108 个方向。</p><p>可以发现边界 cell 无法得到这么多方向，因此去掉边界 cell。所以sizeX=24-2=22，sizeY=22。</p></li><li><p><strong>Step7 PCA降维。</strong></p><p>作者从大量各种分辨率的图片中收集了很多36维特征（按照之前的定义），并且在这些特征上进行了PCA分析，发现了了一个现象：<strong>由前11个主特征向量定义的线性子空间基本包含了hog特征的所有信息。</strong>并且用降维之后的特征在他们的任务（目标检测）中取得了和用36维特征一样的结果。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/5252065-a0c76ca30c0acf6f.webp" alt="img"></p><p>如果用 $C_{ij}​$表示第 <em>i</em> 组 <em>hog</em> 特征的第 <em>j</em> 个方向，则原作者代码中的降维方式分别如下：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}{f_1}\left( j \right) = \frac{{\sum\limits_{i = 1}^4 {{C_{ij}}} }}{{\sqrt 4 }}\\{f_2}\left( i \right) = \frac{{\sum\limits_{j = 1}^{18} {{C_{ij}}} }}{{\sqrt {9 \times 2} }}\end{array} \right.\tag{7-4}</script><p>然后将两种降维方式得到的特征进行组合，得到 27+4=31 组特征。</p><p>原论文流程示意图如下：</p></li></ul></li></ul><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/20180820160426679" alt="img"></p><h3 id="7-2-CN-CN2特征"><a href="#7-2-CN-CN2特征" class="headerlink" title="7.2 CN/CN2特征"></a>7.2 CN/CN2特征</h3><p>该颜色特征将颜色空间划分为了黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄共11种，然后将其投影至10维子空间的标准正交基上，这里作者给出了32768种颜色向量组合。再利用PCA技术，采用奇异值分解方式提取其中主要的两个颜色作为最终的特征。由于CN2特征是单目标跟踪领域中很有效的一种人工特征，这里对其原理做出详细描述：</p><ul><li><p><strong>Step1</strong> 根据如下计算公式，作者给出了模板矩阵w2c（32768×10）：</p><script type="math/tex;mode=display">Index\_img = 1 + \left[ {\frac{R}{8}} \right] + 32 \times \left[ {\frac{G}{8}} \right] + {32^2} \times \left[ {\frac{B}{8}} \right]\tag{7-5}</script><p>将原图的RGB三通道数据带入其中，图中每个像素位置都能得到一个索引号，范围是1~32768，将此索引号带入w2c中，便可得到一个宽高与原图一致，通道数为10的CN矩阵<em>x_pca</em>，与此同时，将其形状重塑为（W×H）×10的二维矩阵；</p></li><li><p><strong>Step2</strong> 逐帧更新外观矩阵：</p><script type="math/tex;mode=display">z\_pca = \left( {1 - learning\_rate} \right) \times z\_pca + learning\_rate \times x\_pca\tag{7-6}</script></li><li><p><strong>Step3</strong> 开始PCA，先按列对矩阵去中心化，并计算协方差矩阵cov（10×10）：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}dat{a_{ij}} = z\_pc{a_{ij}} - \mathop {mean}\limits_{i \in \left[ {1,WH} \right]} (z\_pc{a_{ij}})\\cov = \frac{1}{{HWC - 1}} \times dat{a^T} \times data\end{array} \right.\tag{7-7}</script></li><li><p><strong>Step4</strong> 进行奇异值分解，由于对于任何矩阵$A \in {R^{m \times n}}$都可以利用奇异值分解为$A = UD{V^H}$, 其中$U \in {R^{m \times m}}$,$D \in {R^{m \times n}}$,$V \in {R^{n \times n}}​$,而对于矩阵<em>cov</em>，<em>U=V</em>,那么<em>AU=UD</em>,所以U的列向量是协方差矩阵的特征向量，D是协方差矩阵的特征值：</p><script type="math/tex;mode=display">[U,D,U] = svd\left( {cov} \right)\tag{7-8}</script></li><li><p><strong>Step5</strong> 取U的前2列特征向量，逆分解得到新的协方差矩阵：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}old\_cov = \left( {1 - cp\_rate} \right) \times old\_cov + cp\_rate \times cp\_{\mathop{\rm cov}} \\M = U\left( {:,1:2} \right),N = D\left( {1:2,1:2} \right)\\cp\_{\mathop{\rm cov}}  = M \times N \times {M^T}\end{array} \right.\tag{7-9}</script></li><li><p><strong>Step6</strong> 更新协方差矩阵：</p><script type="math/tex;mode=display">cov = \left( {1 - cp\_rate} \right) \times cp\_cov + cp\_rate \times {\mathop{\rm cov}} \tag{7-10}</script></li><li><p><strong>Step7</strong> 得到CN2特征：</p><script type="math/tex;mode=display">CN2 = reshape(x\_pca \times M,(H,W,2))\tag{7-11}</script><p>下图可以看见原图，灰度图以及CN2特征图的区别，因为CN特征有10个通道，这里我就不放了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190201/1549286021525.png" alt="1549286021525"></p></li></ul><h2 id="8-算法实现"><a href="#8-算法实现" class="headerlink" title="8. 算法实现"></a>8. 算法实现</h2><h3 id="8-1-多通道图像特征矩阵求解"><a href="#8-1-多通道图像特征矩阵求解" class="headerlink" title="8.1 多通道图像特征矩阵求解"></a>8.1 多通道图像特征矩阵求解</h3><p>利用上述理论推导可以求得每一帧 fhog 特征，如果将其视为多通道的话，那么就是 31 通道的图像特征矩阵，然后分别对各通道使用二维汉宁窗进行滤波，为了降低 FFT 过程带来的频谱泄露，其函数形式如下：</p><script type="math/tex;mode=display">f\left( {x,y} \right) = \frac{1}{4}\left( {1 - \cos \left( {2\pi \frac{x}{n}} \right)} \right)\left( {1 - \cos \left( {2\pi \frac{y}{n}} \right)} \right)\tag{8-1}</script><p>然后分别对各个通道求解其<script type="math/tex">\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}</script>，对各通道数据进行多通道处理，处理方式如下:</p><script type="math/tex;mode=display">\begin{array}{l}{k^{xx'}} = g\left( {C\left( x \right)x'} \right)\\\;\;\;\;\;\; = g\left( {\sum\limits_{c = 1}^{L = 31} {C\left( {{x^c}} \right)x'} } \right)\\\;\;\;\;\;\; = g\left( {\sum\limits_{c = 1}^L {{F^{ - 1}}\left( {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} \right)} } \right)\\\;\;\;\;\;\; = g\left( {{F^{ - 1}}\left( {\sum\limits_{c = 1}^L {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} } \right)} \right)\\\;\;\;\;\;\; = {e^{ - \frac{{{{\left\| x \right\|}^2} + {{\left\| {x'} \right\|}^2} - 2{F^{ - 1}}\left( {\sum\limits_{c = 1}^L {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} } \right)}}{{{\sigma ^2}}}}}\end{array}</script><h3 id="8-2-标签制作"><a href="#8-2-标签制作" class="headerlink" title="8.2 标签制作"></a>8.2 标签制作</h3><p>利用第五章所得结论，可以求得${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over \alpha } }$。对于 groundtruth，由于模板函数的中心就是目标框的中心，因此论文中使用高斯分布函数作为标签,其分布函数如下：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}g\left( {x,y} \right) = {e^{ - \frac{{{{\left( {i - cx} \right)}^2} + {{\left( {j - cy} \right)}^2}}}{{2{\sigma ^2}}}}}\\\sigma  = \frac{{\sqrt {sizeX \cdot sizeY} }}{{padding \cdot output\_\sigma }}\end{array} \right.\tag{8-2}</script><p>其中,(cx,cy)表示图像特征矩阵中心，padding 表示扩展框相对目标框的变化比例 2.5，${output_\sigma }​$表示设定的一个值 0.125。</p><h3 id="8-3-多尺度检测"><a href="#8-3-多尺度检测" class="headerlink" title="8.3 多尺度检测"></a>8.3 多尺度检测</h3><p>论文代码中，作者设置了三种尺度，设定尺度步长为 scalestep=1.05，然后分别以 $1.05^{-1}$,$1.05^0$,$1.05^1$三种尺度 scale 进行检测，这里尺度的操作对象是第六章所提到的 roi 矩阵，即<script type="math/tex">roi_{w,h}=scale*roi_{w,h}</script>。首先，为了防止在更新过程中目标框左上角位于图像边界，从而目标框变成了一条线或者一个点，作者将这一类目标框的左上角向远离图像边界的方向移动了一个单位。</p><p>每一尺度都能求得 f(z)响应矩阵，虽然该响应矩阵对应了循环矩阵每个块矩阵的响应，但是第 i 行第 j 列的块矩阵所对应的响应，正是目标框右移 i-1 个单位，下移 j-1 个单位后的响应，即下一帧图像矩阵 z 的响应。对于该响应矩阵，找出其最大响应值 peak<em>value 和最大响应位置 $p</em>{xy}​$。<br>如果最大响应位置不在图像边界，那么分别比较最大响应位置两侧的响应大小，如果右侧比左侧高，或者下侧比上侧高，则分别将最大响应位置向较大的一侧移动一段距离：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}{p_x} = {p_x} + 0.5\frac{{right - left}}{{2peak\_value - right - left}}\\{p_y} = {p_y} + 0.5\frac{{down - up}}{{2peak\_value - down - up}}\end{array} \right.\tag{8-3}</script><p>然后计算此位置与图像中心的距离 res。</p><p>对于不同的尺度，都有着尺度惩罚系数 scale_weight,用此系数乘以该尺度下的最大响应值作为该尺度下的真实最大响应值，取最大响应值对应的尺度为最佳尺度，记为 best_scale。以此来更新目标框参数 T（x,y,w,h）：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}scal{e_z} = scal{e_z} \cdot best\_scale\\{T_{w,h}} = {T_{w,h}} \cdot best\_scale\\{T_{x,y}} = {T_{x,y}} - {T_{w,h}}/2 + re{s_{x,y}} \cdot cell\_size \cdot best\_scale\end{array} \right.\tag{8-4}</script><h3 id="8-4-模板更新"><a href="#8-4-模板更新" class="headerlink" title="8.4 模板更新"></a>8.4 模板更新</h3><p>首先获取原尺度下当前帧的 fhog 特征矩阵 z，作者代码中这一部分没有用汉宁窗进行滤波，可能是考虑到更新时这一部分的权重不大，其模板和${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over \alpha } }​$ 的更新公式如下：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}template = \left( {1 - 0.012} \right) \times template + 0.012z\\\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha }  = \left( {1 - 0.012} \right) \times \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha }  + 0.012{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } }_{xz}}\end{array} \right.\tag{8-5}</script><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9.总结"></a>9.总结</h2><p>整体来讲，本文通过循环矩阵的方式将循环移位的运算复杂度降低了，然后通过引入傅里叶变换使得主要的矩阵乘法变成了点乘，再次降低了运算量。再通过引入对偶空间和核函数，增加了岭回归分类器的性能。整体来说，其算法结构比较简单，也正因如此，其跟踪速度也很快。</p><p>不过其中也暴露了很多问题:</p><ul><li>虽然模板和 ${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over \alpha } }$更新部分对于当前帧的权重比较小，但是其更新是针对整个矩阵进行的，所以如果出现一段遮挡的场景，此模板将一去不复返。这与 siam-fc的模板更新不同，深度网络的跟踪方式通常只是更新目标框的中心位置，以及对目标框大小的微调；</li><li>论文中使用了三个尺度，对于尺度的应用很微小，所以作用很小，在后面Martin Danelljan 所提出的改进算法 DSST 中算法效果有显著提高，不过用了33 个尺度，其对应的算法速度也就降下来了，所以该算法的核心竞争力降低了；</li><li>当目标出现形变时，效果会变得很不好，因为 KCF 的核心其实是模板匹配，目标变形时，自然也就难以匹配好；</li><li>论文中加入的汉宁窗虽然有减少 FFT 频谱泄露的作用，但是由于其分布特性，使得边缘像素cell 的值几乎为 0，因此丢失了大量信息。可能最大响应位置并非最大，甚至有可能过滤掉出现在边缘的目标；</li></ul><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p>High-Speed Tracking with Kernelized Correlation Filters, Joao F.Henriques, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , 2015.</p></li><li><p>Forsyth D . Object Detection with Discriminatively Trained Part-Based Models[J]. 2014.</p></li><li><p><a href="https://www.jianshu.com/p/69a3e39c51f9">https://www.jianshu.com/p/69a3e39c51f9</a></p></li><li><p><a href="https://www.cnblogs.com/torsor/p/8848641.html">https://www.cnblogs.com/torsor/p/8848641.html</a></p></li><li><p>原作者C++源码：<a href="https://github.com/joaofaro/KCFcpp">https://github.com/joaofaro/KCFcpp</a></p></li><li><p>QiangWang复现C++源码：<a href="https://github.com/foolwood/KCF">https://github.com/foolwood/KCF</a></p></li><li><p>原作者博士论文：<a href="http://www.robots.ox.ac.uk/~joao/publications/henriques_phd.pdf">http://www.robots.ox.ac.uk/~joao/publications/henriques_phd.pdf</a></p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;核相关滤波算法是单目标跟踪领域一个举足轻重的算法，而kernelized correlation filters(KCF)是其原始形态，下面我以一个小白的角度慢慢揭开其神秘面纱。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
    
      <category term="目标跟踪" scheme="https://huangpiao.tech/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="KCF" scheme="https://huangpiao.tech/tags/KCF/"/>
    
  </entry>
  
  <entry>
    <title>python中的默认参数陷阱</title>
    <link href="https://huangpiao.tech/2019/01/31/python%E4%B8%AD%E7%9A%84%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E9%99%B7%E9%98%B1/"/>
    <id>https://huangpiao.tech/2019/01/31/python中的默认参数陷阱/</id>
    <published>2019-01-31T13:23:00.000Z</published>
    <updated>2019-01-31T13:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>一直以来，为了函数或者类的方便使用，经常使用默认参数来简化使用，不过最近发现这种方法中存在一个官方陷阱，详见下文。</p></blockquote><a id="more"></a><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>现有下面一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(v, target=[])</span>:</span></span><br><span class="line">    target.append(v)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line">r = add_to(<span class="number">3</span>)</span><br><span class="line">print(r)  <span class="comment"># [1, 2, 3]</span></span><br></pre></td></tr></table></figure><p>我们可以看到，我们想要的输出应该是<code>[3]</code>，然后实际上就出乎我们的意料，原因在于，python对于函数中的默认参数，会在编译的时候申请一块内存，如果默认参数是<code>可变参数</code>，那么重复调用该函数或者类初始化，会共用这一块内存，从而使得实际输出与理论输出不一致。</p><h2 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2.解决方案"></a>2.解决方案</h2><ul><li><p>采用<code>不可变参数</code>作为默认参数</p><p><code>不可变参数</code>包含<code>常数</code>、<code>元组</code>、<code>None</code>等等，这些参数作为默认参数后，重复调用会重复申请一块空间，而对于<code>可变参数</code>——·列表`，则是会共用空间。</p></li><li><p>初始化参数</p><p>每次调用函数或者类构造函数时，都给带有<code>可变参数</code>的默认参数重新赋值，即可重新申请空间。</p></li><li><p>内部判断</p><p>在函数内部判断，从而根据不同的输入条件，赋予不同的值，这样就避免了默认参数的空间申请。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_to</span><span class="params">(v, target=[])</span>:</span></span><br><span class="line">    <span class="keyword">if</span> target == []:</span><br><span class="line">        target = []</span><br><span class="line">    target.append(v)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">add_to(<span class="number">1</span>)</span><br><span class="line">add_to(<span class="number">2</span>)</span><br><span class="line">r = add_to(<span class="number">3</span>)</span><br><span class="line">print(r)  <span class="comment"># [3]</span></span><br></pre></td></tr></table></figure></li></ul><p>下面给出网上的示例图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190131/20181012140252428.png" alt="img"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190131/2018101214120180.png" alt="img"></p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;一直以来，为了函数或者类的方便使用，经常使用默认参数来简化使用，不过最近发现这种方法中存在一个官方陷阱，详见下文。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="python" scheme="https://huangpiao.tech/categories/python/"/>
    
      <category term="在bug中写python" scheme="https://huangpiao.tech/categories/python/%E5%9C%A8bug%E4%B8%AD%E5%86%99python/"/>
    
    
      <category term="python" scheme="https://huangpiao.tech/tags/python/"/>
    
      <category term="默认参数" scheme="https://huangpiao.tech/tags/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>python中的函数传参</title>
    <link href="https://huangpiao.tech/2019/01/31/python%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BC%A0%E5%8F%82/"/>
    <id>https://huangpiao.tech/2019/01/31/python中的函数传参/</id>
    <published>2019-01-31T13:00:00.000Z</published>
    <updated>2019-01-31T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>为了方便使用，我们常希望函数接口更为灵活，既有简便的默认参数，还有方便扩充的可变参数，关键字参数等等，下面我们具体介绍这些函数传参的使用方法。</p></blockquote><a id="more"></a><h2 id="1-常规参数"><a href="#1-常规参数" class="headerlink" title="1. 常规参数"></a>1. 常规参数</h2><p>python中，函数的传参是一个很简单的过程，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(A,B,C)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> C(A),B(A)</span><br></pre></td></tr></table></figure><p>上面我给出的例子中，A,B是两个正常的参数，可能是<strong>常数</strong>，也可能是<strong>列表</strong>/<strong>元组</strong>，而C则是一个<strong>类</strong>或者<strong>函数</strong>，当然，在python中函数也是一个实例，所以可以作为参数。上面函数的使用方法可以是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = test(<span class="number">1</span>,<span class="number">2</span>,func)</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">x,y = test(<span class="number">1</span>,<span class="number">2</span>,C=func)</span><br></pre></td></tr></table></figure><p>上面的用法说明，函数的传参过程中，对于没有关键字的参数，会按照函数定义的参数顺序进行读取，有关键字的则会匹配赋值，从而实现函数传参。而返回值则会将<code>return</code>后面的部分以<code>tuple</code>的方式返回，如果<code>return</code>后为空，则返回<code>None</code>，上面的<code>z</code>就是一个<code>tuple</code>，而<code>x</code>和<code>y</code>则分别表示<code>tuple</code>内的两个元素。</p><h2 id="2-默认参数"><a href="#2-默认参数" class="headerlink" title="2. 默认参数"></a>2. 默认参数</h2><p>有时候函数的参数很多，而在某些场景下，很多参数都是一样的，所以我们希望能够将参数固定，需要改变的时候再传入，因而就有了<code>默认参数</code>，默认参数很好理解：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(A = <span class="number">1</span>, B = <span class="number">2</span>, C = func1)</span>:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>对于这种情况，我们可以有下面几种使用方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test()</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">test(A = <span class="number">3</span>)</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">test(<span class="number">1</span>, C = func2)</span><br></pre></td></tr></table></figure><p>以上方式使得函数传参更为方便了，然而默认参数的设定要注意的是，必须在非默认参数后面，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(A, B, C=func)</span> √</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">test</span><span class="params">(A, B = <span class="number">2</span>, C)</span>  <span class="title">X</span></span></span><br></pre></td></tr></table></figure><h2 id="3-可变参数"><a href="#3-可变参数" class="headerlink" title="3. 可变参数"></a>3. 可变参数</h2><p>除了上面的基本功能之外，python还提供了更加方便的参数工具，当我们不知道我们要使用的参数数量的时候，可以利用python的<code>*</code>进行设计：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(A, *args)</span>:</span></span><br><span class="line">    print(str(A) + <span class="string">':'</span> + str(args))</span><br></pre></td></tr></table></figure><p>上面的<code>*args</code>参数允许我们传入无限制的参数，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">    test(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">output:</span><br><span class="line">    <span class="number">1</span>:(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>说明，<code>*</code>会将我们传入的参数打包为<code>tuple</code>传入，但是这就存在一个问题，如果我们想在程序内部使用的话，传入多个参数也是一个问题，这里实际上<code>*</code>还具有一个功能，那就是解包，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">y = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">z1 = test(<span class="number">1</span>,*x)</span><br><span class="line">z2 = test(<span class="number">1</span>,*y)</span><br></pre></td></tr></table></figure><p>这样的话我们就可利用列表或者元组实现多参数传入~</p><h2 id="4-关键字参数"><a href="#4-关键字参数" class="headerlink" title="4.关键字参数"></a>4.关键字参数</h2><p>上面介绍的<code>*</code>可变参数不具有关键字，因此，在有些时候也不能实现我们想要的功能，如：一个数据表中，表的变量数不定，我们需要传入每个变量的参数，还需要知道每个参数对应的变量名，因此就需要用到关键字可变参数<code>**</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> kwargs:</span><br><span class="line">        <span class="keyword">print</span> str(key) + <span class="string">':'</span> + str(kwargs[key])</span><br></pre></td></tr></table></figure><p>用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">    test(A = <span class="number">1</span>, B = <span class="number">2</span> ,C = <span class="number">3</span>)</span><br><span class="line">output:</span><br><span class="line">    A:<span class="number">1</span></span><br><span class="line">    B:<span class="number">2</span></span><br><span class="line">    C:<span class="number">3</span></span><br></pre></td></tr></table></figure><p>同样的，我们可以利用<code>tuple</code>解包实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">    x = &#123;<span class="string">'A'</span>:<span class="number">1</span>,<span class="string">'B'</span>:<span class="number">2</span>,<span class="string">'C'</span>:<span class="number">3</span>&#125;</span><br><span class="line">    test(**x)</span><br><span class="line">output:</span><br><span class="line">    A:<span class="number">1</span></span><br><span class="line">    B:<span class="number">2</span></span><br><span class="line">    C:<span class="number">3</span></span><br></pre></td></tr></table></figure><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;为了方便使用，我们常希望函数接口更为灵活，既有简便的默认参数，还有方便扩充的可变参数，关键字参数等等，下面我们具体介绍这些函数传参的使用方法。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="python" scheme="https://huangpiao.tech/categories/python/"/>
    
      <category term="python中的奇技淫巧" scheme="https://huangpiao.tech/categories/python/python%E4%B8%AD%E7%9A%84%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7/"/>
    
    
      <category term="python" scheme="https://huangpiao.tech/tags/python/"/>
    
      <category term="函数传参" scheme="https://huangpiao.tech/tags/%E5%87%BD%E6%95%B0%E4%BC%A0%E5%8F%82/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下的定时任务</title>
    <link href="https://huangpiao.tech/2019/01/29/Ubuntu%E4%B8%8B%E7%9A%84%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <id>https://huangpiao.tech/2019/01/29/Ubuntu下的定时任务/</id>
    <published>2019-01-29T15:50:00.000Z</published>
    <updated>2019-01-29T15:50:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Ubuntu上我们有时候希望做一些定时任务，以应对不断变化的状态或者消息收发，见下文。</p></blockquote><a id="more"></a><h2 id="1-crontab"><a href="#1-crontab" class="headerlink" title="1. crontab"></a>1. crontab</h2><p><code>crontab</code>是Ubuntu自带的定时任务功能，其命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;minute&#125; &#123;hour&#125; &#123;day-of-month&#125; &#123;month&#125; &#123;day-of-week&#125; &#123;users&#125; &#123;full-path-to-shell-script or <span class="built_in">command</span>&#125; </span><br><span class="line">o minute: 区间为 0 – 59 </span><br><span class="line">o hour: 区间为0 – 23 </span><br><span class="line">o day-of-month: 区间为0 – 31 </span><br><span class="line">o month: 区间为1 – 12. 1 是1月. 12是12月. </span><br><span class="line">o Day-of-week: 区间为0 – 7. 周日可以是0或7.</span><br></pre></td></tr></table></figure><p>打个比方，我们要设置test.sh程序定时任务，记得利用<code>chmod</code>赋予权限，在<code>/etc/crontab</code>文件中加入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">50 23 * * * root sh path/to/test.sh <span class="comment"># 每天23:50启动</span></span><br><span class="line">*/1 * * * * root path/to/test.sh <span class="comment">#每隔1分钟启动</span></span><br><span class="line">0 23 * * 1-5 root path/to/test.sh <span class="comment">#每周1~5的23:00启动</span></span><br></pre></td></tr></table></figure><h2 id="2-开机自启动"><a href="#2-开机自启动" class="headerlink" title="2. 开机自启动"></a>2. 开机自启动</h2><p>可在开机自启动任务中利用<code>sleep</code>、<code>while</code>和<code>if</code>三部分实现死循环式的定时任务，不过sleep的存在，会避免cpu占用率高，记得用<code>&amp;</code>放在后台执行。</p><h2 id="3-监听启动"><a href="#3-监听启动" class="headerlink" title="3.监听启动"></a>3.监听启动</h2><p>a.先启动定时任务，处于休眠，然后设置一个启动标志，当检测到某情况发生，则启动任务；</p><p>b.绑定在一个程序上，当程序执行，再启动定时任务。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ubuntu上我们有时候希望做一些定时任务，以应对不断变化的状态或者消息收发，见下文。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux探索" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/"/>
    
      <category term="任务执行" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    
    
      <category term="定时任务" scheme="https://huangpiao.tech/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下的开机自启动任务</title>
    <link href="https://huangpiao.tech/2019/01/29/Ubuntu%E4%B8%8B%E7%9A%84%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E4%BB%BB%E5%8A%A1/"/>
    <id>https://huangpiao.tech/2019/01/29/Ubuntu下的开机自启任务/</id>
    <published>2019-01-29T15:00:00.000Z</published>
    <updated>2019-01-29T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>为了方便，我们经常希望将一些常用的软件或者程序设为开机启动，这里我以Ubuntu16.04为例进行讲解,介绍几种常见的开机自启动方法。</p></blockquote><a id="more"></a><h2 id="1-startup-application"><a href="#1-startup-application" class="headerlink" title="1. startup application"></a>1. startup application</h2><ul><li><p>Step1 给执行文件(自带文件头:<code>#!/bin/sh</code>或者<code>#!/usr/bin/env python</code>)权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 777 可执行文件</span><br></pre></td></tr></table></figure></li><li><p>Step2 搜索<code>Startup Application</code>:</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548781801348.png" alt="1548781801348"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548781812967.png" alt="1548781812967"></p><p>直接添加<code>任务名称Name</code>、<code>任务执行文件绝对路径或者执行命令Command</code>和<code>备注Comment</code>。</p></li></ul><h2 id="2-rc-local"><a href="#2-rc-local" class="headerlink" title="2. rc.local"></a>2. rc.local</h2><p>我们可以直接在 <code>/etc/rc.local</code>中添加开机启动命令,：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/sh -e</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># rc.local</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This script is executed at the end of each multiuser runlevel.</span></span><br><span class="line"><span class="comment"># Make sure that the script will "exit 0" on success or any other</span></span><br><span class="line"><span class="comment"># value on error.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to enable or disable this script just change the execution</span></span><br><span class="line"><span class="comment"># bits.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default this script does nothing.</span></span><br><span class="line"><span class="built_in">command</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><p>另外，记得赋予待执行的源文件可执行权限。</p><h2 id="3-init-d"><a href="#3-init-d" class="headerlink" title="3.init.d"></a>3.init.d</h2><p>下面保持当前路径为<code>/etc/init.d</code>：</p><ul><li><p>Step1 在<code>/etc/init.d</code>新建一个开机自启动文件，如：<code>test</code></p></li><li><p>Step2 在test中写入指令，最好带上文件头:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">command</span> &amp;</span><br></pre></td></tr></table></figure><p>之所以加上<code>&amp;</code>是因为有些命令是一直执行的，可以放在后台执行。</p></li><li><p>Step3 赋权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 777 <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li><li><p>Step4 更新开机自启动列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo update-rc.d <span class="built_in">test</span> defaults </span><br><span class="line">或者</span><br><span class="line">sudo update-rc.d <span class="built_in">test</span> defaults 数字</span><br></pre></td></tr></table></figure><p>加上数字是为了防止有些开机自启动任务有顺序要求，数字越大越晚执行。</p><p>删除方式是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-rc.d -f <span class="built_in">test</span> remove</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-systemd"><a href="#4-systemd" class="headerlink" title="4. systemd"></a>4. systemd</h2><p><code>systemd</code>是Ubuntu16.04及之后官方的开机自启动管理方式，我们可以在<code>/etc/systemd/system</code>中新建一个服务<code>test.service</code>,权限记得哦~，然后写入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=this is <span class="built_in">test</span> service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart= &lt;shell <span class="built_in">command</span>&gt;  <span class="comment">#启动命令</span></span><br><span class="line">ExecStop=&lt;shell <span class="built_in">command</span>&gt;    <span class="comment">#停止命令，可缺省</span></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target <span class="comment">#所有用户组都启动这个任务</span></span><br></pre></td></tr></table></figure><p>各模块具体意义可参考<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html">这里</a>。</p><p>然后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> test.service</span><br></pre></td></tr></table></figure><p>如果想即刻运行，则:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start test.service</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上可通过<code>systemctl status commandname.service</code> 来查看开机自启动项目是否设立成功，也可以在开机之后利用进程监控。</p></blockquote><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;为了方便，我们经常希望将一些常用的软件或者程序设为开机启动，这里我以Ubuntu16.04为例进行讲解,介绍几种常见的开机自启动方法。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux探索" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/"/>
    
      <category term="任务执行" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    
    
      <category term="开机自启动" scheme="https://huangpiao.tech/tags/%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下快捷方式的制作</title>
    <link href="https://huangpiao.tech/2019/01/29/Ubuntu%E4%B8%8B%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%E7%9A%84%E5%88%B6%E4%BD%9C/"/>
    <id>https://huangpiao.tech/2019/01/29/Ubuntu下快捷方式的制作/</id>
    <published>2019-01-29T14:30:00.000Z</published>
    <updated>2019-01-29T14:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Ubuntu上为了方便程序的快速执行，我们希望能够像windows那样双击运行程序，下面我会详细介绍。</p></blockquote><a id="more"></a><h2 id="1-设定程序快捷启动"><a href="#1-设定程序快捷启动" class="headerlink" title="1. 设定程序快捷启动"></a>1. 设定程序快捷启动</h2><p>对于可执行文件，Ubuntu上默认的打开方式是<code>gedit</code>，从而可以进行文本编辑，但是，如果我们想直接双击运行的话，则需要简单做以下几件事：</p><ul><li><p>Step1 添加文件头，提供文件执行所需解释器,下面给出几个针对<code>shell</code>和<code>python</code>文件的文件头示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>Step2 给文件可执行权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x 文件</span><br></pre></td></tr></table></figure></li><li><p>Step3 设定双击启动：</p><p>设定双击启动有两种方法，我主要讲第一种，首先我们打开任意一个<strong>目录</strong>，并<strong>最大化</strong>，点击<code>Edit-Perferences-Behavior</code>：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548770186248-1548772652676.png" alt="1548770186248"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548770213469.png" alt="1548770213469"></p><p><strong>可以看到，我们可以设置可执行文件的点击行为：<code>直接执行</code>、<code>文本编辑</code>、<code>询问执行/显示</code>，我的建议是调试模式下选择询问模式，因为询问模式可以选择<code>终端执行、后台执行、显示</code>三种，如果直接执行的话，会变为后台执行。</strong></p><p>选做：</p><p>​ 跟这种方式一样的还有一个软件<code>dconf-editor</code>，主要是面向那些找不着顶端选项的人的，首先安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install dconf-editor</span><br></pre></td></tr></table></figure><p>然后执行<code>dconf-editor</code>，选择<code>org-&gt;gnome-&gt;natuilus-&gt;preferences-&gt;executable-text-activation</code>:</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548771110993.png" alt="1548771110993"></p><p>设置一下就好了。</p></li></ul><h2 id="2-绑定快捷方式"><a href="#2-绑定快捷方式" class="headerlink" title="2. 绑定快捷方式"></a>2. 绑定快捷方式</h2><p>快捷方式的作用，就相当于在快捷执行的基础上增加了<strong>图标</strong>以及<strong>属性值</strong>，绑定快捷方式的过程也是基于第一章中的<code>Step1</code>和<code>Step2</code>，在此基础上，我们可以在<code>/usr/share/applications</code>中新建一个桌面文件<code>文件名.desktop</code>，然后编辑内容，我们以Ubuntu自带蓝牙的桌面文件部分内容为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Name=Bluetooth <span class="comment"># 外显文件名称(（)相当于重命名)</span></span><br><span class="line">Comment=Configure Bluetooth settings</span><br><span class="line">Icon=bluetooth <span class="comment">#写入待显示图标的源文件路径(最好是绝对路径)</span></span><br><span class="line">Exec=unity-control-center bluetooth <span class="comment">#用来写执行执行文件路径(最好是绝对路径)</span></span><br><span class="line">Terminal=<span class="literal">false</span> <span class="comment">#是否以终端形式执行</span></span><br><span class="line">Type=Application <span class="comment">#类型</span></span><br><span class="line">Categories=GTK;GNOME;Settings;X-GNOME-NetworkSettings;HardwareSettings;X-Unity-Settings-Panel;<span class="comment">#分类</span></span><br></pre></td></tr></table></figure><p>有了这些，我们就可以在搜索栏搜索其<code>Name</code>，即：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190129/1548772425710.png" alt="1548772425710"></p><p>单击即可执行，既可以将其拖动到桌面或者侧边栏固定，也可以重命名为想要的名字。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ubuntu上为了方便程序的快速执行，我们希望能够像windows那样双击运行程序，下面我会详细介绍。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux探索" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/"/>
    
      <category term="任务执行" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    
    
      <category term="快捷方式" scheme="https://huangpiao.tech/tags/%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下的多任务执行</title>
    <link href="https://huangpiao.tech/2019/01/29/Ubuntu%E4%B8%8B%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    <id>https://huangpiao.tech/2019/01/29/Ubuntu下的多任务执行/</id>
    <published>2019-01-29T04:30:00.000Z</published>
    <updated>2019-01-29T04:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Ubuntu上我们有时候需要同时开启多个任务，可能是串行执行，可能是并行执行，也可能是多终端执行等等，下面我将介绍Ubuntu中开启多任务的方式。</p></blockquote><a id="more"></a><h2 id="1-任务执行的监控"><a href="#1-任务执行的监控" class="headerlink" title="1. 任务执行的监控"></a>1. 任务执行的监控</h2><p>我们之前在<a href="https://huangpiao.tech/2019/01/26/Ubuntu%E5%85%B3%E9%97%AD%E6%8C%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E8%BF%9B%E7%A8%8B/">Ubuntu关闭指定程序进程</a>中介绍过如何查看指定程序的进程。当然啦，除此之外，我们的终端或者UI界面也能够反应程序的执行与否，即通过前端显示来判断程序运行情况。</p><p>另外，如果我们将程序设为后台执行，可利用<code>nohup</code>命令，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python test.py &gt; my_nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>其中<code>my_nohup.log</code>是日志路径。</p><h2 id="2-串行任务执行"><a href="#2-串行任务执行" class="headerlink" title="2. 串行任务执行"></a>2. 串行任务执行</h2><p>串行任务的执行方式我们见到过很多，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install cmake build-essential libopencv-dev</span><br></pre></td></tr></table></figure><p>或者直接用换行方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">command1</span><br><span class="line">command2</span><br><span class="line">command3</span><br></pre></td></tr></table></figure><p>亦或是<code>;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command1;command2;</span><br></pre></td></tr></table></figure><h2 id="3-并行任务执行"><a href="#3-并行任务执行" class="headerlink" title="3.并行任务执行"></a>3.并行任务执行</h2><h3 id="3-1-隐式并行"><a href="#3-1-隐式并行" class="headerlink" title="3.1 隐式并行"></a>3.1 隐式并行</h3><p>隐式并行的方式就是利用<code>&amp;</code>并行执行多任务，即：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">command1 &amp;</span><br><span class="line">command2 &amp;</span><br><span class="line">command3 &amp;</span><br></pre></td></tr></table></figure><p>这样的话，多任务会并行执行，并且共享一个终端。</p><h3 id="3-2-显式并行"><a href="#3-2-显式并行" class="headerlink" title="3.2 显式并行"></a>3.2 显式并行</h3><p><strong>选看</strong>：</p><p>显式并行最好的方式就是开多个终端执行了，皮一下:laughing:,其实还有一种可以在同一窗口执行多终端的方式，即：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install screen</span><br></pre></td></tr></table></figure><p>我们可以依次创建新的子窗口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -S 窗口名</span><br><span class="line"><span class="built_in">command</span></span><br></pre></td></tr></table></figure><p>screen的用法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">screen -ls <span class="comment">#显示所有子窗口</span></span><br><span class="line">screen -r screenid <span class="comment">#切换至指定窗口</span></span><br><span class="line">ctrl+a+n <span class="comment">#切换至下一窗口</span></span><br><span class="line">ctel+a+p <span class="comment">#切换至上一窗口</span></span><br></pre></td></tr></table></figure><p>不过，上面都是题外话啦~，那么，我现在想并行开多个终端执行并行任务，可行吗？事实是可行的。</p><p><strong>正文：</strong></p><p>我们可以利用<code>gnome-terminal</code>命令开启多个终端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gnome-terminal -x <span class="built_in">command</span> <span class="comment">#开启一个窗口执行一个命令</span></span><br><span class="line">gnome-terminal -e <span class="string">'command'</span> <span class="comment">#开启一个窗口</span></span><br></pre></td></tr></table></figure><p>利用上述指令即可完成多终端任务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gnome-terminal -e <span class="string">'command1'</span></span><br><span class="line">gnome-terminal -e <span class="string">'command2'</span></span><br><span class="line">gnome-terminal -e <span class="string">'command3'</span></span><br><span class="line">gnome-terminal -e <span class="string">'command4'</span></span><br></pre></td></tr></table></figure><p>上述命令有个问题，就是命令执行完毕会自动关闭窗口，为了避免，可以加上<code>exec bash</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gnome-terminal -e <span class="string">'bash -c "command;exec bash"'</span></span><br></pre></td></tr></table></figure><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ubuntu上我们有时候需要同时开启多个任务，可能是串行执行，可能是并行执行，也可能是多终端执行等等，下面我将介绍Ubuntu中开启多任务的方式。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux探索" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/"/>
    
      <category term="任务执行" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    
    
      <category term="多任务" scheme="https://huangpiao.tech/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下opencv的安装与卸载</title>
    <link href="https://huangpiao.tech/2019/01/28/Ubuntu%E4%B8%8Bopencv%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%8D%B8%E8%BD%BD/"/>
    <id>https://huangpiao.tech/2019/01/28/Ubuntu下opencv的安装与卸载/</id>
    <published>2019-01-28T15:00:00.000Z</published>
    <updated>2019-01-28T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Ubuntu上我们一般需要安装opencv来完成图像处理等程序操作，下面我以opencv3.4.0在Ubuntu16.04上的安装为例进行讲解。</p></blockquote><a id="more"></a><h2 id="1-系统自带opencv"><a href="#1-系统自带opencv" class="headerlink" title="1. 系统自带opencv"></a>1. 系统自带opencv</h2><p>Ubuntu16.04系统资源库自带<code>opencv2.4.8</code>，如果不需要用到高级功能，这点即可。其下载方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libopencv-dev</span><br></pre></td></tr></table></figure><p>判断系统上是否存在opencv，可输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pkg-config --modversion opencv</span><br></pre></td></tr></table></figure><p>卸载方法为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt remove libopencv-dev</span><br></pre></td></tr></table></figure><h2 id="2-官网下载安装opencv"><a href="#2-官网下载安装opencv" class="headerlink" title="2.官网下载安装opencv"></a>2.官网下载安装opencv</h2><h3 id="2-1-安装opencv"><a href="#2-1-安装opencv" class="headerlink" title="2.1 安装opencv"></a>2.1 安装opencv</h3><ul><li><p>Step1 卸载已有opencv</p></li><li><p>Step2 进入<a href="https://opencv.org/releases.html">Opencv官网</a>选择对应版本<code>source</code>下载。</p></li><li><p>Step3 在下载路径执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"opencv-3.4.0/"</span> ];<span class="keyword">then</span></span><br><span class="line">unzip opencv-3.4.0.zip -d ./</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"opencv package has been unzipped"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sudo apt-get install cmake pkg-config build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev  -y</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> opencv-3.4.0/</span><br><span class="line">sudo mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">sudo make clean</span><br><span class="line">cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span>  ..</span><br><span class="line">sudo make -j8</span><br><span class="line">sudo make install</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/ld.so.conf.d/opencv.conf | grep -c <span class="string">'/usr/local/lib'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'/usr/local/lib'</span> &gt;&gt; /etc/ld.so.conf.d/opencv.conf</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/bash.bashrc | grep -c <span class="string">'PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig'</span> &gt;&gt;  /etc/bash.bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/bash.bashrc | grep -c <span class="string">'export PKG_CONFIG_PATH'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'export PKG_CONFIG_PATH'</span> &gt;&gt;  /etc/bash.bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sudo ldconfig</span><br><span class="line">sudo updatedb</span><br><span class="line"><span class="built_in">source</span> /etc/bash.bashrc</span><br></pre></td></tr></table></figure></li><li><p>Step4 如果安装过程中卡在<code>ippicv</code>这里，可以去<a href="https://github.com/opencv/opencv_3rdparty/blob/ippicv/master_20170822/ippicv/ippicv_2017u3_lnx_intel64_general_20170822.tgz">这里</a>下载,然后重命名为<code>4e0352ce96473837b1d671ce87f17359-ippicv_2017u3_lnx_intel64_general_20170822.tgz</code>，放入<code>opencv-3.4.0/.cache/ippicv/</code>下，重新安装。</p></li></ul><h3 id="2-2-卸载opencv"><a href="#2-2-卸载opencv" class="headerlink" title="2.2 卸载opencv"></a>2.2 卸载opencv</h3><p>卸载方法如下，进入下载路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> opencv-3.4.0/build</span><br><span class="line">sudo make uninstall</span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line">sudo rm -rf opencv-3.4.0/</span><br><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/include/opencv2 /usr/<span class="built_in">local</span>/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/<span class="built_in">local</span>/share/opencv /usr/<span class="built_in">local</span>/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/<span class="built_in">local</span>/bin/opencv* /usr/<span class="built_in">local</span>/lib/libopencv* /usr/<span class="built_in">local</span>/lib/pkgconfig</span><br><span class="line">sudo rm -rf /etc/ld.so.conf.d/opencv.conf</span><br><span class="line">sudo sed -i <span class="string">'/PKG_CONFIG_PATH/d'</span> /etc/bash.bashrc</span><br><span class="line">sudo ldconfig</span><br><span class="line">sudo updatedb</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">source</span> /etc/bash.bashrc</span><br></pre></td></tr></table></figure><h2 id="3-python版opencv"><a href="#3-python版opencv" class="headerlink" title="3.python版opencv"></a>3.python版opencv</h2><p>python版本的opencv可通过<code>pip</code>直接下载安装，<code>pip</code>下载源建议设置为国内源，即在当前用户<code>HOME</code>目录下新建一个<code>.pip/pip.conf</code>文件，写入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure><p>然后下载安装<code>python-opencv</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure><p>卸载方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall opencv-python</span><br></pre></td></tr></table></figure><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ubuntu上我们一般需要安装opencv来完成图像处理等程序操作，下面我以opencv3.4.0在Ubuntu16.04上的安装为例进行讲解。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="opencv" scheme="https://huangpiao.tech/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下绑定USB设备</title>
    <link href="https://huangpiao.tech/2019/01/28/Ubuntu%E7%BB%91%E5%AE%9A%E8%AE%BE%E5%A4%87USB/"/>
    <id>https://huangpiao.tech/2019/01/28/Ubuntu绑定设备USB/</id>
    <published>2019-01-28T15:00:00.000Z</published>
    <updated>2019-01-28T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>由于板卡属于外部组件，通过USB和服务器建立联系，而Linux系统对于外部USB接口是根据插入顺序命名的，这一点对于板卡的访问有很大的局限性，因此我根据板卡的内核型号不同对其进行了绑定。</p></blockquote><a id="more"></a><ul><li><p><strong>Step1</strong> <strong>查询主设备号</strong>。利用cat /proc/devices 查询ttyUSB的主设备号，每个usb接口都会在/dev目录下产生一个ttyUSB*文件</p></li><li><p><strong>Step2</strong> 利用<code>lsusb</code>查询当前的接口情况：</p></li></ul><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548686336131.png" alt="1548686336131"></p><p>​ 可以看到有三个同型号的usb接口，和一个用了转接头的其他型号usb接口。</p><ul><li><strong>Step3</strong> <strong>查询板卡详细信息。</strong>以ttyUSB0为例，利用udevadm info -a /dev/ttyUSB0这种方式查询该板卡的详细信息：</li></ul><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548686364634.png" alt="1548686364634"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548686384342.png" alt="1548686384342"></p><p>​ 第一个KERNEL是唯一的，第二KERNELS部分有很多，所以需要先找设备的生产号，即idProduct和idVendor， 与上一步的查询结果对应；</p><ul><li><p><strong>Step4 利用板卡信息，建立软连接</strong>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/udev/rules.d/4-device.rules</span><br></pre></td></tr></table></figure><p>（这里4对应了上面查询的主设备号，-device随意取名，后缀为rules），编辑内容如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548686677620.png" alt="1548686677620"></p><p>将上面查询的信息填入其中，并填写一个链接名字，如SYMLINK+=“device0”</p></li><li><p><strong>Step5 重新插拔USB端口，重启电脑</strong>，之后输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l /dev/device*</span><br></pre></td></tr></table></figure></li></ul><p>​ <img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548686695407.png" alt="1548686695407"></p><p>​ 可以发现虽然ttyUSB序号变了，但是device已经自动重定向了。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;由于板卡属于外部组件，通过USB和服务器建立联系，而Linux系统对于外部USB接口是根据插入顺序命名的，这一点对于板卡的访问有很大的局限性，因此我根据板卡的内核型号不同对其进行了绑定。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="USB" scheme="https://huangpiao.tech/tags/USB/"/>
    
      <category term="板卡" scheme="https://huangpiao.tech/tags/%E6%9D%BF%E5%8D%A1/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04下GPU环境的搭建</title>
    <link href="https://huangpiao.tech/2019/01/28/Ubuntu%E4%B8%8BGPU%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    <id>https://huangpiao.tech/2019/01/28/Ubuntu下GPU环境的搭建/</id>
    <published>2019-01-28T14:00:00.000Z</published>
    <updated>2019-01-28T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Ubuntu下的GPU环境搭建包括显卡驱动的安装，CUDA的安装以及cudnn的安装配置，下面我将具体介绍N卡的GPU环境配置。</p></blockquote><a id="more"></a><h2 id="1-显卡驱动的安装"><a href="#1-显卡驱动的安装" class="headerlink" title="1. 显卡驱动的安装"></a>1. 显卡驱动的安装</h2><h3 id="1-1-官网下载安装"><a href="#1-1-官网下载安装" class="headerlink" title="1.1 官网下载安装"></a>1.1 官网下载安装</h3><p>对于N(Nvidia)卡的驱动安装，我们可以在官网搜索最新的适配的驱动，然后手动安装：</p><ul><li><p>Step1 进入<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn">官网显卡驱动页面</a>搜索合适的显卡驱动并下载：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548679790126.png" alt="1548679790126"></p><ul><li><p>Step2 添加当前显卡驱动进入黑名单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/modprobe.d/blacklist.conf </span><br><span class="line"></span><br><span class="line">blacklist nouveau </span><br><span class="line">blacklist lbm-nouveau </span><br><span class="line">options nouveau modeset=0 </span><br><span class="line"><span class="built_in">alias</span> nouveau off </span><br><span class="line"><span class="built_in">alias</span> lbm-nouveau off</span><br></pre></td></tr></table></figure></li><li><p>Step3 禁止nouveau:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf</span><br><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure></li><li><p>Step4 重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></li><li><p>Step5 进入字符模式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctrl + alt + F1</span><br></pre></td></tr></table></figure></li><li><p>Step6 输入用户名密码之后，关闭x server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop </span><br><span class="line">sudo init 3</span><br></pre></td></tr></table></figure></li><li><p>Step7 切换至下载路径，一般默认<code>/home/用户名/Downloads/</code>，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> nvidia目录</span><br><span class="line">chmod 777 NVIDIA-Linux-x86_64-版本号.run</span><br><span class="line">sudo sh NVIDIA-Linux-x86_64-版本号.run --no-opengl-files</span><br></pre></td></tr></table></figure></li><li><p>Step8 重启电脑</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="1-2-系统自动更新下载"><a href="#1-2-系统自动更新下载" class="headerlink" title="1.2 系统自动更新下载"></a>1.2 系统自动更新下载</h3><p>Ubuntu16.04系统自带两个显卡驱动，一个是默认的nouveau驱动，另一个是Nvidia-384显卡驱动，我们可以进入<code>设置-软件与更新-附加驱动</code>可以看到：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548681018673.png" alt="1548681018673"></p><p>选择NVIDIA相应驱动，然后点击<code>Revert</code>即可安装，重启可生效。当然，我们还可以通过命令行安装，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nvidia-384</span><br></pre></td></tr></table></figure><p>这两种方式本质都是一样的，都是基于已有软件源进行安装的，为了安装更新的驱动版本，可以更新软件源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><p>建议在设置中设置默认下载源为中国的软件源。</p><h3 id="1-3踩坑指南"><a href="#1-3踩坑指南" class="headerlink" title="1.3踩坑指南"></a>1.3踩坑指南</h3><ul><li><p>N卡驱动安装生效需要在BIOS设置<code>Secure Boot</code>为<code>false</code>；</p></li><li><p>如果安装失败，且无法进入桌面，可以先进入tty界面(ctrl+alt+F1)，卸载N卡驱动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br></pre></td></tr></table></figure><p>如果是通过runfile安装的，则在安装目录执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./NVIDIA-Linux-x86_64-版本号.run --uninstall</span><br></pre></td></tr></table></figure></li><li><p>输入<code>nvidia-smi</code>或者<code>nvidia-settings</code>，如果有打印信息，则说明安装成功：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548682031861.png" alt="1548682031861"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548682052228.png" alt="1548682052228"></p></li></ul><h2 id="2-CUDA的安装配置"><a href="#2-CUDA的安装配置" class="headerlink" title="2.CUDA的安装配置"></a>2.CUDA的安装配置</h2><p>CUDA是为N卡准备的GPU运算加速库，可以方便我们的GPU计算，一般都自带一个比较旧的显卡驱动，可能跟系统不适配，所以最好不选择安装其自带的显卡驱动，另外，CUDA和显卡驱动也有适配关系：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/20181229015209262.png" alt="20181229015209262"></p><h3 id="2-1-官网下载安装"><a href="#2-1-官网下载安装" class="headerlink" title="2.1 官网下载安装"></a>2.1 官网下载安装</h3><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA官网</a>提供了CUDA的下载安装文件，选择我们想要的版本进行安装即可：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548681862651.png" alt="1548681862651"></p><p>下载提供的所有文件，然后依次安装即可，同样是在下载路径执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 777 cuda*.run</span><br><span class="line">sudo sh ./cuda文件1.run</span><br><span class="line">sudo sh ./cuda文件2.run</span><br></pre></td></tr></table></figure><p>安装过程注意：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">在安装cuda过程中首先会遇到一大串协议，直接Ctrl+C停止，并输入accept即可跳过</span><br><span class="line">安装过程出现的选项选择方式如下：</span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"> </span><br><span class="line">Do you want to install the OpenGL libraries?</span><br><span class="line">(y)es/(n)o/(q)uit [ default is yes ]:  y</span><br><span class="line"> </span><br><span class="line">Install the CUDA 8.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"> </span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-9.0 ]: </span><br><span class="line"> </span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"> </span><br><span class="line">Install the CUDA 8.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line">Enter CUDA Samples Location [ default is /home/dell ]:</span><br></pre></td></tr></table></figure><p>最后我们需要配置环境，由于比较懒，直接命令行执行，以<code>cuda8.0</code>为例:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/profile | grep -c <span class="string">'export PATH=/usr/local/cuda-8.0/bin:/usr/local/cuda/bin:$PATH'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'export PATH=/usr/local/cuda-8.0/bin:/usr/local/cuda/bin:$PATH'</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/profile | grep -c <span class="string">'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH'</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/ld.so.conf.d/cuda.conf | grep -c <span class="string">'/usr/local/cuda-8.0/lib64'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'/usr/local/cuda-8.0/lib64'</span> &gt;&gt; /etc/ld.so.conf.d/cuda.conf</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat /etc/ld.so.conf.d/cuda.conf | grep -c <span class="string">'/usr/local/cuda/lib64'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'/usr/local/cuda/lib64'</span> &gt;&gt; /etc/ld.so.conf.d/cuda.conf</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ 0 -eq $(cat ~/.bash_profile | grep -c <span class="string">'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64'</span>) ] ;<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH'</span> &gt;&gt; ~/.bash_profile</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sudo ldconfig</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><p>终端执行<code>nvcc -V</code>，如果显示cuda信息，则环境配置成功，否则重启电脑重试。</p><h3 id="2-2-Conda安装"><a href="#2-2-Conda安装" class="headerlink" title="2.2 Conda安装"></a>2.2 Conda安装</h3><p>如果你使用的是<code>anaconda</code>包管理器，可以直接安装下载cuda，可以提前设置下载源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>由于conda的环境管理机制，我们在下载安装很多深度框架(tensorflow-gpu)的同时会自动安装cuda和cudnn：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548683120751.png" alt="1548683120751"></p><h3 id="2-3-卸载"><a href="#2-3-卸载" class="headerlink" title="2.3 卸载"></a>2.3 卸载</h3><p>对于<code>cuda8.0</code>直接执行:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo /usr/<span class="built_in">local</span>/cuda/bin/uninstall_cuda_8.0.pl</span><br><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/cuda*/</span><br><span class="line">sudo rm -rf /etc/ld.so.conf.d/cuda.conf</span><br><span class="line">sudo sed -i <span class="string">'cuda-8.0/d'</span> /etc/profile</span><br><span class="line">sudo sed -i <span class="string">'cuda-8.0/d'</span> ~/.bash_profile</span><br><span class="line">sudo ldconfig</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="3-cudnn的安装配置"><a href="#3-cudnn的安装配置" class="headerlink" title="3.cudnn的安装配置"></a>3.cudnn的安装配置</h2><h3 id="3-1-官网下载安装"><a href="#3-1-官网下载安装" class="headerlink" title="3.1 官网下载安装"></a>3.1 官网下载安装</h3><p>进入<a href="https://developer.nvidia.com/rdp/cudnn-archive">cudnn官网</a>注册登录,选择合适的cudnn版本：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190128/1548683283036.png" alt="1548683283036"></p><p>下载得到的是一个压缩包，在下载路径执行(我下的对应版本是<code>cuda8.0, cudnn7.1</code>,若使用可自行修改数字)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf cudnn-8.0-linux-x64-v7.1.tgz</span><br><span class="line">sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include</span><br><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so.7 /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so</span><br><span class="line">sudo cp cuda/lib64/lib* /usr/<span class="built_in">local</span>/cuda/lib64/</span><br><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so.7 /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so</span><br><span class="line">sudo ln -s /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so.7.1.3 /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so.7</span><br><span class="line">sudo ln -s /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so.7 /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn.so</span><br><span class="line">sudo rm -rf cuda/</span><br></pre></td></tr></table></figure><h3 id="3-2-Conda安装"><a href="#3-2-Conda安装" class="headerlink" title="3.2 Conda安装"></a>3.2 Conda安装</h3><p>见2.2节</p><h3 id="3-3卸载"><a href="#3-3卸载" class="headerlink" title="3.3卸载"></a>3.3卸载</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/cuda/include/cudnn.h</span><br><span class="line">sudo  rm -rf /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Ubuntu下的GPU环境搭建包括显卡驱动的安装，CUDA的安装以及cudnn的安装配置，下面我将具体介绍N卡的GPU环境配置。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="cuda" scheme="https://huangpiao.tech/tags/cuda/"/>
    
      <category term="cudnn" scheme="https://huangpiao.tech/tags/cudnn/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下关闭指定程序进程</title>
    <link href="https://huangpiao.tech/2019/01/26/Ubuntu%E5%85%B3%E9%97%AD%E6%8C%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E8%BF%9B%E7%A8%8B/"/>
    <id>https://huangpiao.tech/2019/01/26/Ubuntu关闭指定程序进程/</id>
    <published>2019-01-26T15:30:00.000Z</published>
    <updated>2019-01-26T15:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>有时候我们需要通过命令行关闭特定进程，对于指定进程号的关闭很容易，但是如果需要关闭指定的程序或者关闭指定路径下的程序则变得不那么容易，下面我们将会对此进行讨论。另外，很多方式都会有权限限制，所以可以适时采用<code>sudo</code>方式执行。</p></blockquote><a id="more"></a><h2 id="1-进程资源监控"><a href="#1-进程资源监控" class="headerlink" title="1.进程资源监控"></a>1.进程资源监控</h2><h3 id="1-1-gnome-system-monitor"><a href="#1-1-gnome-system-monitor" class="headerlink" title="1.1 gnome-system-monitor"></a>1.1 gnome-system-monitor</h3><p><code>gnome-system-monitor</code>是Ubuntu系统自带的资源管理器，可以让我们在界面中动态查看所有进程资源以及计算内存消耗情况，可直接在命令行中输入<code>gnome-system-monitor</code>：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515584058.png" alt="1548515584058"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515605903.png" alt="1548515605903"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515617181.png" alt="1548515617181"></p><p>可以看见，以上内容能够很容易帮我们监控系统资源，但是如果我们需要将监控级别设定到指定程序级别，则需要对该界面进行相关配置：</p><ul><li><p>左上角显示设置</p><p>我们可以通过左上角的按钮查询指定程序的进程号，也可以显示出每一个进程的执行命令行信息：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515819111.png" alt="1548515819111"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515869298.png" alt="1548515869298"></p></li><li><p>右上角筛选设置</p><p>默认显示的是当前用户所有的进程信息，但如果想知道当前活跃的进程信息，并显示父子进程的依赖关系，我们可以通过右上角按钮选择：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548515955559.png" alt="1548515955559"></p></li></ul><h3 id="1-2-top"><a href="#1-2-top" class="headerlink" title="1.2 top"></a>1.2 top</h3><p><code>top</code>也是Ubuntu系统自带的一个系统资源监控指令，同样是在命令行执行：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548516050502.png" alt="1548516050502"></p><p>其中上方的<code>%Cpu</code>的意义如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">us — 用户空间占用CPU的百分比。</span><br><span class="line">sy — 内核空间占用CPU的百分比。</span><br><span class="line">ni — 改变过优先级的进程占用CPU的百分比</span><br><span class="line">id — 空闲CPU百分比</span><br><span class="line">wa — IO等待占用CPU的百分比</span><br><span class="line">hi — 硬中断（Hardware IRQ）占用CPU的百分比</span><br><span class="line">si — 软中断（Software Interrupts）占用CPU的百分比</span><br></pre></td></tr></table></figure><p>而下方的则是各个进程的详细信息，其中要注意的是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PID: 进程号</span><br><span class="line">USER: 进程所有者</span><br><span class="line">PR: 进程优先级</span><br><span class="line">VIRT: 进程所占虚拟内存大小，单位：B</span><br><span class="line">RES: 进程所占物理内存大小，单位：B</span><br><span class="line">SHR: 进程所占共享内存大小，单位：B</span><br><span class="line">%CPU: 进程所占CPU使用率</span><br><span class="line">%MEM：进程所占内存使用率</span><br><span class="line">TIME+: 进程启动后所占总的CPU时间</span><br><span class="line">COMMAND: 进程命令名</span><br></pre></td></tr></table></figure><p>特别的，键盘按下<code>1</code>会显示所有核的信息，按下<code>q</code>会退出当前窗口。</p><h3 id="1-3-htop"><a href="#1-3-htop" class="headerlink" title="1.3 htop"></a>1.3 htop</h3><p><code>htop</code>相对<code>top</code>来说，是一个更加清晰的系统资源监控器，不过需要先安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install htop</span><br></pre></td></tr></table></figure><p>同样在命令行执行：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548516682654.png" alt="1548516682654"></p><h2 id="2-利用进程号关闭进程"><a href="#2-利用进程号关闭进程" class="headerlink" title="2.利用进程号关闭进程"></a>2.利用进程号关闭进程</h2><p>指定进程号，如果要关闭其对应进程，只需要执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> 进程号</span><br><span class="line"><span class="built_in">kill</span> -15 进程号</span><br></pre></td></tr></table></figure><p>以上两种方式会关闭正在执行的进程，不过依然会有部分程序会延时相应，或者不相应，所以我们可以强制关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 进程号</span><br></pre></td></tr></table></figure><p>进程号一定要填，不然默认关闭所有进程，包括我们的系统进程。</p><h2 id="3-利用程序名关闭进程"><a href="#3-利用程序名关闭进程" class="headerlink" title="3.利用程序名关闭进程"></a>3.利用程序名关闭进程</h2><h3 id="3-1-直接关闭"><a href="#3-1-直接关闭" class="headerlink" title="3.1 直接关闭"></a>3.1 直接关闭</h3><p>Ubuntu提供了直接关闭进程的指令<code>killall</code>和<code>pkill</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">killall 进程名</span><br><span class="line">pkill 进程名</span><br></pre></td></tr></table></figure><p>不过这两个方法有三个缺点：</p><ul><li><p>进程名指的是执行命令的名称，并不是命令行，即：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test.py =&gt;python</span><br></pre></td></tr></table></figure></li><li><p>进程名如果超过15个字符会自动截断：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">killall bt_uinfo_memcached x</span><br><span class="line">killall bt_uinfo_memcac √</span><br></pre></td></tr></table></figure></li><li><p>对于图形界面等关闭不了，因为属于tty,不属于command。</p></li></ul><h3 id="3-2-先查找再关闭"><a href="#3-2-先查找再关闭" class="headerlink" title="3.2 先查找再关闭"></a>3.2 先查找再关闭</h3><ul><li><p>我们在第一章介绍<code>gnome-system-monitor</code>时，提到其自带查找进程号的功能，即<code>search for open files</code>;</p></li><li><p>Ubuntu提供了一种简单完全匹配工具<code>pidof</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pidof 进程名</span><br></pre></td></tr></table></figure><p>注意这里的进程名必须是完整的名称，如:<code>python</code>。</p></li><li><p>Ubuntu还提供了命令行工具进行查找：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -e | grep 进程名</span><br><span class="line">ps -ef | grep 进程名</span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548518549579.png" alt="1548518549579"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548518423771.png" alt="1548518423771"></p></li></ul><p>我们可以看到，仅仅<code>-e</code>所能匹配的进程不全，但是<code>-ef</code>能匹配更详细的命令行信息，更重要的是这种方式不是完整匹配，而是部分匹配。</p><ul><li><p>对于<code>ps -e</code>，Ubuntu提供了更简洁的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pgrep 进程名</span><br><span class="line">pgrep -a 进程名</span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548518865856.png" alt="1548518865856"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548518904099.png" alt="1548518904099"></p><p><code>pgrep</code>的缺点在于，无法实现<code>ps -ef</code>的命令行匹配功能，其优点在于可以只打印进程号，方便后续关闭。</p></li><li><p>我们如果既想拥有<code>pgrep</code>那样直接打印进程号的简洁，又想拥有<code>ps -ef |grep</code>方式匹配完整命令行的强大，可以利用<code>awk</code>语言编程实现，利用进程号在第二列的特性：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep 进程信息 |awk <span class="string">'&#123;print $2&#125;'</span></span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548519375467.png" alt="1548519375467"></p></li></ul><p>有了这些进程的查找方式，那么关闭进程就很容易了，那么，如果我们想要一次性执行查找和关闭怎么办？可以利用<code>xargs</code>命令，下面为了保证权限问题，都加入了<code>sudo</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pidof python |xargs sudo <span class="built_in">kill</span> -9</span><br><span class="line">pgrep python |xags sudo <span class="built_in">kill</span> -9</span><br><span class="line">ps -ef |grep python |awk <span class="string">'&#123;print $2&#125;'</span>|xargs sudo <span class="built_in">kill</span> -9</span><br></pre></td></tr></table></figure><h2 id="4-利用python关闭进程"><a href="#4-利用python关闭进程" class="headerlink" title="4.利用python关闭进程"></a>4.利用python关闭进程</h2><h3 id="4-1-os系统指令"><a href="#4-1-os系统指令" class="headerlink" title="4.1 os系统指令"></a>4.1 os系统指令</h3><p>python自身可以利用<code>os</code>模块的<code>system</code>功能执行命令行指令，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.system(<span class="string">'kill -9 进程号'</span>)</span><br></pre></td></tr></table></figure><p>另外Python也可以获取自身程序的进程号：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.getpid()</span><br></pre></td></tr></table></figure><h3 id="4-2-psutil库"><a href="#4-2-psutil库" class="headerlink" title="4.2 psutil库"></a>4.2 psutil库</h3><p>psutil库能够轻松实现获取系统运行的进程和系统利用率（包括CPU、内存、磁盘、网络等）信息，它主要用来做系统监控，性能分析，进程管理。其安装方式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install psutil</span><br></pre></td></tr></table></figure><p>这里我不介绍其详细功能，只介绍如何利用其关闭特定进程。</p><ul><li><p>获取当前系统所有进程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psutil.pids()</span><br></pre></td></tr></table></figure><p>返回所有进程号列表。</p></li><li><p>获取每个进程的详细信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psutil.Process(进程号)</span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548520601287.png" alt="1548520601287"></p><p>返回的是该进程所对应的类，通过观察其属性可见，有很多信息可以利用：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548521312890.png" alt="1548521312890"></p></li></ul><p>那么我们可以这样进行匹配，假如我们要关闭包含<code>train</code>字样的python进程，那么可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> psutil.pids():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        p = psutil.Process(i)</span><br><span class="line">        <span class="keyword">if</span> p.name() == <span class="string">'python'</span> <span class="keyword">and</span> <span class="string">'train'</span> <span class="keyword">in</span> <span class="string">''</span>.join(p.cmdline()):</span><br><span class="line">            os.system(<span class="string">'kill -9 '</span>+str(i))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>之所以要加入保护，是因为进程随时都处于启动和关闭状态，有的进程可能在处理过程中关闭了。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;有时候我们需要通过命令行关闭特定进程，对于指定进程号的关闭很容易，但是如果需要关闭指定的程序或者关闭指定路径下的程序则变得不那么容易，下面我们将会对此进行讨论。另外，很多方式都会有权限限制，所以可以适时采用&lt;code&gt;sudo&lt;/code&gt;方式执行。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux探索" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/"/>
    
      <category term="线程/进程管理" scheme="https://huangpiao.tech/categories/Linux%E6%8E%A2%E7%B4%A2/%E7%BA%BF%E7%A8%8B-%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="Ubuntu" scheme="https://huangpiao.tech/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>利用Hexo在多台电脑上提交和更新博客</title>
    <link href="https://huangpiao.tech/2019/01/26/%E5%88%A9%E7%94%A8Hexo%E5%9C%A8%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E4%B8%8A%E6%8F%90%E4%BA%A4%E5%92%8C%E6%9B%B4%E6%96%B0%E5%8D%9A%E5%AE%A2/"/>
    <id>https://huangpiao.tech/2019/01/26/利用Hexo在多台电脑上提交和更新博客/</id>
    <published>2019-01-26T11:10:00.000Z</published>
    <updated>2019-01-26T11:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>考虑到Hexo博客每次的生成和部署都需要环境、依赖资源和所有博客文件，所以如果我们要换一台电脑写博客，则需要配置相同的环境、依赖和所有博客文件，最简单的方法就是用U盘或者其他存储方式拷贝，不过我这里我要讲的是利用Github远程仓库进行更新部署。</p></blockquote><a id="more"></a><h2 id="1-分析依赖"><a href="#1-分析依赖" class="headerlink" title="1. 分析依赖"></a>1. 分析依赖</h2><h3 id="1-1环境依赖"><a href="#1-1环境依赖" class="headerlink" title="1.1环境依赖"></a>1.1环境依赖</h3><p>要想对我们的博客进行正常的<strong>编辑</strong>和<strong>部署</strong>，需要安装以下几个部分：</p><ul><li>Step1下载安装<a href="https://nodejs.org/en/">Node.js</a>和<a href="https://git-scm.com/download/win">Git</a>;</li><li>Step2 配置本地Git环境，由于我们已经注册了相关信息，所以只需要设置好ssh密钥，方法见我之前的博客<a href="https://huangpiao.tech/2019/01/19/Windows%E5%B9%B3%E5%8F%B0%E4%B8%8BGithub%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E6%90%AD%E5%BB%BA/">Windows平台下Github远程仓库的搭建</a>;</li><li>Step3 如果需要部署到Coding，那么还可以继续参照我的这篇博客<a href="https://huangpiao.tech/2019/01/21/%E5%88%A9%E7%94%A8Hexo%E5%B0%86%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2%E5%88%B0GitPages%E5%92%8CCodingPages/">利用Hexo将博客部署到GitPages和CodingPages</a>进行ssh密钥绑定;</li><li>Step4 下载安装<a href="https://www.typora.io/">Typora</a>等Markdown编辑器;</li><li>Step5 下载安装腾讯云对象存储客户端<a href="https://cloud.tencent.com/document/product/436/11366">cosbrowser</a>。</li></ul><h3 id="1-2资源配置依赖"><a href="#1-2资源配置依赖" class="headerlink" title="1.2资源配置依赖"></a>1.2资源配置依赖</h3><p>我们在制作博客网站的过程中安装了很多npm的资源包，这些资源包很多，我们没必要一一拷贝，具体如下：</p><ul><li><p>安装Hexo:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>利用<code>package.json</code>部署资源：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"hexo-site"</span>,</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"0.0.0"</span>,</span><br><span class="line">  <span class="attr">"private"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">"hexo"</span>: &#123;</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.8.0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"eslint"</span>: <span class="string">"4.12.1"</span>,</span><br><span class="line">    <span class="attr">"gulp"</span>: <span class="string">"^4.0.0"</span>,</span><br><span class="line">    <span class="attr">"hexo"</span>: <span class="string">"^3.7.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-deployer-git"</span>: <span class="string">"^1.0.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-archive"</span>: <span class="string">"^0.1.5"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-baidu-sitemap"</span>: <span class="string">"^0.1.6"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-category"</span>: <span class="string">"^0.1.3"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-cname"</span>: <span class="string">"^0.3.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-feed"</span>: <span class="string">"^1.2.2"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-index"</span>: <span class="string">"^0.2.1"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-searchdb"</span>: <span class="string">"^1.0.8"</span>,</span><br><span class="line">    <span class="attr">"hexo-generator-tag"</span>: <span class="string">"^0.2.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-helper-live2d"</span>: <span class="string">"^3.1.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-neat"</span>: <span class="string">"^1.0.4"</span>,</span><br><span class="line">    <span class="attr">"hexo-pdf"</span>: <span class="string">"^1.1.1"</span>,</span><br><span class="line">    <span class="attr">"hexo-renderer-ejs"</span>: <span class="string">"^0.3.1"</span>,</span><br><span class="line">    <span class="attr">"hexo-renderer-markdown-it-plus"</span>: <span class="string">"^1.0.4"</span>,</span><br><span class="line">    <span class="attr">"hexo-renderer-mathjax"</span>: <span class="string">"^0.6.0"</span>,</span><br><span class="line">    <span class="attr">"hexo-renderer-stylus"</span>: <span class="string">"^0.3.3"</span>,</span><br><span class="line">    <span class="attr">"hexo-server"</span>: <span class="string">"^0.3.3"</span>,</span><br><span class="line">    <span class="attr">"hexo-wordcount"</span>: <span class="string">"^6.0.1"</span>,</span><br><span class="line">    <span class="attr">"live2d-widget-model-tororo"</span>: <span class="string">"^1.0.5"</span>,</span><br><span class="line">    <span class="attr">"live2d-widget-model-wanko"</span>: <span class="string">"^1.0.5"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"devDependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"babel-cli"</span>: <span class="string">"^6.26.0"</span>,</span><br><span class="line">    <span class="attr">"babel-preset-es2015"</span>: <span class="string">"^6.24.1"</span>,</span><br><span class="line">    <span class="attr">"gulp-babel"</span>: <span class="string">"^8.0.0"</span>,</span><br><span class="line">    <span class="attr">"gulp-htmlclean"</span>: <span class="string">"^2.7.22"</span>,</span><br><span class="line">    <span class="attr">"gulp-htmlmin"</span>: <span class="string">"^5.0.1"</span>,</span><br><span class="line">    <span class="attr">"gulp-minify-css"</span>: <span class="string">"^1.2.4"</span>,</span><br><span class="line">    <span class="attr">"gulp-uglify"</span>: <span class="string">"^3.0.1"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们只需要在同一目录下执行即可自动下载安装所有资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li></ul><h3 id="1-3博客内容依赖"><a href="#1-3博客内容依赖" class="headerlink" title="1.3博客内容依赖"></a>1.3博客内容依赖</h3><p>博客内容依赖包括<code>主题文件</code>和<code>博客文件</code>，因此我们需要的部分就是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">|—— scaffolds</span><br><span class="line">|—— <span class="built_in">source</span></span><br><span class="line">|—— themes</span><br><span class="line">|—— .gitignore</span><br><span class="line">|—— _config.yml</span><br><span class="line">|—— db.json</span><br><span class="line">|—— gulpfile.js</span><br><span class="line">|—— package.json</span><br><span class="line">|—— package-lock.json</span><br></pre></td></tr></table></figure><h2 id="2-部署远程仓库"><a href="#2-部署远程仓库" class="headerlink" title="2.部署远程仓库"></a>2.部署远程仓库</h2><h3 id="2-1同步远程仓库"><a href="#2-1同步远程仓库" class="headerlink" title="2.1同步远程仓库"></a>2.1同步远程仓库</h3><p>部署远程仓库的目的是将本地的博客源文件同步到远程仓库，当然并不是<code>master</code>分支，具体过程如下：</p><ul><li><p>Step1 删除第三方主题下的<code>.git</code>文件夹；</p></li><li><p>Step2 配置<code>.gitignore</code>文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure></li><li><p>Step3 将项目源文件同步到远程仓库对应的<code>hexo</code>分支：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init  //初始化本地仓库</span><br><span class="line">git add -A //添加本地所有文件到仓库        </span><br><span class="line">git commit -m <span class="string">"blog源文件"</span> //添加commit</span><br><span class="line">git branch hexo //添加本地仓库分支hexo</span><br><span class="line">git remote add origin 博客的git地址  //添加远程仓库</span><br><span class="line">git push origin hexo //将本地仓库的源文件推送到远程仓库hexo分支</span><br></pre></td></tr></table></figure><p>如果出现报错信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Repository not found.</span><br><span class="line">fatal: Could not read from remote repository.</span><br></pre></td></tr></table></figure><p>则首先检查ssh配置是否正确，并检查git地址是否一致，利用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p>如果不一致，则通过如下方式改变：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote set-url origin git地址</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-2-隐藏源文件"><a href="#2-2-隐藏源文件" class="headerlink" title="2.2 隐藏源文件"></a>2.2 隐藏源文件</h3><p>如果将原文件同步至分支，则会由于GitPages或者CodingPages的特性，自动变为公开目录，如果其中有一些不希望别人看见的信息，则会变得不方便，所以我们可以新建一个私有仓库进行同步。</p><ul><li><p>Step1 如果之前已经新建了hexo分支，那么可以选择性的删除该分支；</p></li><li><p>Step2 新建一个私有仓库：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190126/1548504467259.png" alt="1548504467259"></p></li><li><p>Step3 更新远程仓库（如果不行的话，更新ssh）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init  //初始化本地仓库</span><br><span class="line">git remote add origin 博客的git地址  //添加远程仓库</span><br><span class="line">git pull origin master</span><br><span class="line">git add . //添加本地所有文件到仓库        </span><br><span class="line">git commit -m <span class="string">"blog源文件"</span> //添加commit</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-更新远程和本地仓库"><a href="#3-更新远程和本地仓库" class="headerlink" title="3.更新远程和本地仓库"></a>3.更新远程和本地仓库</h2><p>对于本地电脑，如果是第一次使用，则输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b &lt;分支&gt; &lt;server&gt; &lt;待存放路径&gt; //&lt;server&gt; 是指在线仓库的地址</span><br><span class="line"><span class="built_in">cd</span> &lt;待存放路径&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>如果不是第一次操作，对于本地仓库的更新，可执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;考虑到Hexo博客每次的生成和部署都需要环境、依赖资源和所有博客文件，所以如果我们要换一台电脑写博客，则需要配置相同的环境、依赖和所有博客文件，最简单的方法就是用U盘或者其他存储方式拷贝，不过我这里我要讲的是利用Github远程仓库进行更新部署。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Hexo" scheme="https://huangpiao.tech/tags/Hexo/"/>
    
      <category term="Github" scheme="https://huangpiao.tech/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客优化之Next主题功能强化</title>
    <link href="https://huangpiao.tech/2019/01/25/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8BNext%E4%B8%BB%E9%A2%98%E5%8A%9F%E8%83%BD%E5%BC%BA%E5%8C%96/"/>
    <id>https://huangpiao.tech/2019/01/25/Hexo博客优化之Next主题功能强化/</id>
    <published>2019-01-24T18:30:00.000Z</published>
    <updated>2019-01-24T18:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>这里是Hexo博客优化的最后一篇了，主要讲的是讲的是功能层面的优化，如：网站加速、评论功能、在线聊天功能、一键分享功能等等。</p></blockquote><a id="more"></a><h2 id="1-添加网站评论功能"><a href="#1-添加网站评论功能" class="headerlink" title="1. 添加网站评论功能"></a>1. 添加网站评论功能</h2><h3 id="1-1来必力"><a href="#1-1来必力" class="headerlink" title="1.1来必力"></a>1.1来必力</h3><p><a href="https://livere.com">来必力</a>是一款韩国的评论软件，先进入注册登录，然后安装免费版本：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349243475.png" alt="1548349243475"></p><p>安装之后获取安装代码里面的<code>uid</code>，放入主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livere_uid:</span> <span class="string">你的uid</span></span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349386333.png" alt="1548349386333"></p><p>进入管理后台可以管理评论：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349437963.png" alt="1548349437963"></p><h3 id="1-2Valine"><a href="#1-2Valine" class="headerlink" title="1.2Valine"></a>1.2Valine</h3><p>Valine是国内的一款极简风格的评论软件，首先进入<a href="https://leancloud.cn/">LeanCloud</a>注册，然后在控制台随便创建一个项目后，获取密钥：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349609105.png" alt="1548349609105"></p><p>然后修改主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version.</span></span><br><span class="line"><span class="attr">  appid:</span>  <span class="string">你的appid</span></span><br><span class="line"><span class="attr">  appkey:</span>  <span class="string">你的appkey</span></span><br><span class="line"><span class="attr">  notify:</span> <span class="literal">false</span> <span class="comment"># 邮件提醒</span></span><br><span class="line"><span class="attr">  verify:</span> <span class="literal">false</span> <span class="comment"># 验证码</span></span><br><span class="line"><span class="attr">  placeholder:</span> <span class="string">ヾﾉ≧∀≦)o</span> <span class="string">来呀！吐槽一番吧！</span> <span class="comment"># 默认输入信息</span></span><br><span class="line"><span class="attr">  avatar:</span> <span class="string">mm</span> <span class="comment"># gravatar style</span></span><br><span class="line"><span class="attr">  guest_info:</span> <span class="string">nick</span> <span class="comment"># 用户可输入的信息，支持：昵称nick,邮箱mail和链接link</span></span><br><span class="line"><span class="attr">  pageSize:</span> <span class="number">10</span> <span class="comment"># pagination size，每页评论数</span></span><br><span class="line"><span class="attr">  visitor:</span> <span class="literal">false</span> <span class="comment"># leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br><span class="line"><span class="attr">  comment_count:</span> <span class="literal">true</span> <span class="comment"># 评论计数</span></span><br></pre></td></tr></table></figure><p>其实在Web设置中可以添加我们的域名，差不多，效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349895518.png" alt="1548349895518"></p><p>其后台在我们的项目-存储-Comments中：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548349972701.png" alt="1548349972701"></p><p>鉴于Valine比较适合我博客主题风格，我还是选择它。</p><h2 id="2-添加网站分享功能"><a href="#2-添加网站分享功能" class="headerlink" title="2.添加网站分享功能"></a>2.添加网站分享功能</h2><p>这里我采用的是<a href="https://github.com/theme-next/theme-next-needmoreshare2">needmoreshare2</a>,首先在<code>themes/next/</code>下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-needmoreshare2 <span class="built_in">source</span>/lib/needsharebutton</span><br></pre></td></tr></table></figure><p>然后配置主题文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">needmoreshare2:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  postbottom:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span> <span class="comment">#是否开启博客分享按钮</span></span><br><span class="line"><span class="attr">    options:</span></span><br><span class="line"><span class="attr">      iconStyle:</span> <span class="string">box</span></span><br><span class="line"><span class="attr">      boxForm:</span> <span class="string">horizontal</span></span><br><span class="line"><span class="attr">      position:</span> <span class="string">bottomCenter</span></span><br><span class="line"><span class="attr">      networks:</span> <span class="string">Weibo,Wechat,Douban,QQZone,Twitter,Facebook</span></span><br><span class="line"><span class="attr">  float:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span> <span class="comment">#网站分享按钮</span></span><br><span class="line"><span class="attr">    options:</span></span><br><span class="line"><span class="attr">      iconStyle:</span> <span class="string">box</span></span><br><span class="line"><span class="attr">      boxForm:</span> <span class="string">horizontal</span></span><br><span class="line"><span class="attr">      position:</span> <span class="string">middleRight</span></span><br><span class="line"><span class="attr">      networks:</span> <span class="string">Weibo,Wechat,Douban,QQZone,Twitter,Facebook</span></span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548348093270.png" alt="1548348093270"></p><h2 id="3-博文压缩"><a href="#3-博文压缩" class="headerlink" title="3.博文压缩"></a>3.博文压缩</h2><p>我们利用Hexo生成的博客文件中存在大量的空格和空行，从而使得博客资源中有很多不必要的内存消耗，使得网站加载变慢，所以可以利用neat进行博文压缩，首先安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-neat --save</span><br></pre></td></tr></table></figure><p>为了在开启<code>hexo-neat</code>的同时，不要将我们的动态配置压缩了，可在站点配置文件中加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hexo-neat</span></span><br><span class="line"><span class="comment"># 博文压缩</span></span><br><span class="line"><span class="attr">neat_enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 压缩html</span></span><br><span class="line"><span class="attr">neat_html:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"><span class="comment"># 压缩css  </span></span><br><span class="line"><span class="attr">neat_css:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/*.min.css'</span></span><br><span class="line"><span class="comment"># 压缩js</span></span><br><span class="line"><span class="attr">neat_js:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  mangle:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  output:</span></span><br><span class="line"><span class="attr">  compress:</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/*.min.js'</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/jquery.fancybox.pack.js'</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/index.js'</span> </span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/clicklove.js'</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'**/fireworks.js'</span></span><br></pre></td></tr></table></figure><p>当然，除此之外还可以用<code>gulp</code>插件进行压缩，先安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install gulp -g</span><br><span class="line">npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save</span><br></pre></td></tr></table></figure><p>然后在blog主目录下添加<code>gulpfile.js</code>文件：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> gulp = <span class="built_in">require</span>(<span class="string">'gulp'</span>);</span><br><span class="line"><span class="keyword">var</span> minifycss = <span class="built_in">require</span>(<span class="string">'gulp-minify-css'</span>);</span><br><span class="line"><span class="keyword">var</span> uglify = <span class="built_in">require</span>(<span class="string">'gulp-uglify'</span>);</span><br><span class="line"><span class="keyword">var</span> htmlmin = <span class="built_in">require</span>(<span class="string">'gulp-htmlmin'</span>);</span><br><span class="line"><span class="keyword">var</span> htmlclean = <span class="built_in">require</span>(<span class="string">'gulp-htmlclean'</span>);</span><br><span class="line"><span class="comment">// 压缩 public 目录 css</span></span><br><span class="line">gulp.task(<span class="string">'minify-css'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.css'</span>)</span><br><span class="line">        .pipe(minifycss())</span><br><span class="line">        .pipe(gulp.dest(<span class="string">'./public'</span>));</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 压缩 public 目录 html</span></span><br><span class="line">gulp.task(<span class="string">'minify-html'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.html'</span>)</span><br><span class="line">    .pipe(htmlclean())</span><br><span class="line">    .pipe(htmlmin(&#123;</span><br><span class="line">         removeComments: <span class="literal">true</span>,</span><br><span class="line">         minifyJS: <span class="literal">true</span>,</span><br><span class="line">         minifyCSS: <span class="literal">true</span>,</span><br><span class="line">         minifyURLs: <span class="literal">true</span>,</span><br><span class="line">    &#125;))</span><br><span class="line">    .pipe(gulp.dest(<span class="string">'./public'</span>))</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 压缩 public/js 目录 js</span></span><br><span class="line">gulp.task(<span class="string">'minify-js'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> gulp.src(<span class="string">'./public/**/*.js'</span>)</span><br><span class="line">        .pipe(uglify())</span><br><span class="line">        .pipe(gulp.dest(<span class="string">'./public'</span>));</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 执行 gulp 命令时执行的任务</span></span><br><span class="line">gulp.task(<span class="string">'default'</span>, [</span><br><span class="line">    <span class="string">'minify-html'</span>,<span class="string">'minify-css'</span>,<span class="string">'minify-js'</span></span><br><span class="line">]);</span><br></pre></td></tr></table></figure><h2 id="4-DaoVoice在线联系"><a href="#4-DaoVoice在线联系" class="headerlink" title="4.DaoVoice在线联系"></a>4.DaoVoice在线联系</h2><p>首先我们在<a href="http://dashboard.daovoice.io/get-started?invite_code=7f3d6e70">DaoVoice</a>上注册一个账号，然后进入应用设置-安装到网站-仅匿名用户：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548344134147.png" alt="1548344816726"></p><p>聊天设置中可以设置聊天窗口样式：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548344816726.png" alt="1548344816726"></p><p>相应地，我们还能设置接受消息方式，既能控制台访问，也能微信或者邮件接受消息：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548345514881.png" alt="1548345514881"></p><h2 id="5-加速鼠标响应"><a href="#5-加速鼠标响应" class="headerlink" title="5.加速鼠标响应"></a>5.加速鼠标响应</h2><p>在themes/next/下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-fastclick <span class="built_in">source</span>/lib/fastclick</span><br></pre></td></tr></table></figure><p>然后设置主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">fastclick</span> <span class="string">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="6-添加cdn加速"><a href="#6-添加cdn加速" class="headerlink" title="6.添加cdn加速"></a>6.添加cdn加速</h2><p>Next主题官方提供了一些软件的CDN加速，我们可以在主题配置文件中设置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">vendors:</span></span><br><span class="line">  <span class="comment"># Internal path prefix. Please do not edit it.</span></span><br><span class="line"><span class="attr">  _internal:</span> <span class="string">lib</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 2.1.3</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># jquery: //cdn.jsdelivr.net/npm/jquery@2/dist/jquery.min.js</span></span><br><span class="line">  <span class="comment"># jquery: //cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js</span></span><br><span class="line"><span class="attr">  jquery:</span> <span class="string">//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 2.1.5</span></span><br><span class="line">  <span class="comment"># See: https://fancyapps.com/fancybox</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># fancybox: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js</span></span><br><span class="line">  <span class="comment"># fancybox: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.js</span></span><br><span class="line">  <span class="comment"># fancybox_css: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css</span></span><br><span class="line">  <span class="comment"># fancybox_css: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.css</span></span><br><span class="line"><span class="attr">  fancybox:</span> <span class="string">//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js</span></span><br><span class="line"><span class="attr">  fancybox_css:</span> <span class="string">//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.6</span></span><br><span class="line">  <span class="comment"># See: https://github.com/ftlabs/fastclick</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># fastclick: //cdn.jsdelivr.net/npm/fastclick@1/lib/fastclick.min.js</span></span><br><span class="line">  <span class="comment"># fastclick: //cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js</span></span><br><span class="line"><span class="attr">  fastclick:</span> <span class="string">//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.9.7</span></span><br><span class="line">  <span class="comment"># See: https://github.com/tuupola/jquery_lazyload</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># lazyload: //cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js</span></span><br><span class="line">  <span class="comment"># lazyload: //cdnjs.cloudflare.com/ajax/libs/jquery_lazyload/1.9.7/jquery.lazyload.min.js</span></span><br><span class="line"><span class="attr">  lazyload:</span> <span class="string">//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.2.1</span></span><br><span class="line">  <span class="comment"># See: http://velocityjs.org</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># velocity: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js</span></span><br><span class="line">  <span class="comment"># velocity: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js</span></span><br><span class="line">  <span class="comment"># velocity_ui: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js</span></span><br><span class="line">  <span class="comment"># velocity_ui: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js</span></span><br><span class="line"><span class="attr">  velocity:</span> <span class="string">//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js</span></span><br><span class="line"><span class="attr">  velocity_ui:</span> <span class="string">//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 0.7.9</span></span><br><span class="line">  <span class="comment"># See: https://faisalman.github.io/ua-parser-js</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># ua_parser: //cdn.jsdelivr.net/npm/ua-parser-js@0/src/ua-parser.min.js</span></span><br><span class="line">  <span class="comment"># ua_parser: //cdnjs.cloudflare.com/ajax/libs/UAParser.js/0.7.9/ua-parser.min.js</span></span><br><span class="line"><span class="attr">  ua_parser:</span> <span class="string">//cdn.jsdelivr.net/ua-parser.js/0.7.10/ua-parser.min.js</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 4.6.2</span></span><br><span class="line">  <span class="comment"># See: https://fontawesome.com</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># fontawesome: //cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css</span></span><br><span class="line">  <span class="comment"># fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css</span></span><br><span class="line"><span class="attr">  fontawesome:</span> <span class="string">//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 2.4.1</span></span><br><span class="line">  <span class="comment"># See: https://www.algolia.com</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># algolia_instant_js: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.js</span></span><br><span class="line">  <span class="comment"># algolia_instant_css: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.css</span></span><br><span class="line"><span class="attr">  algolia_instant_js:</span></span><br><span class="line"><span class="attr">  algolia_instant_css:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.2</span></span><br><span class="line">  <span class="comment"># See: https://github.com/HubSpot/pace</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># pace: //cdn.jsdelivr.net/npm/pace-js@1/pace.min.js</span></span><br><span class="line">  <span class="comment"># pace: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js</span></span><br><span class="line">  <span class="comment"># pace_css: //cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css</span></span><br><span class="line">  <span class="comment"># pace_css: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css</span></span><br><span class="line"><span class="attr">  pace:</span> <span class="string">//cdn.bootcss.com/pace/1.0.2/pace.min.js</span></span><br><span class="line"><span class="attr">  pace_css:</span> <span class="string">//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.min.css</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/theme-next/theme-next-canvas-nest</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># canvas_nest: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js</span></span><br><span class="line">  <span class="comment"># canvas_nest_nomobile: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest-nomobile.min.js</span></span><br><span class="line"><span class="attr">  canvas_nest:</span> <span class="string">//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js</span></span><br><span class="line"><span class="attr">  canvas_nest_nomobile:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/theme-next/theme-next-three</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># three: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js</span></span><br><span class="line">  <span class="comment"># three_waves: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three-waves.min.js</span></span><br><span class="line">  <span class="comment"># canvas_lines: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_lines.min.js</span></span><br><span class="line">  <span class="comment"># canvas_sphere: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_sphere.min.js</span></span><br><span class="line"><span class="attr">  three:</span></span><br><span class="line"><span class="attr">  three_waves:</span></span><br><span class="line"><span class="attr">  canvas_lines:</span></span><br><span class="line"><span class="attr">  canvas_sphere:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/zproo/canvas-ribbon</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># canvas_ribbon: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js</span></span><br><span class="line"><span class="attr">  canvas_ribbon:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 3.3.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/ethantw/Han</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># han: //cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css</span></span><br><span class="line">  <span class="comment"># han: //cdnjs.cloudflare.com/ajax/libs/Han/3.3.0/han.min.css</span></span><br><span class="line"><span class="attr">  han:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 3.3.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/vinta/pangu.js</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># pangu: //cdn.jsdelivr.net/npm/pangu@3/dist/browser/pangu.min.js</span></span><br><span class="line">  <span class="comment"># pangu: //cdnjs.cloudflare.com/ajax/libs/pangu/3.3.0/pangu.min.js</span></span><br><span class="line"><span class="attr">  pangu:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/revir/need-more-share2</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># needmoreshare2_js: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js</span></span><br><span class="line">  <span class="comment"># needmoreshare2_css: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css</span></span><br><span class="line"><span class="attr">  needmoreshare2_js:</span></span><br><span class="line"><span class="attr">  needmoreshare2_css:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.0.0</span></span><br><span class="line">  <span class="comment"># See: https://github.com/theme-next/theme-next-bookmark</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># bookmark: //cdn.jsdelivr.net/gh/theme-next/theme-next-bookmark@1/bookmark.min.js</span></span><br><span class="line"><span class="attr">  bookmark:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Internal version: 1.1</span></span><br><span class="line">  <span class="comment"># See: https://github.com/theme-next/theme-next-reading-progress</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># reading_progress: //cdn.jsdelivr.net/gh/theme-next/theme-next-reading-progress@1/reading_progress.min.js</span></span><br><span class="line"><span class="attr">  reading_progress:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># leancloud-storage</span></span><br><span class="line">  <span class="comment"># See: https://www.npmjs.com/package/leancloud-storage</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># leancloud: //cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js</span></span><br><span class="line"><span class="attr">  leancloud:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># valine</span></span><br><span class="line">  <span class="comment"># See: https://github.com/xCss/Valine</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># valine: //cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js</span></span><br><span class="line">  <span class="comment"># valine: //cdnjs.cloudflare.com/ajax/libs/valine/1.3.4/Valine.min.js</span></span><br><span class="line"><span class="attr">  valine:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># gitalk</span></span><br><span class="line">  <span class="comment"># See: https://github.com/gitalk/gitalk</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># gitalk_js: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js</span></span><br><span class="line">  <span class="comment"># gitalk_css: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css</span></span><br><span class="line"><span class="attr">  gitalk_js:</span></span><br><span class="line"><span class="attr">  gitalk_css:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># js-md5</span></span><br><span class="line">  <span class="comment"># See: https://github.com/emn178/js-md5</span></span><br><span class="line">  <span class="comment"># Example:</span></span><br><span class="line">  <span class="comment"># md5: //cdn.jsdelivr.net/npm/js-md5@0/src/md5.min.js</span></span><br><span class="line"><span class="attr">  md5:</span></span><br></pre></td></tr></table></figure><h2 id="7-添加lazyload"><a href="#7-添加lazyload" class="headerlink" title="7. 添加lazyload"></a>7. 添加lazyload</h2><p><a href="https://github.com/theme-next/theme-next-jquery-lazyload">lazylod</a>可以在用户不查看的时候，不加载相关部分，从而提升网站加载速度，设置方法同上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-jquery-lazyload <span class="built_in">source</span>/lib/jquery_lazyload</span><br></pre></td></tr></table></figure><p>然后配置主题文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lazyload:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="8-网站动态元素延时加载"><a href="#8-网站动态元素延时加载" class="headerlink" title="8.网站动态元素延时加载"></a>8.网站动态元素延时加载</h2><p>我们的网站添加了许多动态元素之后，加载速度会变慢，所以可以先不加载动态元素，等静态元素加载完之后再加载动态元素，这样就加速了网站的登入。可设置主题文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use velocity to animate everything.</span></span><br><span class="line"><span class="attr">motion:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  async:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  transition:</span></span><br><span class="line">    <span class="comment"># Transition variants:</span></span><br><span class="line">    <span class="comment"># fadeIn | fadeOut | flipXIn | flipXOut | flipYIn | flipYOut | flipBounceXIn | flipBounceXOut | flipBounceYIn | flipBounceYOut</span></span><br><span class="line">    <span class="comment"># swoopIn | swoopOut | whirlIn | whirlOut | shrinkIn | shrinkOut | expandIn | expandOut</span></span><br><span class="line">    <span class="comment"># bounceIn | bounceOut | bounceUpIn | bounceUpOut | bounceDownIn | bounceDownOut | bounceLeftIn | bounceLeftOut | bounceRightIn | bounceRightOut</span></span><br><span class="line">    <span class="comment"># slideUpIn | slideUpOut | slideDownIn | slideDownOut | slideLeftIn | slideLeftOut | slideRightIn | slideRightOut</span></span><br><span class="line">    <span class="comment"># slideUpBigIn | slideUpBigOut | slideDownBigIn | slideDownBigOut | slideLeftBigIn | slideLeftBigOut | slideRightBigIn | slideRightBigOut</span></span><br><span class="line">    <span class="comment"># perspectiveUpIn | perspectiveUpOut | perspectiveDownIn | perspectiveDownOut | perspectiveLeftIn | perspectiveLeftOut | perspectiveRightIn | perspectiveRightOut</span></span><br><span class="line"><span class="attr">    post_block:</span> <span class="string">fadeIn</span></span><br><span class="line"><span class="attr">    post_header:</span> <span class="string">slideDownIn</span></span><br><span class="line"><span class="attr">    post_body:</span> <span class="string">slideDownIn</span></span><br><span class="line"><span class="attr">    coll_header:</span> <span class="string">slideLeftIn</span></span><br><span class="line">    <span class="comment"># Only for Pisces | Gemini.</span></span><br><span class="line"><span class="attr">    sidebar:</span> <span class="string">slideUpIn</span></span><br></pre></td></tr></table></figure><h2 id="9-添加站内搜索"><a href="#9-添加站内搜索" class="headerlink" title="9.添加站内搜索"></a>9.添加站内搜索</h2><p>Next集成了站内搜索功能，可先安装依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure><p>然后设置主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># if auto, trigger search by changing input</span></span><br><span class="line">  <span class="comment"># if manual, trigger search by pressing enter key or search button</span></span><br><span class="line"><span class="attr">  trigger:</span> <span class="string">auto</span></span><br><span class="line">  <span class="comment"># show top n results per article, show all results by setting to -1</span></span><br><span class="line"><span class="attr">  top_n_per_article:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># unescape html strings to the readable one</span></span><br><span class="line"><span class="attr">  unescape:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548347791673.png" alt="1548347791673"></p><h2 id="10-添加百度谷歌收录"><a href="#10-添加百度谷歌收录" class="headerlink" title="10.添加百度谷歌收录"></a>10.添加百度谷歌收录</h2><p>要想让我们的博客被百度、谷歌等搜索引擎索引到，需要提交我们的域名，谷歌很快就能收录，但是百度要一两个月，具体步骤如下：</p><ul><li><p>在百度搜搜引擎中查看自己域名是否被收录:<code>site:huangpiao.tech</code></p></li><li><p>然后点击<code>提交网址</code>，并在百度站长中提交申请，并验证网站：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548352288610.png" alt="1548352288610"></p><p>我选择将验证文件放入<code>blog/source</code>中,然后进行部署，为了防止渲染造成的文件失效，需要在这个验证文件上面加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">layout:</span> <span class="literal">false</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure></li><li><p>验证痛过之后，在我们的博客主目录下载安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure><p>当我们在此加载博客会在public目录生成<code>sitemap.xml</code>和<code>baidusitemap.xml</code></p></li><li><p>修改博客站点配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动生成sitemap</span></span><br><span class="line"><span class="attr">sitemap:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">sitemap.xml</span></span><br><span class="line"><span class="attr">baidusitemap:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">baidusitemap.xml</span></span><br></pre></td></tr></table></figure></li><li><p>修改博客主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEO</span></span><br><span class="line"><span class="attr">baidu_push:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="11-添加公式编辑功能"><a href="#11-添加公式编辑功能" class="headerlink" title="11.添加公式编辑功能"></a>11.添加公式编辑功能</h2><p>Next6主题集成了mathjax和katex两种公式编辑功能,其中后者渲染速度比前者快很多，只不过支持的功能少一点。</p><ul><li><p>mathjax:</p><p>在博客主目录执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-marked --save</span><br><span class="line">npm i hexo-renderer-kramed --save <span class="comment"># 或者 hexo-renderer-pandoc</span></span><br></pre></td></tr></table></figure><p>修改<code>node_modules\kramed\lib\rules\inline.js</code>中对应地方：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>然后修改主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">cdn:</span> <span class="string">//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line"><span class="attr">mhchem:</span> <span class="string">//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0</span></span><br></pre></td></tr></table></figure><p>在博客内容中，添加<code>mathjax = true</code>。</p></li><li><p>katex:</p><p>在博客主目录执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-marked --save</span><br><span class="line">npm i hexo-renderer-markdown-it-plus --save</span><br></pre></td></tr></table></figure><p>然后修改主题配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">math:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">#engine: mathjax</span></span><br><span class="line">  engine: katex</span><br></pre></td></tr></table></figure></li></ul><p>具体问题可以查看<a href="https://github.com/theme-next/hexo-theme-next/blob/5cb74444ee9c89d8cc414daed4453d09a28d67d9/docs/zh-CN/MATH.md">官方文档</a>，至少我这里没有生效:sob:</p><h2 id="12-添加流程图支持"><a href="#12-添加流程图支持" class="headerlink" title="12.添加流程图支持"></a>12.添加流程图支持</h2><p>对于流程图<code>flowchart</code>或者更好的<code>mermaid</code>可以先下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-flowchart</span><br><span class="line">npm install hexo-filter-mermaid-diagrams </span><br><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><p>然后在站点配置文件中加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">flowchart:</span></span><br><span class="line">  <span class="comment"># raphael:   # optional, the source url of raphael.js</span></span><br><span class="line">  <span class="comment"># flowchart: # optional, the source url of flowchart.js</span></span><br><span class="line"><span class="attr">  options:</span> <span class="comment"># options used for `drawSVG`</span></span><br><span class="line"><span class="comment"># mermaid chart</span></span><br><span class="line"><span class="attr">mermaid:</span> <span class="comment">## mermaid url https://github.com/knsv/mermaid</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span>  <span class="comment"># default true</span></span><br><span class="line"><span class="attr">  version:</span> <span class="string">"7.1.2"</span> <span class="comment"># default v7.1.2</span></span><br><span class="line"><span class="attr">  options:</span>  </span><br><span class="line"><span class="attr">external_link:</span> <span class="literal">false</span> <span class="comment">#这个已经有了，修改即可</span></span><br></pre></td></tr></table></figure><p>对于mermaid，则需要在<code>themes/next/layout/_partial/footer.swig</code>中加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.mermaid.enable %&#125;</span><br><span class="line">  &lt;script src=&apos;https://unpkg.com/mermaid@&#123;&#123; theme.mermaid.version &#125;&#125;/dist/mermaid.min.js&apos;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">    if (window.mermaid) &#123;</span><br><span class="line">      mermaid.initialize(&#123;theme: &apos;forest&apos;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><h2 id="13-增加文章置顶功能"><a href="#13-增加文章置顶功能" class="headerlink" title="13.增加文章置顶功能"></a>13.增加文章置顶功能</h2><p>修改 <code>hero-generator-index</code> 插件，把文件：<code>node_modules/hexo-generator-index/lib/generator.js</code> 内的代码替换为：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">'use strict'</span>;</span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">'hexo-pagination'</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> config = <span class="keyword">this</span>.config;</span><br><span class="line">  <span class="keyword">var</span> posts = locals.posts;</span><br><span class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></span><br><span class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></span><br><span class="line">    &#125;);</span><br><span class="line">  <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">'page'</span>;</span><br><span class="line">  <span class="keyword">return</span> pagination(<span class="string">''</span>, posts, &#123;</span><br><span class="line">    perPage: config.index_generator.per_page,</span><br><span class="line">    layout: [<span class="string">'index'</span>, <span class="string">'archive'</span>],</span><br><span class="line">    format: paginationDir + <span class="string">'/%d/'</span>,</span><br><span class="line">    data: &#123;</span><br><span class="line">      __index: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在文章中添加 <code>top</code> 值，数值越大文章越靠前，如</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 解决Charles乱码问题</span><br><span class="line">date: 2017-05-22 22:45:48</span><br><span class="line">tags: 技巧</span><br><span class="line">categories: 技巧</span><br><span class="line">copyright: true</span><br><span class="line">top: 100</span><br><span class="line">---</span><br></pre></td></tr></table></figure><hr><h2 id="14-参考链接"><a href="#14-参考链接" class="headerlink" title="14.参考链接"></a>14.参考链接</h2><ol><li><a href="https://hoxis.github.io/hexo-next-daovoice.html">https://hoxis.github.io/hexo-next-daovoice.html</a></li><li><a href="https://blog.csdn.net/blue_zy/article/details/79071414">https://blog.csdn.net/blue_zy/article/details/79071414</a></li><li><a href="https://www.jianshu.com/p/61abc6c43220">https://www.jianshu.com/p/61abc6c43220</a></li><li><a href="https://blog.csdn.net/blue_zy/article/details/79071414">https://blog.csdn.net/blue_zy/article/details/79071414</a></li><li><a href="https://blog.csdn.net/lvonve/article/details/80200348">https://blog.csdn.net/lvonve/article/details/80200348</a></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;这里是Hexo博客优化的最后一篇了，主要讲的是讲的是功能层面的优化，如：网站加速、评论功能、在线聊天功能、一键分享功能等等。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Next" scheme="https://huangpiao.tech/tags/Next/"/>
    
      <category term="Hexo" scheme="https://huangpiao.tech/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客优化之Next主题美化</title>
    <link href="https://huangpiao.tech/2019/01/24/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8BNext%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/"/>
    <id>https://huangpiao.tech/2019/01/24/Hexo博客优化之Next主题美化/</id>
    <published>2019-01-24T15:02:00.000Z</published>
    <updated>2019-01-24T15:02:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>有了前面几篇博客的介绍，我们就可以很容易的搭建并编辑我们的博客了，不过既然是属于自己的博客网站，自然也就想让其更加美观，更有意思，所以呢我下面介绍一下Hexo博客的主题美化操作。</p></blockquote><a id="more"></a><h2 id="1-Next主题"><a href="#1-Next主题" class="headerlink" title="1. Next主题"></a>1. Next主题</h2><p>Hexo博客支持很多主题风格，其中<a href="https://github.com/theme-next/hexo-theme-next">Next</a>主题是Github上Star最多的主题，其一直在更新维护，支持非常多的外部插件和功能选项。我目前使用的是Next6.0版本，下面我会介绍基于Next6主题的界面美化手法。</p><h3 id="1-1-Next主题的安装配置"><a href="#1-1-Next主题的安装配置" class="headerlink" title="1.1  Next主题的安装配置"></a>1.1 Next主题的安装配置</h3><p>Next主题的安装方式很简单，只需要在博客主目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>然后设置站点配置文件_config.yml：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure><p>即可将我们的Hexo博客主题替换为Next主题。</p><h3 id="1-2-主题简单配置"><a href="#1-2-主题简单配置" class="headerlink" title="1.2  主题简单配置"></a>1.2 主题简单配置</h3><p>Next主题提供很多方便的功能，我们来一一介绍：</p><ul><li><p><strong>Next主题风格</strong>：</p><p>Next提供了四中主题风格scheme，可以在主题配置文件blog/themes/next/_config.yml文件中进行选择，分别是Muse、Mist、Pisces、Gemini：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548267375540.png" alt="1548267375540"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548267403909.png" alt="1548267403909"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548267428521.png" alt="1548267428521"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548267447923.png" alt="1548267447923"></p><p>这里我选择的是Gemini主题，也就是最后一种样式；</p></li><li><p><strong>Next主题一般配置</strong>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">override：false</span> <span class="comment">#表示是否将主题置为默认样式</span></span><br><span class="line"><span class="attr">cache:</span></span><br><span class="line"><span class="attr">enable:true</span> <span class="comment">#表示添加缓存功能，这样浏览器后续打开我们的博客网站会更快</span></span><br><span class="line"><span class="attr">menu:</span> <span class="comment">#设置博客各个页面的相对路径，默认根路径是blog/source</span></span><br><span class="line"><span class="attr">  home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">home</span></span><br><span class="line"><span class="attr">  about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">user</span></span><br><span class="line"><span class="attr">  tags:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">tags</span></span><br><span class="line"><span class="attr">  categories:</span> <span class="string">/categories/</span> <span class="string">||</span> <span class="string">th</span></span><br><span class="line"><span class="attr">  archives:</span> <span class="string">/archives/</span> <span class="string">||</span> <span class="string">archive</span></span><br><span class="line">  <span class="comment">#schedule: /schedule/ || calendar #日历</span></span><br><span class="line">  <span class="comment">#sitemap: /sitemap.xml || sitemap #站点地图，供搜索引擎爬取</span></span><br><span class="line">  <span class="comment">#commonweal: /404/ || heartbeat # 腾讯公益404</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable/Disable menu icons / item badges.</span></span><br><span class="line"><span class="attr">menu_settings:</span></span><br><span class="line"><span class="attr">  icons:</span> <span class="literal">true</span> <span class="comment"># 是否显示各个页面的图标</span></span><br><span class="line"><span class="attr">  badges:</span> <span class="literal">true</span> <span class="comment"># 是否显示分类/标签/归档页的内容量</span></span><br><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span></span><br></pre></td></tr></table></figure><p>以上是Next最常规的配置，而相应的站点配置blog/_config.yml文件的基本配置为：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">见渊の博客</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">记录生活中的点点滴滴</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">直到这一刻微笑着说话为止，我至少留下了一公升眼泪</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">黄飘</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span> <span class="comment"># 主题语言</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">Asia/Shanghai</span> <span class="comment">#中国的时区，不要乱改城市</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'</span></span><br><span class="line"><span class="attr">url:</span> <span class="attr">https://huangpiao.tech</span> <span class="comment">#绑定域名</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/</span>  <span class="comment">#默认根路径，指向实际的source</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">:year/:month/:day/:title/</span></span><br><span class="line"><span class="attr">permalink_defaults:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory</span></span><br><span class="line"><span class="attr">source_dir:</span> <span class="string">source</span></span><br><span class="line"><span class="attr">public_dir:</span> <span class="string">public</span></span><br><span class="line"><span class="attr">tag_dir:</span> <span class="string">tags</span></span><br><span class="line"><span class="attr">archive_dir:</span> <span class="string">archives</span></span><br><span class="line"><span class="attr">category_dir:</span> <span class="string">categories</span></span><br><span class="line"><span class="attr">code_dir:</span> <span class="string">downloads/code</span></span><br><span class="line"><span class="attr">i18n_dir:</span> <span class="string">:lang</span></span><br><span class="line"><span class="attr">skip_render:</span> <span class="string">README.md</span> <span class="comment"># 部署的时候不包含的文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Writing</span></span><br><span class="line"><span class="attr">new_post_name:</span> <span class="string">:title.md</span> <span class="comment"># 默认的新博文名称</span></span><br><span class="line"><span class="attr">default_layout:</span> <span class="string">post</span> <span class="comment"># 默认布局</span></span><br><span class="line"><span class="attr">titlecase:</span> <span class="literal">false</span> <span class="comment"># Transform title into titlecase</span></span><br><span class="line"><span class="attr">external_link:</span> <span class="literal">true</span> <span class="comment"># Open external links in new tab</span></span><br><span class="line"><span class="attr">filename_case:</span> <span class="number">0</span> <span class="comment">#把博客名称改成小写/大写（1,2）</span></span><br><span class="line"><span class="attr">render_drafts:</span> <span class="literal">false</span> <span class="comment"># 是否显示草稿</span></span><br><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">false</span> <span class="comment">#是否启用资源文件夹（用来存放相对路径图片或文件）</span></span><br><span class="line"><span class="attr">relative_link:</span> <span class="literal">false</span> <span class="comment"># 把链接改为与根目录的相对位址</span></span><br><span class="line"><span class="attr">future:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">highlight:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span> <span class="comment">#是否开启代码高亮</span></span><br><span class="line"><span class="attr">  line_number:</span> <span class="literal">true</span> <span class="comment">#是否增加代码行号</span></span><br><span class="line"><span class="attr">  auto_detect:</span> <span class="literal">true</span> <span class="comment">#自动判断代码语言</span></span><br><span class="line"><span class="attr">  tab_replace:</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Home page setting</span></span><br><span class="line"><span class="comment"># path: Root path for your blogs index page. (default = '')</span></span><br><span class="line"><span class="comment"># per_page: Posts displayed per page. (0 = disable pagination)</span></span><br><span class="line"><span class="comment"># order_by: Posts order. (Order by date descending by default)</span></span><br><span class="line"><span class="attr">index_generator:</span> <span class="comment">#首页博客分布</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">''</span> <span class="comment">#博客的默认路径</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="number">10</span> <span class="comment">#每页博客数量上限</span></span><br><span class="line"><span class="attr">  order_by:</span> <span class="bullet">-date</span> <span class="comment">#博客排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Date / Time format</span></span><br><span class="line"><span class="comment">## Hexo uses Moment.js to parse and display date</span></span><br><span class="line"><span class="comment">## You can customize the date format as defined in</span></span><br><span class="line"><span class="comment">## http://momentjs.com/docs/#/displaying/format/</span></span><br><span class="line"><span class="attr">date_format:</span> <span class="string">YYYY-MM-DD</span> <span class="comment">#博客日期格式</span></span><br><span class="line"><span class="attr">time_format:</span> <span class="attr">HH:mm:ss</span> <span class="comment">#博客时间格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pagination</span></span><br><span class="line"><span class="comment">## Set per_page to 0 to disable pagination</span></span><br><span class="line"><span class="attr">per_page:</span> <span class="number">10</span> <span class="comment">#同上</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#归档页的分页设置</span></span><br><span class="line"><span class="attr">archive_generator:</span> <span class="comment">#归档页的配置</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="number">30</span> <span class="comment">#归档页每页博客数</span></span><br><span class="line"><span class="attr">  yearly:</span> <span class="literal">true</span> <span class="comment">#按年归档</span></span><br><span class="line"><span class="attr">  monthly:</span> <span class="literal">true</span> <span class="comment">#按月归档</span></span><br><span class="line"><span class="comment">#标签页的分页设置</span></span><br><span class="line"><span class="attr">tag_generator:</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="number">20</span> <span class="comment">#标签页每页博客数</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">theme:</span> <span class="string">next6</span> <span class="comment">#选择博客主题，名字为themes中选择的主题文件夹名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span> <span class="comment">#博客部署</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">git</span></span><br><span class="line"><span class="attr">  repo:</span> </span><br><span class="line"><span class="attr">    github:</span> <span class="attr">https://github.com/nightmaredimple/nightmaredimple.github.io.git</span></span><br><span class="line"><span class="attr">    coding:</span> <span class="attr">https://git.coding.net/nightmaredimple/nightmaredimple.git</span></span><br><span class="line"><span class="attr">  branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>以上的效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548268494890.png" alt="1548268494890"></p></li></ul><h2 id="2-添加博客自定义图标"><a href="#2-添加博客自定义图标" class="headerlink" title="2.添加博客自定义图标"></a>2.添加博客自定义图标</h2><p>我们博客的默认图标是<code>H</code>，不过Next支持修改图标，下面是我的图标：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548268092825.png" alt="1548268092825"></p><p>博客网站的图标可以在<a href="https://www.easyicon.net/">easyicon</a>、<a href="http://www.bitbug.net/">bitbug</a>、<a href="https://www.iconfont.cn/plus/user/detail?uid=41718">iconfont</a>等网站选择和制作，然后选择或者创建相应大小的图标文件，放置在blog/themes/next/sources/images目录下，并在主题配置文件中进行如下配置，只需要设置small和medium两个就可以：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">favicon:</span></span><br><span class="line"><span class="attr">  small:</span> <span class="string">/images/16x16.png</span></span><br><span class="line"><span class="attr">  medium:</span> <span class="string">/images/32x32.png</span></span><br><span class="line"><span class="attr">  apple_touch_icon:</span> <span class="string">/images/128x128.png</span></span><br><span class="line"><span class="attr">  safari_pinned_tab:</span> <span class="string">/images/logo2.svg</span></span><br></pre></td></tr></table></figure><h2 id="3-鼠标点击特效"><a href="#3-鼠标点击特效" class="headerlink" title="3. 鼠标点击特效"></a>3. 鼠标点击特效</h2><p>鼠标的点击红心特效如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/5308475-78e64c0a80bb559e.gif" alt="img"></p><p>具体步骤如下：</p><p>在<code>/themes/next/source/js/src</code>下新建文件 clicklove.js ，接着把下面的代码拷贝粘贴到 clicklove.js 文件中：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!<span class="function"><span class="keyword">function</span>(<span class="params">e,t,a</span>)</span>&#123;<span class="function"><span class="keyword">function</span> <span class="title">n</span>(<span class="params"></span>)</span>&#123;c(<span class="string">".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"</span>),o(),r()&#125;<span class="function"><span class="keyword">function</span> <span class="title">r</span>(<span class="params"></span>)</span>&#123;<span class="keyword">for</span>(<span class="keyword">var</span> e=<span class="number">0</span>;e&lt;d.length;e++)d[e].alpha&lt;=<span class="number">0</span>?(t.body.removeChild(d[e].el),d.splice(e,<span class="number">1</span>)):(d[e].y--,d[e].scale+=<span class="number">.004</span>,d[e].alpha-=<span class="number">.013</span>,d[e].el.style.cssText=<span class="string">"left:"</span>+d[e].x+<span class="string">"px;top:"</span>+d[e].y+<span class="string">"px;opacity:"</span>+d[e].alpha+<span class="string">";transform:scale("</span>+d[e].scale+<span class="string">","</span>+d[e].scale+<span class="string">") rotate(45deg);background:"</span>+d[e].color+<span class="string">";z-index:99999"</span>);requestAnimationFrame(r)&#125;<span class="function"><span class="keyword">function</span> <span class="title">o</span>(<span class="params"></span>)</span>&#123;<span class="keyword">var</span> t=<span class="string">"function"</span>==<span class="keyword">typeof</span> e.onclick&amp;&amp;e.onclick;e.onclick=<span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;t&amp;&amp;t(),i(e)&#125;&#125;<span class="function"><span class="keyword">function</span> <span class="title">i</span>(<span class="params">e</span>)</span>&#123;<span class="keyword">var</span> a=t.createElement(<span class="string">"div"</span>);a.className=<span class="string">"heart"</span>,d.push(&#123;<span class="attr">el</span>:a,<span class="attr">x</span>:e.clientX<span class="number">-5</span>,<span class="attr">y</span>:e.clientY<span class="number">-5</span>,<span class="attr">scale</span>:<span class="number">1</span>,<span class="attr">alpha</span>:<span class="number">1</span>,<span class="attr">color</span>:s()&#125;),t.body.appendChild(a)&#125;<span class="function"><span class="keyword">function</span> <span class="title">c</span>(<span class="params">e</span>)</span>&#123;<span class="keyword">var</span> a=t.createElement(<span class="string">"style"</span>);a.type=<span class="string">"text/css"</span>;<span class="keyword">try</span>&#123;a.appendChild(t.createTextNode(e))&#125;<span class="keyword">catch</span>(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(<span class="string">"head"</span>)[<span class="number">0</span>].appendChild(a)&#125;<span class="function"><span class="keyword">function</span> <span class="title">s</span>(<span class="params"></span>)</span>&#123;<span class="keyword">return</span><span class="string">"rgb("</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">","</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">","</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">")"</span>&#125;<span class="keyword">var</span> d=[];e.requestAnimationFrame=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="keyword">return</span> e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||<span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;setTimeout(e,<span class="number">1e3</span>/<span class="number">60</span>)&#125;&#125;(),n()&#125;(<span class="built_in">window</span>,<span class="built_in">document</span>);</span><br></pre></td></tr></table></figure><p>在<code>\themes\next\layout\_layout.swig</code>文件末尾添加：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&lt;!--</span> <span class="string">页面点击小红心</span> <span class="bullet">--&gt;</span></span><br><span class="line"><span class="string">&lt;script</span> <span class="string">type="text/javascript"</span> <span class="string">src="/js/src/clicklove.js"&gt;&lt;/script&gt;</span></span><br></pre></td></tr></table></figure><p>当然，还有一种特效(只能选一个)：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/2017-12-18-084649.png" alt="img"></p><p>跟那个红心是差不多的，首先在<code>themes/next/source/js/src</code>里面建一个叫<code>fireworks.js</code>的文件，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;use strict&quot;;function updateCoords(e)&#123;pointerX=(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY=e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t=anime.random(0,360)*Math.PI/180,a=anime.random(50,180),n=[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=colors[anime.random(0,colors.length-1)],a.radius=anime.random(16,32),a.endPos=setParticuleDirection(a),a.draw=function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle=a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=&quot;#F00&quot;,a.radius=0.1,a.alpha=0.5,a.lineWidth=6,a.draw=function()&#123;ctx.globalAlpha=a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth=a.lineWidth,ctx.strokeStyle=a.color,ctx.stroke(),ctx.globalAlpha=1&#125;,a&#125;function renderParticule(e)&#123;for(var t=0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a=createCircle(e,t),n=[],i=0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n=this,i=arguments;clearTimeout(a),a=setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl=document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx=canvasEl.getContext(&quot;2d&quot;),numberOfParticules=30,pointerX=0,pointerY=0,tap=&quot;mousedown&quot;,colors=[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize=debounce(function()&#123;canvasEl.width=2*window.innerWidth,canvasEl.height=2*window.innerHeight,canvasEl.style.width=window.innerWidth+&quot;px&quot;,canvasEl.style.height=window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render=anime(&#123;duration:1/0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!==e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!==e.target.id&amp;&amp;&quot;A&quot;!==e.target.nodeName&amp;&amp;&quot;IMG&quot;!==e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX=(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY=e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t=anime.random(0,360)*Math.PI/180,a=anime.random(50,180),n=[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=colors[anime.random(0,colors.length-1)],a.radius=anime.random(16,32),a.endPos=setParticuleDirection(a),a.draw=function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle=a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=&quot;#F00&quot;,a.radius=0.1,a.alpha=0.5,a.lineWidth=6,a.draw=function()&#123;ctx.globalAlpha=a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth=a.lineWidth,ctx.strokeStyle=a.color,ctx.stroke(),ctx.globalAlpha=1&#125;,a&#125;function renderParticule(e)&#123;for(var t=0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a=createCircle(e,t),n=[],i=0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n=this,i=arguments;clearTimeout(a),a=setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl=document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx=canvasEl.getContext(&quot;2d&quot;),numberOfParticules=30,pointerX=0,pointerY=0,tap=&quot;mousedown&quot;,colors=[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize=debounce(function()&#123;canvasEl.width=2*window.innerWidth,canvasEl.height=2*window.innerHeight,canvasEl.style.width=window.innerWidth+&quot;px&quot;,canvasEl.style.height=window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render=anime(&#123;duration:1/0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!==e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!==e.target.id&amp;&amp;&quot;A&quot;!==e.target.nodeName&amp;&amp;&quot;IMG&quot;!==e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;</span><br></pre></td></tr></table></figure><p>打开<code>themes/next/layout/_layout.swig</code>,在<code>&lt;/body&gt;</code>上面写下如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.fireworks %&#125;</span><br><span class="line">   &lt;canvas class=&quot;fireworks&quot; style=&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;/canvas&gt; </span><br><span class="line">   &lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/animejs/2.2.0/anime.min.js&quot;&gt;&lt;/script&gt; </span><br><span class="line">   &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/fireworks.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>打开主题配置文件，在里面最后写下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Fireworks</span><br><span class="line">fireworks: true</span><br></pre></td></tr></table></figure><h2 id="4-添加动态背景"><a href="#4-添加动态背景" class="headerlink" title="4.添加动态背景"></a>4.添加动态背景</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/5308475-ef603580be708882.gif" alt="img"></p><p>上面这种只是其中一种动态背景，新版的Next主题集成了该功能，只需要在主题配置中设置如下即可，下面每个模块只设置其中一个为<code>true</code>，具体效果如何可自己尝试：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Canvas-nest</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-canvas-nest</span></span><br><span class="line"><span class="attr">canvas_nest:</span> <span class="comment"># 网络背景</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  onmobile:</span> <span class="literal">true</span> <span class="comment"># display on mobile or not</span></span><br><span class="line"><span class="attr">  color:</span> <span class="string">'0,0,0'</span> <span class="comment"># RGB values, use ',' to separate</span></span><br><span class="line"><span class="attr">  opacity:</span> <span class="number">0.5</span> <span class="comment"># the opacity of line: 0~1</span></span><br><span class="line"><span class="attr">  zIndex:</span> <span class="bullet">-1</span> <span class="comment"># z-index property of the background</span></span><br><span class="line"><span class="attr">  count:</span> <span class="number">150</span> <span class="comment"># the number of lines</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># JavaScript 3D library.</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-three</span></span><br><span class="line"><span class="comment"># three_waves</span></span><br><span class="line"><span class="attr">three_waves:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># canvas_lines</span></span><br><span class="line"><span class="attr">canvas_lines:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># canvas_sphere</span></span><br><span class="line"><span class="attr">canvas_sphere:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Canvas-ribbon</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-canvas-ribbon</span></span><br><span class="line"><span class="comment"># size: The width of the ribbon.</span></span><br><span class="line"><span class="comment"># alpha: The transparency of the ribbon.</span></span><br><span class="line"><span class="comment"># zIndex: The display level of the ribbon.</span></span><br><span class="line"><span class="attr">canvas_ribbon:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  size:</span> <span class="number">300</span></span><br><span class="line"><span class="attr">  alpha:</span> <span class="number">0.6</span></span><br><span class="line"><span class="attr">  zIndex:</span> <span class="bullet">-1</span></span><br></pre></td></tr></table></figure><p>另外需要在blog中下载相应资源包，具体见上面的链接，下面我给出canvas_nest的下载方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-canvas-nest themes/next/<span class="built_in">source</span>/lib/canvas-nest</span><br></pre></td></tr></table></figure><h2 id="5-修改标签样式"><a href="#5-修改标签样式" class="headerlink" title="5. 修改标签样式"></a>5. 修改标签样式</h2><p>博客底部的标签样式默认为<code>#tag</code>，我们可以将其改成：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548273942824.png" alt="1548273942824"></p><p>只需要修改模板<code>/themes/next/layout/_macro/post.swig</code>，搜索 <code>rel=&quot;tag&quot;&gt;#</code>，将 # 换成<code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code></p><h2 id="6-作者头像设置"><a href="#6-作者头像设置" class="headerlink" title="6. 作者头像设置"></a>6. 作者头像设置</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548274016625.png" alt="1548274016625"></p><p>可以设置当鼠标放置在头像上时，头像自动旋转，具体设置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># in theme directory(source/images): /images/avatar.gif</span></span><br><span class="line">  <span class="comment"># in site  directory(source/uploads): /uploads/avatar.gif</span></span><br><span class="line">  <span class="comment"># You can also use other linking images.</span></span><br><span class="line"><span class="attr">  url:</span> <span class="string">/images/author.jpg</span>  <span class="comment">#将我们的头像图片放置在blog/themes/next/source/images目录下，填写具体地址</span></span><br><span class="line">  <span class="comment"># If true, the avatar would be dispalyed in circle.</span></span><br><span class="line"><span class="attr">  rounded:</span> <span class="literal">true</span> <span class="comment">#鼠标放在头像上时是否旋转</span></span><br><span class="line">  <span class="comment"># The value of opacity should be choose from 0 to 1 to set the opacity of the avatar.</span></span><br><span class="line"><span class="attr">  opacity:</span> <span class="number">1</span> <span class="comment">#头像放缩指数</span></span><br><span class="line">  <span class="comment"># If true, the avatar would be rotated with the cursor.</span></span><br><span class="line"><span class="attr">  rotated:</span> <span class="literal">true</span> <span class="comment">#头像是否设为圆形，否则为矩形</span></span><br></pre></td></tr></table></figure><h2 id="7-文章结束标志"><a href="#7-文章结束标志" class="headerlink" title="7.文章结束标志"></a>7.文章结束标志</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548274181405.png" alt="1548274181405"></p><p>在路径 <code>\themes\next\layout\_macro</code> 中新建 <code>passage-end-tag.swig</code> 文件,并添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>接着打开<code>\themes\next\layout\_macro\post.swig</code>文件，在<code>post-body</code> 之后(<code>END POST BODY</code>)， <code>post-footer</code> 之前添加如代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include 'passage-end-tag.swig' %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后打开主题配置文件（<code>_config.yml</code>),在末尾添加：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文章末尾添加“本文结束”标记</span></span><br><span class="line"><span class="attr">passage_end_tag:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="8-侧边栏设置"><a href="#8-侧边栏设置" class="headerlink" title="8.侧边栏设置"></a>8.侧边栏设置</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548274885067.png" alt="1548274885067"></p><p>设置主题配置文件，其中<code>social</code>表示社交信息，我们可以填入我们相关的链接，格式为<code>链接名:链接地址 || 链接图标</code>，其中链接图标必须是<a href="http://fontawesome.dashgame.com/">FontAwesome</a>网站中存在的图标名。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Posts / Categories / Tags in sidebar.</span></span><br><span class="line"><span class="attr">site_state:</span> <span class="literal">true</span> <span class="comment"># 是否在侧边栏加入日志、分类、标签等跳转链接</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Social Links</span></span><br><span class="line"><span class="comment"># Usage: `Key: permalink || icon`</span></span><br><span class="line"><span class="comment"># Key is the link label showing to end users.</span></span><br><span class="line"><span class="comment"># Value before `||` delimeter is the target permalink.</span></span><br><span class="line"><span class="comment"># Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.</span></span><br><span class="line"><span class="attr">social:</span></span><br><span class="line"><span class="attr">  GitHub:</span> <span class="attr">https://github.com/nightmaredimple</span> <span class="string">||</span> <span class="string">github</span> <span class="comment">#</span></span><br><span class="line"><span class="attr">  CSDN:</span> <span class="attr">https://blog.csdn.net/nightmare_dimple</span> <span class="string">||</span> <span class="string">crosshairs</span></span><br><span class="line">  <span class="comment">#E-Mail: mailto:yourname@gmail.com || envelope</span></span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || facebook</span></span><br><span class="line">  <span class="comment">#VK Group: https://vk.com/yourname || vk</span></span><br><span class="line">  <span class="comment">#StackOverflow: https://stackoverflow.com/yourname || stack-overflow</span></span><br><span class="line">  <span class="comment">#YouTube: https://youtube.com/yourname || youtube</span></span><br><span class="line">  <span class="comment">#Instagram: https://instagram.com/yourname || instagram</span></span><br><span class="line">  <span class="comment">#Skype: skype:yourname?call|chat || skype</span></span><br><span class="line"></span><br><span class="line"><span class="attr">social_icons:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span> <span class="comment">#是否显示社交图标</span></span><br><span class="line"><span class="attr">  icons_only:</span> <span class="literal">false</span> <span class="comment">#是否仅显示社交图标</span></span><br><span class="line"><span class="attr">  transition:</span> <span class="literal">true</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Follow me on GitHub banner in right-top corner.</span></span><br><span class="line"><span class="comment"># Usage: `permalink || title`</span></span><br><span class="line"><span class="comment"># Value before `||` delimeter is the target permalink.</span></span><br><span class="line"><span class="comment"># Value after `||` delimeter is the title and aria-label name.</span></span><br><span class="line"><span class="attr">github_banner:</span> <span class="attr">https://github.com/nightmaredimple</span> <span class="string">||</span> <span class="string">Follow</span> <span class="string">me</span> <span class="string">on</span> <span class="string">GitHub</span> <span class="comment">#添加右上角github绑带</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Blog rolls</span></span><br><span class="line"><span class="attr">links_icon:</span> <span class="string">link</span></span><br><span class="line"><span class="attr">links_title:</span> <span class="string">Links</span></span><br><span class="line"><span class="attr">links_layout:</span> <span class="string">block</span></span><br><span class="line"><span class="comment">#links_layout: inline</span></span><br><span class="line"><span class="comment">#links:</span></span><br><span class="line">  <span class="comment">#Title: http://example.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sidebar Avatar</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="comment">#头像设置</span></span><br><span class="line">  <span class="comment"># in theme directory(source/images): /images/avatar.gif</span></span><br><span class="line">  <span class="comment"># in site  directory(source/uploads): /uploads/avatar.gif</span></span><br><span class="line">  <span class="comment"># You can also use other linking images.</span></span><br><span class="line"><span class="attr">  url:</span> <span class="string">/images/author.jpg</span></span><br><span class="line">  <span class="comment"># If true, the avatar would be dispalyed in circle.</span></span><br><span class="line"><span class="attr">  rounded:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># The value of opacity should be choose from 0 to 1 to set the opacity of the avatar.</span></span><br><span class="line"><span class="attr">  opacity:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># If true, the avatar would be rotated with the cursor.</span></span><br><span class="line"><span class="attr">  rotated:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Table Of Contents in the Sidebar</span></span><br><span class="line"><span class="attr">toc:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span> <span class="comment">#是否自动生成目录</span></span><br><span class="line">  <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line"><span class="attr">  number:</span> <span class="literal">false</span> <span class="comment">#目录是否自动产生编号</span></span><br><span class="line">  <span class="comment"># If true, all words will placed on next lines if header width longer then sidebar width.</span></span><br><span class="line"><span class="attr">  wrap:</span> <span class="literal">false</span> <span class="comment">#标题过长是否换行</span></span><br><span class="line">  <span class="comment"># Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` var.</span></span><br><span class="line"><span class="attr">  max_depth:</span> <span class="number">6</span> <span class="comment">#最大标题深度</span></span><br><span class="line"></span><br><span class="line"><span class="attr">sidebar:</span></span><br><span class="line">  <span class="comment"># Sidebar Position, available values: left | right (only for Pisces | Gemini).</span></span><br><span class="line"><span class="attr">  position:</span> <span class="string">left</span> <span class="comment">#侧边栏位置</span></span><br><span class="line">  <span class="comment">#position: right</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Manual define the sidebar width.</span></span><br><span class="line">  <span class="comment"># If commented, will be default for:</span></span><br><span class="line">  <span class="comment"># Muse | Mist: 320</span></span><br><span class="line">  <span class="comment"># Pisces | Gemini: 240</span></span><br><span class="line">  <span class="comment">#width: 300</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Sidebar Display, available values (only for Muse | Mist):</span></span><br><span class="line">  <span class="comment">#  - post    expand on posts automatically. Default.</span></span><br><span class="line">  <span class="comment">#  - always  expand for all pages automatically</span></span><br><span class="line">  <span class="comment">#  - hide    expand only when click on the sidebar toggle icon.</span></span><br><span class="line">  <span class="comment">#  - remove  Totally remove sidebar including sidebar toggle.</span></span><br><span class="line"><span class="attr">  display:</span> <span class="string">post</span></span><br><span class="line">  <span class="comment">#display: always</span></span><br><span class="line">  <span class="comment">#display: hide</span></span><br><span class="line">  <span class="comment">#display: remove</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Sidebar offset from top menubar in pixels (only for Pisces | Gemini).</span></span><br><span class="line"><span class="attr">  offset:</span> <span class="number">12</span> <span class="comment">#侧边栏相对主菜单像素距离</span></span><br><span class="line">  <span class="comment"># Back to top in sidebar.</span></span><br><span class="line"><span class="attr">  b2t:</span> <span class="literal">true</span> <span class="comment">#是否提供一键置顶</span></span><br><span class="line">  <span class="comment"># Scroll percent label in b2t button.</span></span><br><span class="line"><span class="attr">  scrollpercent:</span> <span class="literal">true</span> <span class="comment">#是否显示当前阅读进度</span></span><br><span class="line">  <span class="comment"># Enable sidebar on narrow view (only for Muse | Mist).</span></span><br><span class="line"><span class="attr">  onmobile:</span> <span class="literal">false</span> <span class="comment">#手机上是否显示侧边栏</span></span><br></pre></td></tr></table></figure><h2 id="9-文章阴影设置"><a href="#9-文章阴影设置" class="headerlink" title="9.文章阴影设置"></a>9.文章阴影设置</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/5308475-21046c442900bf3f.png" alt="img"></p><p>打开<code>\themes\next\source\css\_custom\custom.styl</code>,向里面加入：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 主页文章添加阴影效果</span><br><span class="line"> <span class="selector-class">.post</span> &#123;</span><br><span class="line">   <span class="attribute">margin-top</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">margin-bottom</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">padding</span>: <span class="number">25px</span>;</span><br><span class="line">   <span class="attribute">-webkit-box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">5px</span> <span class="built_in">rgba</span>(202, 203, 203, .5);</span><br><span class="line">   <span class="attribute">-moz-box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">5px</span> <span class="built_in">rgba</span>(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><strong>这个功能会影响排版，我已经取消了。</strong></p><h2 id="10-添加文章版权信息"><a href="#10-添加文章版权信息" class="headerlink" title="10. 添加文章版权信息"></a>10. 添加文章版权信息</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548336818289.png" alt="1548336818289"></p><p>要想开启博客的版权功能，需要设置主题配置文件：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">creative_commons:</span></span><br><span class="line"><span class="attr">  license:</span> <span class="string">by-nc-sa</span></span><br><span class="line"><span class="attr">  sidebar:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  post:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="11-设置博客底部布局"><a href="#11-设置博客底部布局" class="headerlink" title="11.设置博客底部布局"></a>11.设置博客底部布局</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548338104016.png" alt="1548338104016"></p><p>这一部分对应主题配置文件中的：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup.</span></span><br><span class="line">  <span class="comment"># If not defined, current year will be used.</span></span><br><span class="line"><span class="attr">  since:</span> <span class="number">2019</span> <span class="comment">#建站时间</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line"><span class="attr">  icon:</span></span><br><span class="line">    <span class="comment"># Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons/</span></span><br><span class="line">    <span class="comment"># `heart` is recommended with animation in red (#ff0000).</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">heart</span>  <span class="comment">#作者图标（默认是author人像)</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line"><span class="attr">    animated:</span> <span class="literal">true</span> <span class="comment">#图标是否闪动</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line"><span class="attr">    color:</span> <span class="string">"#808080"</span> <span class="comment">#图标颜色</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># If not defined, `author` from Hexo main config will be used.</span></span><br><span class="line"><span class="attr">  copyright:</span> <span class="string">黄飘</span> <span class="comment">#别填bool型，最后显示的东西是copyright || author，即左边没有设置的话就显示作者</span></span><br><span class="line">  <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line"><span class="attr">  powered:</span></span><br><span class="line">    <span class="comment"># Hexo link (Powered by Hexo).</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">false</span> <span class="comment">#是否显示 Powered by hexo</span></span><br><span class="line">    <span class="comment"># Version info of Hexo after Hexo link (vX.X.X).</span></span><br><span class="line"><span class="attr">    version:</span> <span class="literal">false</span> <span class="comment">#是否显示Hexo版本</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  theme:</span></span><br><span class="line">    <span class="comment"># Theme &amp; scheme info link (Theme - NexT.scheme).</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">false</span> <span class="comment">#是否显示主题信息</span></span><br><span class="line">    <span class="comment"># Version info of NexT after scheme info (vX.X.X).</span></span><br><span class="line"><span class="attr">    version:</span> <span class="literal">false</span> <span class="comment">#是否显示主题版本</span></span><br><span class="line">  <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line">  <span class="comment"># Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer.</span></span><br><span class="line">  <span class="comment"># http://www.miitbeian.gov.cn</span></span><br><span class="line"><span class="attr">  beian:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">false</span> <span class="comment">#是否显示网站备案信息</span></span><br><span class="line"><span class="attr">    icp:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line">  <span class="comment"># Any custom text can be defined here.</span></span><br><span class="line">  <span class="comment">#custom_text: Hosted by &lt;a href="https://pages.coding.me" class="theme-link" rel="noopener" target="_blank"&gt;Coding Pages&lt;/a&gt;</span></span><br></pre></td></tr></table></figure><h2 id="12-添加打赏"><a href="#12-添加打赏" class="headerlink" title="12. 添加打赏"></a>12. 添加打赏</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548338764886.png" alt="1548338764886"></p><p>在主题配置文件中设置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">reward:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  comment:</span> <span class="string">坚持原创技术分享，您的支持将鼓励我继续创作！</span></span><br><span class="line"><span class="attr">  wechatpay:</span> <span class="string">/images/wechatpay.jpg</span></span><br><span class="line"><span class="attr">  alipay:</span> <span class="string">/images/alipay.jpg</span></span><br><span class="line">  <span class="comment">#bitcoin: /images/bitcoin.jpg</span></span><br></pre></td></tr></table></figure><p>自己获取自己的支付收款码，放置在next/source/images中</p><h2 id="13-添加页面宠物"><a href="#13-添加页面宠物" class="headerlink" title="13. 添加页面宠物"></a>13. 添加页面宠物</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548338832593.png" alt="1548338832593"></p><p>首先在博客目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -save hexo-helper-live2d</span><br></pre></td></tr></table></figure><p>然后在<strong>站点</strong>配置文件中加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">live2d:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  scriptFrom:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">  pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line"><span class="attr">  pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line"><span class="attr">  pluginModelPath:</span> <span class="string">assets/</span></span><br><span class="line"><span class="attr">  tagMode:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  model:</span></span><br><span class="line"><span class="attr">    use:</span> <span class="string">live2d-widget-model-wanko</span>  <span class="comment">#选择哪种模型</span></span><br><span class="line"><span class="attr">  display:</span> <span class="comment">#放置位置和大小</span></span><br><span class="line"><span class="attr">    position:</span> <span class="string">right</span></span><br><span class="line"><span class="attr">    width:</span> <span class="number">150</span></span><br><span class="line"><span class="attr">    height:</span> <span class="number">300</span></span><br><span class="line"><span class="attr">  mobile:</span></span><br><span class="line"><span class="attr">    show:</span> <span class="literal">false</span> <span class="comment">#是否在手机端显示</span></span><br></pre></td></tr></table></figure><p>上面模型的选择可在<a href="https://github.com/xiazeyu/live2d-widget-models">lived2d</a>中选择，并下载相应的模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install live2d-widget-model-wanko</span><br></pre></td></tr></table></figure><h2 id="14-设置代码块样式"><a href="#14-设置代码块样式" class="headerlink" title="14.设置代码块样式"></a>14.设置代码块样式</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548340016692.png" alt="1548340016692"></p><p>代码块的行号显示在上面已经介绍了，位于站点配置文件，对于代码块的主题我么还能设置其背景，增加复制按钮等，可修改主题配置文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Code Highlight theme</span></span><br><span class="line"><span class="comment"># Available values: normal | night | night eighties | night blue | night bright</span></span><br><span class="line"><span class="comment"># https://github.com/chriskempson/tomorrow-theme</span></span><br><span class="line"><span class="attr">highlight_theme:</span> <span class="string">night</span></span><br><span class="line"><span class="attr">codeblock:</span></span><br><span class="line">  <span class="comment"># Manual define the border radius in codeblock</span></span><br><span class="line">  <span class="comment"># Leave it empty for the default 1</span></span><br><span class="line"><span class="attr">  border_radius:</span></span><br><span class="line">  <span class="comment"># Add copy button on codeblock</span></span><br><span class="line"><span class="attr">  copy_button:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Show text copy result</span></span><br><span class="line"><span class="attr">    show_result:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="15-设置博客摘要显示"><a href="#15-设置博客摘要显示" class="headerlink" title="15.设置博客摘要显示"></a>15.设置博客摘要显示</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548340037906.png" alt="1548340037906"></p><p>对于摘要显示，首先我们需要开启摘要功能，修改主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Automatically scroll page to section which is under &lt;!-- more --&gt; mark.</span></span><br><span class="line"><span class="attr">scroll_to_more:</span> <span class="literal">true</span> <span class="comment">#选取博客正文&lt;!--more--&gt;前的内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatically saving scroll position on each post/page in cookies.</span></span><br><span class="line"><span class="attr">save_scroll:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatically excerpt description in homepage as preamble text.</span></span><br><span class="line"><span class="attr">excerpt_description:</span> <span class="literal">true</span> <span class="comment">#自动截取摘要</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatically Excerpt. Not recommend.</span></span><br><span class="line"><span class="comment"># Use &lt;!-- more --&gt; in the post to control excerpt accurately.</span></span><br><span class="line"><span class="attr">auto_excerpt:</span> </span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span> <span class="comment">#自动截取一定程度的摘要</span></span><br><span class="line"><span class="attr">  length:</span> <span class="number">150</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read more button</span></span><br><span class="line"><span class="comment"># If true, the read more button would be displayed in excerpt section.</span></span><br><span class="line"><span class="attr">read_more_btn:</span> <span class="literal">true</span> <span class="comment">#显示阅读全文按钮</span></span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548340204881.png" alt="1548340204881"></p><h2 id="16-设置RSS订阅"><a href="#16-设置RSS订阅" class="headerlink" title="16.设置RSS订阅"></a>16.设置RSS订阅</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/5308475-a54daae937107550.png" alt="img"></p><p>在博客主目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure><p>在站点配置文件中修改：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: http://hexo.io/plugins/</span></span><br><span class="line"><span class="attr">plugins:</span> <span class="string">hexo-generate-feed</span></span><br></pre></td></tr></table></figure><p>然后设置主题配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set rss to false to disable feed link.</span></span><br><span class="line"><span class="comment"># Leave rss as empty to use site's feed link.</span></span><br><span class="line"><span class="comment"># Set rss to specific value if you have burned your feed already.</span></span><br><span class="line"><span class="attr">rss:</span> <span class="string">/atom.xml</span></span><br></pre></td></tr></table></figure><h2 id="17-修改文章链接样式"><a href="#17-修改文章链接样式" class="headerlink" title="17.修改文章链接样式"></a>17.修改文章链接样式</h2><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/5308475-8cc4fc18c399af7e.gif" alt="img"></p><p>修改文件 <code>themes\next\source\css\_common\components\post\post.styl</code>，在末尾添加如下css样式，：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span> <span class="string">文章内链接文本样式</span></span><br><span class="line"><span class="string">.post-body</span> <span class="string">p</span> <span class="string">a&#123;</span></span><br><span class="line"><span class="attr">  color:</span> <span class="comment">#0593d3;</span></span><br><span class="line"><span class="attr">  border-bottom:</span> <span class="string">none;</span></span><br><span class="line"><span class="attr">  border-bottom:</span> <span class="number">1</span><span class="string">px</span> <span class="string">solid</span> <span class="comment">#0593d3;</span></span><br><span class="line">  <span class="string">&amp;:hover</span> <span class="string">&#123;</span></span><br><span class="line"><span class="attr">    color:</span> <span class="comment">#fc6423;</span></span><br><span class="line"><span class="attr">    border-bottom:</span> <span class="string">none;</span></span><br><span class="line"><span class="attr">    border-bottom:</span> <span class="number">1</span><span class="string">px</span> <span class="string">solid</span> <span class="comment">#fc6423;</span></span><br><span class="line">  <span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="18-增加阅读次数-时长和访客数"><a href="#18-增加阅读次数-时长和访客数" class="headerlink" title="18.增加阅读次数/时长和访客数"></a>18.增加阅读次数/时长和访客数</h2><p>Next6版本集成了多种相关功能，除了已有的<a href="http://busuanzi.ibruce.info/">busuanzi</a>，目前又加入了<a href="https://github.com/theme-next/hexo-symbols-count-time">symbols-count-time</a>，二者在主题配置文件中的相关设置方法如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line"><span class="attr">  separated_meta:</span> <span class="literal">true</span> <span class="comment">#分隔符|</span></span><br><span class="line"><span class="attr">  item_text_post:</span> <span class="literal">true</span> <span class="comment">#是否统计站点总字数</span></span><br><span class="line"><span class="attr">  item_text_total:</span> <span class="literal">true</span> <span class="comment">#是否同级文章总字数</span></span><br><span class="line"><span class="attr">  awl:</span> <span class="number">2</span> <span class="comment">#平均每个字符的长度</span></span><br><span class="line"><span class="attr">  wpm:</span> <span class="number">300</span> <span class="comment">#words per minute</span></span><br><span class="line"> </span><br><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span>  <span class="comment">#是否开启不蒜子统计功能</span></span><br><span class="line"><span class="attr">  total_visitors:</span> <span class="literal">true</span> <span class="comment">#是否统计总访客数</span></span><br><span class="line"><span class="attr">  total_visitors_icon:</span> <span class="string">user</span> <span class="comment">#访客数图标为人像</span></span><br><span class="line"><span class="attr">  total_views:</span> <span class="literal">true</span> <span class="comment">#是否同级总访问数</span></span><br><span class="line"><span class="attr">  total_views_icon:</span> <span class="string">eye</span> <span class="comment">#访问数图标为眼睛</span></span><br><span class="line"><span class="attr">  post_views:</span> <span class="literal">true</span> <span class="comment">#是否统计文章访问数</span></span><br><span class="line"><span class="attr">  post_views_icon:</span> <span class="string">eye</span> <span class="comment">#访问数图标为眼睛</span></span><br></pre></td></tr></table></figure><p>其中前者还需在站点配置文件中加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line"><span class="attr">  symbols:</span> <span class="literal">true</span> <span class="comment">#是否统计字数</span></span><br><span class="line"><span class="attr">  time:</span> <span class="literal">false</span> <span class="comment">#是否统计阅读时长</span></span><br><span class="line"><span class="attr">  total_symbols:</span> <span class="literal">true</span> <span class="comment">#是否统计总字数</span></span><br><span class="line"><span class="attr">  total_time:</span> <span class="literal">false</span> <span class="comment">#是否统计总阅读时长</span></span><br></pre></td></tr></table></figure><p>相关依赖如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-symbols-count-time --save</span><br></pre></td></tr></table></figure><p>不过symbols-count-time的数字经常不显示，不知道是不是我配置的问题，不过不担心，因为busuanzi自带了这些功能（除了阅读时长，不过这个意义不大），只需要修改<code>next/layout/_partials/footer.swig</code>文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &#123;% if config.symbols_count_time.total_symbols %&#125;</span><br><span class="line">    &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-item-icon&quot;&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-area-chart&quot;&gt;&lt;/i&gt;</span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">    &#123;% if theme.symbols_count_time.item_text_total %&#125;</span><br><span class="line">      &lt;span class=&quot;post-meta-item-text&quot;&gt;&#123;&#123; __(&apos;symbols_count_time.count_total&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/span&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;span title=&quot;&#123;&#123; __(&apos;post.totalcount&apos;) &#125;&#125;&quot;&gt;</span><br><span class="line">     &#123;&#123; totalcount(site, &apos;0,0.0a&apos;) &#125;&#125;字 &lt;/span&gt;</span><br><span class="line">    &lt;!--&lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count_total&apos;) &#125;&#125;&quot;&gt;&#123;&#123;symbolsCountTotal(site)&#125;&#125;&lt;/span&gt;--&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>以及修改<code>next/layout/_macro/post.swig</code>文件中的symbol部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">     &#123;% if config.symbols_count_time.symbols or config.symbols_count_time.time %&#125;</span><br><span class="line">       &lt;div class=&quot;post-symbolscount&quot;&gt;</span><br><span class="line">         &#123;% if not theme.symbols_count_time.separated_meta %&#125;</span><br><span class="line">           &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">         &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">         &#123;% if config.symbols_count_time.symbols %&#125;</span><br><span class="line">           &lt;span class=&quot;post-meta-item-icon&quot;&gt;</span><br><span class="line">             &lt;i class=&quot;fa fa-file-word-o&quot;&gt;&lt;/i&gt;</span><br><span class="line">           &lt;/span&gt;</span><br><span class="line">           &#123;% if theme.symbols_count_time.item_text_post %&#125;</span><br><span class="line">             &lt;span class=&quot;post-meta-item-text&quot;&gt;&#123;&#123; __(&apos;symbols_count_time.count&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/span&gt;</span><br><span class="line">           &#123;% endif %&#125;</span><br><span class="line">           &lt;!-- &lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count&apos;) &#125;&#125;&quot;&gt;&#123;#</span><br><span class="line">           #&#125;&#123;&#123; symbolsCount(post.content) &#125;&#125;&#123;#</span><br><span class="line">         #&#125;&lt;/span&gt; --&gt;</span><br><span class="line">&lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count&apos;) &#125;&#125;&quot;&gt;</span><br><span class="line">             &#123;&#123; wordcount(post.content) &#125;&#125;字</span><br><span class="line">           &lt;/span&gt;</span><br><span class="line">         &#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>最终效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548342001495.png" alt="1548342001495"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190124/1548342014674.png" alt="1548342014674"></p><h2 id="19-加入网易云音乐播放器"><a href="#19-加入网易云音乐播放器" class="headerlink" title="19.加入网易云音乐播放器"></a>19.加入网易云音乐播放器</h2><p>首先在网页搜索网易云音乐，选择音乐，并生成外链：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548351757165.png" alt="1548351757165"></p><p>然后得到外链html代码：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548351853181.png" alt="1548351853181"></p><p>将代码粘贴到一个合适的位置，建议放在侧边栏，在<code>Blog/themes/next/layout/_macro/sidebar.swig</code>文件下，选择位置复制进去，不同位置效果不同：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190125/1548351938639.png" alt="1548351938639"></p><h2 id="20-参考资料"><a href="#20-参考资料" class="headerlink" title="20. 参考资料"></a>20. 参考资料</h2><ol><li><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html">http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html</a></li><li><a href="https://www.jianshu.com/p/1ff2fcbdd155">https://www.jianshu.com/p/1ff2fcbdd155</a></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;有了前面几篇博客的介绍，我们就可以很容易的搭建并编辑我们的博客了，不过既然是属于自己的博客网站，自然也就想让其更加美观，更有意思，所以呢我下面介绍一下Hexo博客的主题美化操作。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Next" scheme="https://huangpiao.tech/tags/Next/"/>
    
      <category term="Hexo" scheme="https://huangpiao.tech/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客优化之内容编辑</title>
    <link href="https://huangpiao.tech/2019/01/22/Hexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8B%E5%86%85%E5%AE%B9%E7%BC%96%E8%BE%91/"/>
    <id>https://huangpiao.tech/2019/01/22/Hexo博客优化之内容编辑/</id>
    <published>2019-01-22T15:50:00.000Z</published>
    <updated>2019-01-22T16:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>之前介绍了Hexo博客平台的搭建，这次我们来看看博客内容怎么编辑。Hexo要求博客必须用Github风格的Markdown语言进行编辑，因此我们就需要了解文字、图片、公式等内容的编辑和排版方式。其中，图片的存储需要利用云服务器的对象存储服务或者图床，下面会具体介绍。</p></blockquote><a id="more"></a><h2 id="1-Markdown编辑器"><a href="#1-Markdown编辑器" class="headerlink" title="1. Markdown编辑器"></a>1. Markdown编辑器</h2><p>既然Markdown是一门语言，那么与之对应的就有很多种编辑器，不仅如此，Markdown的语法在不同的编辑器或者平台（简书、CSDN、Github)还有所不同，既有规则的不同，还有功能的不同，不过大体的语法是相同的。据说Mac上的Markdown编辑器很不错，不过现在我们只介绍Windows上的。由于Hexo需要Github风格的Markdown语法，所以我们最好不用各类博客平台上的在线Markdown编辑器，而是采用支持Github风格Markdown语法的Markdown离线编辑器。</p><h3 id="1-1-Atom"><a href="#1-1-Atom" class="headerlink" title="1.1 Atom"></a>1.1 Atom</h3><p><a href="https://atom.io/">Atom</a>是由Github开发的一款文字与代码编辑器，可以用Git进行版本控制。既然是Github开发的，那么就少不了开源社区众多的支持，所以其支持的语言很丰富，功能也很强大，基本上每个小功能都有一堆的插件供你下载安装，具有极强的定制性,这个比较适合喜欢折腾的人…</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548156767318.png" alt="1548156767318"></p><p>可以看到，<strong>Atom就像一个自带版本库的IDE，支持各种语言，对于所需要的包（主题、语言支持、实时预览等功能）都可以直接在界面中选择安装</strong>，非常的方便，不过由于我写博客主要是在Windows上，所以暂时不想这么折腾，以后只用Linux的时候可以考虑用这个，各种包下下来，估计有差不多1个G了，毕竟不是主要面向的Markdown。</p><h3 id="1-2-Typora"><a href="#1-2-Typora" class="headerlink" title="1.2 Typora"></a>1.2 Typora</h3><p><a href="https://typora.io">Typora</a>是一款免费的Markdown编辑器,支持Windows,OS X和Linux，单就Markdown来说，不逊色于Atom。另外Typora能够实时将Mardown语法转换为渲染后的画面显示，注意，不是并排显示，而是直接替换原有的Markdown部分。其还支持<strong>图片直接拖入，自动生成Markdown语句，word方式生成表格、插入图片、latex公式编辑、代码块高亮、自动显示和插入目录、主题选择</strong>等，可谓相当强大了。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548157486696.png" alt="1548157486696"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548158568834.png" alt="1548158568834"></p><p>主题背景很爽，略逊色于Atom的背景，不过相当简洁了，很照顾我这类选择困难症患者。</p><h3 id="1-3-MarkdownPad2"><a href="#1-3-MarkdownPad2" class="headerlink" title="1.3 MarkdownPad2"></a>1.3 MarkdownPad2</h3><p>MarkdownPad2是Windows平台曾经最优秀的Markdown编辑器了，其部分免费，如果要使用Github风格Markdown语法或者其他高级功能，则需要收费，当然。。。也有破解码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Email:Soar360@live.com</span><br><span class="line">License:GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ==</span><br></pre></td></tr></table></figure><p>然后在工具-选项中设置自己的偏好：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548157795782.png" alt="1548157795782"></p><p>其优点在于，既能够<strong>并排实时预览</strong>，还能够在网页中查看效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548157917594.png" alt="1548157917594"></p><p>看上去很不错的样子，不过其自身不具有图片居中、公式编辑、流程图绘制等功能的，要想使用需要结合html语法和添加样式表等方式扩展，不过这种方式实时预览效果会不大好。</p><p>PS：Windows上使用MardownPad2的时候，实时预览功能一般不能使用，所以需要下载依赖<a href="http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe">Awesomium 1.6.6 SDK</a>。</p><h3 id="1-4-HexoEditor"><a href="#1-4-HexoEditor" class="headerlink" title="1.4 HexoEditor"></a>1.4 HexoEditor</h3><p>HexoEditor，顾名思义，是开源社区贡献者自主开发的一个针对Hexo的博客编辑器，支持自动创建post，也支持<strong>图片的云端上传功能，对于公式编辑、表格、流程图等也都支持</strong>，不过目前我试用的体验一般，有很多Bug，而且安装方式不适合不懂电脑的用户：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548158476175.png" alt="1548158476175"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548158491796.png" alt="1548158491796"></p><p>暂时不推荐使用，不过待其成熟还是可以试试的。</p><p>综上呢，我在Windows上使用的Markdown编辑器是Typora~</p><h2 id="2-Markdown语法"><a href="#2-Markdown语法" class="headerlink" title="2. Markdown语法"></a>2. Markdown语法</h2><p>Markdown语法差异比较多，不过常用的语法基本上在各个编辑器的快捷指令中都包含了，所以不用担心，下面我介绍一些常用的语法：</p><ul><li><strong>标题</strong>：利用<code>#</code>符号可以设定标题，1个表示一级标题，2个表示二级标题，如此类推；</li><li><strong>引用</strong>：利用<code>&gt;</code>可以创建引用块；</li><li><strong>代码</strong>：利用左上角<strong>`</strong>将待显示内容包起来即可；</li><li><strong>代码块</strong>：利用3个<strong>`</strong>描述即可创建代码块(之所以不敲出来是因为这样会让内容变乱)；</li><li><strong>斜体</strong>：利用<code>ctrl+I</code>即可实现斜体；</li><li><strong>加粗</strong>：利用<code>ctrl+B</code>即可实现粗体；</li><li><strong>线状</strong>：水平分割线可以用<code>---</code>或者<code>***</code>实现，下划线可以用<code>ctrl+U</code>实现，删除线可利用<code>~~</code>包裹实现；</li><li>列表：无序列表用<code>-</code>，有序列表用<code>数字+.</code>。</li></ul><p>以上可以看上面介绍<code>MarkdownPad2</code>的图效果，对于图片、公式、表格、流程图等特殊元素，我会在下面具体介绍。</p><h2 id="3-图片及链接编排"><a href="#3-图片及链接编排" class="headerlink" title="3. 图片及链接编排"></a>3. 图片及链接编排</h2><p>图片实际上是一种超链接，所以链接的种类可以分为跳转链接、图片链接两种，具体如下：</p><h3 id="3-1-跳转链接"><a href="#3-1-跳转链接" class="headerlink" title="3.1 跳转链接"></a>3.1 跳转链接</h3><p>跳转链接分为两种：站外跳转和站内跳转：</p><ul><li><p>站外跳转：这种方式很简单，利用下面的方式即可实现点击跳转：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[说明](链接)</span><br></pre></td></tr></table></figure></li><li><p>站内跳转：这种方式适用于目录跳转或者引用跳转，这个要用html语法，不过我试验了多次都不成功，:sob:</p></li></ul><h3 id="3-2-图片编辑与排版"><a href="#3-2-图片编辑与排版" class="headerlink" title="3.2 图片编辑与排版"></a>3.2 图片编辑与排版</h3><p>图片的超链接创建方式有三种：</p><ul><li>Markdown语法实现：<code>![](链接)</code></li><li>拖入/粘贴实现：将剪切板或者本地图片拖入Typora可以自动生成Mardown语句，链接会自动显示为文件本地位置，也可以在设置里面设置默认存放位置，以及绝对路径or相对路径。</li><li>利用<code>ctrl+shitf+I</code>或者<code>工具栏-插入图片</code>可以点击插入本地图片。</li></ul><p>除此之外，Typora的自动创建图片链接的方式可以让图片自动居中，如果我们需要让图片居中或者放缩，则需要用到下面的语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div align=center&gt;&lt;img src = <span class="string">"图片链接"</span> height =xxx width = xxx /&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-图片存储"><a href="#3-3-图片存储" class="headerlink" title="3.3 图片存储"></a>3.3 图片存储</h3><p>无论是GitPages、CodingPages还是云服务器，其提供的免费存储空间都是有上限的，所以我不建议在写博客的时候使用相对路径存储图片，然后上传，这种方式仅针对文字为主的博主。这种方式一般采用图片链接为<code>相对路径</code>并且在_post目录下<strong>新建一个存放图片的目录，并命名为博客名</strong>即可。下面我主要讲利用云服务器对象存储功能实现的图床。</p><h4 id="3-3-1-七牛云"><a href="#3-3-1-七牛云" class="headerlink" title="3.3.1 七牛云"></a>3.3.1 七牛云</h4><p>七牛云是早期比较方便的一个图床，提供了免费的10G存储空间，而且还提供了免费的对象存储空间域名：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548161205227.png" alt="1548161205227"></p><p>除此之外还有图像处理样式，这个是Mardown语法所不具备的，要想使用下面的功能，只需要在图片链接后面加上相应的图像处理样式即可自动处理，可谓极其方便了。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548161255183.png" alt="1548161255183"></p><p>不过七牛云不支持文件夹的上传，所以需要利用相应的工具实现：</p><ul><li><p>Windows端同步上传客户端（<a href="https://developer.qiniu.com/kodo/tools/1666/qsunsync">QSunSync</a>)：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548161643347.png" alt="1548161643347"></p></li><li><p>极简图床以及一些支持七牛云的服务端。</p></li></ul><p>以上均需要提供七牛云的密钥，这个可以在七牛云控制台的个人中心-密钥管理看到。但是七牛云有一个很大弊端：<strong>其提供的免费域名每一个月更新一次，也就是说你上传的博客中所采用的图片每个月都需要重新上传，如果想要自定义域名，则需要备案，但是我们采用的服务器并不是我们的，所以不能备案。</strong></p><h4 id="3-3-2-阿里云"><a href="#3-3-2-阿里云" class="headerlink" title="3.3.2 阿里云"></a>3.3.2 阿里云</h4><p>阿里云对于新用户提供了优惠服务，即免费享用6个月40G的OSS对象存储空间，免费期过后，一年也只需要9元，相当的实惠，并且其提供的域名是永久的，所以不用担心失效。问题在于，图片的云端管理包括：对象存储、下行流量、CDN回源流量、请求次数等服务，仅仅对象存储只能满足我们的存储要求，如果需要供他人或者自己访问，则需要另行购买资源包，很贵！</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548162113897.png" alt="1548162113897"></p><p>如果要是用的话，记得选择“标准存储”和“公共读私有写”模式，阿里云支持文件夹上传和图片处理服务。</p><h4 id="3-3-3-腾讯云"><a href="#3-3-3-腾讯云" class="headerlink" title="3.3.3 腾讯云"></a>3.3.3 腾讯云</h4><p>腾讯云COS对象存储相对阿里云OSS来说，提供了更加便利的免费服务：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/Tencent-cos.png" alt="Tencent-cos"></p><p>是不是很爽，虽然我不大相信未来也不收费，我看了下，价格相对阿里云来说便宜很多，而且未来我也有能力支撑这笔费用了。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548162581492.png" alt="1548162581492"></p><p>可以看到除了图像处理服务，其他的功能都有，而且腾讯云还提供了客户端上传工具<a href="https://cloud.tencent.com/document/product/436/11366">cosbrowser</a>。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548162644297.png" alt="1548162644297"></p><p>综上呢，我选择了腾讯云对象存储，我们利用Typora的自动生成图片链接以及自动复制到相应目录的功能，然后将图片文件夹利用<code>cosbrowser</code>上传至云端，再利用Typora的替换功能将本地路径前缀替换为云端路径前缀即可完成图片的管理。</p><h2 id="4-公式编排"><a href="#4-公式编排" class="headerlink" title="4. 公式编排"></a>4. 公式编排</h2><p>公式编排所利用的是Latex语法，这一点Typora已经集成了，除此之外我们在上传Hexo的时候需要开启mathjax或者katex选项。具体内容如下：</p><h3 id="4-1-Latex公式编辑"><a href="#4-1-Latex公式编辑" class="headerlink" title="4.1 Latex公式编辑"></a>4.1 Latex公式编辑</h3><p>Latex公式编辑主要依赖Latex语法，在Markdown中需要利用$$<code>包裹实现行内公式编辑，或者</code>$$$包裹实现行间公式编辑，具体的Latex语法可能对于普通玩家来说比较麻烦，不过有以下几种便捷方式：</p><ul><li><p><a href="http://www.codecogs.com/latex/eqneditor.php">eqneditor</a>：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548164999775.png" alt="1548164999775"></p></li><li><p><a href="http://www.sciweavers.org/free-online-latex-equation-editor">sciweavers</a></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548165578915.png" alt="1548165578915"></p></li><li><p><a href="http://www.hostmath.com/">hostmath</a>：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548165688021.png" alt="1548165688021"></p></li><li><p><a href="http://www.mathtype.cn/">Mathtype</a>：</p><p>平时敲公式都使用的Mathtype，所以就像能不能把公式直接转换为Latex语句，事实证明可以：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548165974780.png" alt="1548165974780"></p><p>在mathtype-preferences中设置如上的格式，然后复制公式到任意地方，即可自动转为latex语句，只需要将首尾字符替换为$$<code>或者</code>$$$即可，是不是很简单~​</p></li></ul><h3 id="4-2-Hexo的Latex渲染支持"><a href="#4-2-Hexo的Latex渲染支持" class="headerlink" title="4.2 Hexo的Latex渲染支持"></a>4.2 Hexo的Latex渲染支持</h3><p>虽然我们的Markdown编辑器支持了Latex语法，但是Hexo还没支持，所以需要在其中开启相关开关，我是用的是Next6主题，其中自带了相关接口，主题相关内容我后面会单独开博客介绍。</p><h2 id="5-表格和流程图编排"><a href="#5-表格和流程图编排" class="headerlink" title="5. 表格和流程图编排"></a>5. 表格和流程图编排</h2><h3 id="5-1-表格"><a href="#5-1-表格" class="headerlink" title="5.1 表格"></a>5.1 表格</h3><p>我们先看下一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| Tables   |      Are      |  Cool |</span><br><span class="line">|----------|:-------------:|------:|</span><br><span class="line">| col 1 is |  left-aligned | $1600 |</span><br><span class="line">| col 2 is |    centered   |   $12 |</span><br><span class="line">| col 3 is | right-aligned |    $1 |</span><br></pre></td></tr></table></figure><p>上例中<code>------</code>表示左对齐，<code>:------：</code>表示居中，<code>-----:</code>表示右对齐，效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548169846840.png" alt="1548169846840"></p><p>不过每次这么敲太麻烦了，还好Typora提供了表格工具：<code>ctrl+T</code>或者<code>段落-表格</code>：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548170038108.png" alt="1548170038108"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548170073956.png" alt="1548170073956"></p><p>这样就跟word一样了，不过缺点在于，我们很难创建复杂表格，虽然excel支持转html的功能，不过bug多多~</p><h3 id="5-2-流程图"><a href="#5-2-流程图" class="headerlink" title="5.2 流程图"></a>5.2 流程图</h3><p>流程图我一般都用visio画，因为功能强大，不过呢如果用visio的话，就只能用截图显示了，那就避免不了背景冲突，如果不在意可以用。这里我要说的是在markdown里面写流程图，流程图的创建逻辑无非就是前后的逻辑流，所以可以根据各个对象之间的逻辑进行绘制。Typora本身支持流程图、时序图、甘特图等，可利用创建代码块的方式创建。其中flow支持流程图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548170834971.png" alt="1548170834971"></p><p>而mermaid既支持流程图，又支持时序图和甘特图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548171331991.png" alt="1548171331991"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548171299774.png" alt="1548171299774"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548171270900.png" alt="1548171270900"></p><p>是不是很厉害，不过呢，要想在hexo中支持这些功能，还需要下载安装一些插件才行，后续博客我会介绍。</p><h2 id="6-内容导航"><a href="#6-内容导航" class="headerlink" title="6. 内容导航"></a>6. 内容导航</h2><p>有了以上的内容之后，我们怎么设置内容导航，更方便自己和读者了解文章呢：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548171507572.png" alt="1548171507572"></p><p>对此我们就需要设置分类、标签以及目录。</p><h3 id="6-1-Pages导航"><a href="#6-1-Pages导航" class="headerlink" title="6.1 Pages导航"></a>6.1 Pages导航</h3><p>在hexo中，<code>首页</code>是默认开启的，而<code>关于</code>，<code>标签</code>，<code>分类</code>，<code>归档</code>这些都是需要自己创建和设置的。对于这些页面，我们需要完成以下步骤：</p><ul><li><p>step1 修改scaffolds中的page样板：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">type: &#123;&#123;title&#125;&#125;</span><br><span class="line">comments: false</span><br></pre></td></tr></table></figure><p>其中，title表示默认的标题，即<code>关于</code>，<code>标签</code>，<code>分类</code>，<code>归档</code>，其type也是相应的，date会自动获取当前时间,而comments设置为false的原因是，这些界面不需要评论窗口。</p></li><li><p>step2 创建pages:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo new page categories</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page about</span><br><span class="line">hexo new page archives</span><br></pre></td></tr></table></figure><p>注意拼写！</p></li><li><p>Step3 hexo配置：</p><p>在hexo的主题配置文件中设置menu参数：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line"><span class="attr">  home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">home</span></span><br><span class="line"><span class="attr">  about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">user</span></span><br><span class="line"><span class="attr">  tags:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">tags</span></span><br><span class="line"><span class="attr">  categories:</span> <span class="string">/categories/</span> <span class="string">||</span> <span class="string">th</span></span><br><span class="line"><span class="attr">  archives:</span> <span class="string">/archives/</span> <span class="string">||</span> <span class="string">archive</span></span><br></pre></td></tr></table></figure><p>这样我们的博客就有了上述页面</p></li><li><p>Step4 设置博客标签,hexo的配置语言支持yaml格式，所以下面的方式可以设置多个标签：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Markdown</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">对象存储</span></span><br></pre></td></tr></table></figure></li><li><p>Step5 设置博客分类：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">软件安装与使用</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Hexo</span></span><br></pre></td></tr></table></figure><p>分类与标签有所不同，上面这种方式表示二级分类，并不是并行的！另外一定要注意<code>-</code>后面有空格！</p><p>最后我们的博客分类和标签页面都能找到相应的博客，各自博客上也会显示其分类和标签~</p></li><li><p>Step6 简介就是<code>about</code>页面，这个页面就是用来写我们的个人简介的，所以可以直接在blog/source/about/index.md中直接按照写博客的方式书写我们的简介。</p></li></ul><h3 id="6-2-目录"><a href="#6-2-目录" class="headerlink" title="6.2 目录"></a>6.2 目录</h3><p>博客的目录有两种生成方式，一种是博客自带的目录，可以利用html语法写，不过Typora自带目录生成，可利用<code>[toc]</code>或者<code>段落-内容目录</code>自动生成目录，并支持跳转，不过这种方式Markdown本身不兼容，所以需要下载安装一些配置,还需要修改一些配置：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548172658285.png" alt="1548172658285"></p><p>如果不想折腾的话，hexo的一些主题已经自带了目录功能，以我使用的Next6主题为例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">toc:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line"><span class="attr">  number:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># If true, all words will placed on next lines if header width longer then sidebar width.</span></span><br><span class="line"><span class="attr">  wrap:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` var.</span></span><br><span class="line"><span class="attr">  max_depth:</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><ul><li><p><code>enable</code>表示是否开启目录生成，如果开启的话，会自动检测我们Markdown中的标题;</p></li><li><p><code>number</code>表示是否自动加入标题序号，按照<code>1</code>,<code>1.1</code>,<code>1.1.1</code>等进行标注；</p></li><li><p><code>warp</code>表示如果标题过长的话，是否允许标题换行；</p></li><li><p><code>max_Depth</code>表示允许的最大标题等级。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548172973805.png" alt="1548172973805"></p></li></ul><h2 id="7-pdf编辑"><a href="#7-pdf编辑" class="headerlink" title="7. pdf编辑"></a>7. pdf编辑</h2><p>如果我们想在word上编辑，然后放到博客上，可以先利用word另存为pdf，然后利用pdf插件实现。这一点在我所使用的Next6主题中采用了，具体如下：</p><ul><li><p>step1 安装pdf插件</p><p>进入我们博客的主题目录，如blog/themes/next6，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-pdf <span class="built_in">source</span>/lib/pdf</span><br></pre></td></tr></table></figure><p>或者在blog/目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-pdf --save</span><br></pre></td></tr></table></figure><p>上面第一种是next官方支持的，后者是最常用的。</p></li><li><p>step2 hexo 配置（如果采用第一种安装方式）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pdf:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  height:</span> <span class="number">500</span><span class="string">px</span></span><br><span class="line"><span class="attr">  pdfobject:</span></span><br><span class="line"><span class="attr">    cdn:</span> <span class="string">//cdnjs.cloudflare.com/ajax/libs/pdfobject/2.1.1/pdfobject.min.js</span></span><br></pre></td></tr></table></figure></li><li><p>step3 存放pdf</p><p>存放pdf的方式跟图片一模一样，具体我就不说了</p></li><li><p>step4 插入pdf</p><p>官方支持的方式是：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[](pdf链接)</span><br></pre></td></tr></table></figure><p>而采用方法二的使用方式是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf pdf链接 %&#125;</span><br></pre></td></tr></table></figure><p>官方的那个我总是编译出问题，所以下面给出方法二的效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190122/1548178854323.png" alt="1548178854323"></p></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;之前介绍了Hexo博客平台的搭建，这次我们来看看博客内容怎么编辑。Hexo要求博客必须用Github风格的Markdown语言进行编辑，因此我们就需要了解文字、图片、公式等内容的编辑和排版方式。其中，图片的存储需要利用云服务器的对象存储服务或者图床，下面会具体介绍。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Markdown" scheme="https://huangpiao.tech/tags/Markdown/"/>
    
      <category term="对象存储" scheme="https://huangpiao.tech/tags/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>利用Hexo将博客部署到GitPages和CodingPages</title>
    <link href="https://huangpiao.tech/2019/01/21/%E5%88%A9%E7%94%A8Hexo%E5%B0%86%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2%E5%88%B0GitPages%E5%92%8CCodingPages/"/>
    <id>https://huangpiao.tech/2019/01/21/利用Hexo将博客部署到GitPages和CodingPages/</id>
    <published>2019-01-20T18:00:00.000Z</published>
    <updated>2020-01-19T13:58:30.624Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>本人有做总结的习惯，以往都是写word文档，然后存放在qq里面，不过这样的话不是很方便管理，于是萌生了写博客的想法。既然写博客，那何不更有仪式感一点，所以就利用Hexo制作了一个属于自己的博客页面，然后托管至GitPages和CodingPages上，前者是国外的，后者是国内的，方便访问加速。最后呢，我在阿里云上注册了一个域名进行域名解析，于是乎，我就有了一个属于自己的博客网站。</p></blockquote><a id="more"></a><h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><p>Hexo是一个快速，简单而强大的博客框架，可以使用Markdown（或其他语言）编写文章，Hexo可以在几秒钟内生成具有美丽主题的静态文件。其包管理器是Node.js平台的<code>npm</code>，而我们的博客网站时需压迫依托于一个有效的站点和空间的，这里可以选择国外的GitPages，或者国内的CodingPages，分别属于Github和Coding。其中Markdown部分我会单独有一篇博客介绍。</p><p><strong>(1) Node.js</strong></p><p>Node.js是基于Chrome JavaScript运行时建立的一个平台，npm是node.js的包管理工具.在<a href="https://nodejs.org/en/">这里</a>可以下载Node.js,请选择适合自己电脑的最新版本，然后一路next安装，记得选择添加至环境变量，不然后面很麻烦。<br>安装完之后利用cmd或者其他命令行工具输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure><p>如果打印出版本号，那么说明安装成功，否则试着手动配置一下环境变量。</p><p><strong>(2) Git</strong></p><p><code>Git Bash</code>是一款git的命令行工具，支持Mac、Linux和Windows，点击<a href="https://git-scm.com/download/win">这里</a>可以下载安装。最好选择添加至环境变量，或者手动添加git的bin路径至环境变量，这样就能在Windows命令行或者Git Bash工具中直接使用git命令。git主要用于版本控制，另外这里hexo的操作很多都需要在git bash进行，不然会出现奇怪错误，想要了解更多git知识可以参考以下链接：<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000">Git教程—廖雪峰网站</a>。</p><p><strong>(3) Hexo</strong></p><p>Hexo就是用来将我们用Markdown写好的博客生成html页面,并自动部署到Github或者Coding上，其依托于npm包管理器，所以可以直接下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>这条语句的作用是将hexo命令行工具安装在全局，要注意的是npm安装软件时，经常会出现warning，不过不要紧，只要不是error就行。</p><hr><h2 id="2-建站"><a href="#2-建站" class="headerlink" title="2. 建站"></a>2. 建站</h2><p><strong>Step1 初始化站点</strong></p><p>首先我们新建一个存放博客的文件目录，例如blog/，然后进入该目录，利用命令行工具进行站点部署<code>hexo init</code>，也可以直接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>当然也可以直接执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init blog</span><br></pre></td></tr></table></figure><p>这一步会将远端的hexo文件克隆到博客目录。</p><p><strong>Step2 安装依赖</strong></p><p>进入blog目录，然后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure><p>自动安装hexo所需要的依赖项，此时的博客目录应该如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">|—— node_modules</span><br><span class="line">|—— scaffolds</span><br><span class="line">|—— <span class="built_in">source</span></span><br><span class="line">||—— _posts</span><br><span class="line">|—— themes</span><br><span class="line">||—— landscape</span><br><span class="line">|—— .gitignore</span><br><span class="line">|—— _config.yml</span><br><span class="line">|—— db.json</span><br><span class="line">|—— package.json</span><br></pre></td></tr></table></figure><p>其中node_modules里面放的是利用<code>npm install --save</code>下载安装的软件，scaffolds里面存放的是各种脚手架模板，也就是说可以创建post、page、draft三种页面，会存放在source目录。source目录内的文件就是待发布的文件，themes目录里面存放的是我们所引用的主题模板，默认的是landscape主题。.gitignore存放的是我们部署到github或者coding.me上时需要忽略的文件，_config.yml是站点配置文件，与之相应地，在主题中还有一个主题配置文件，这两个在后面的配置中很重要。</p><p><strong>Step3 创建页面</strong></p><p>在blog目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean;hexo g;hexo s</span><br></pre></td></tr></table></figure><p>即清空站点静态页面缓存（清空public文件夹)，在本地生成静态页面（在public文件夹)，开启本地服务器，这时候，我们可以在网页中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:4000</span><br></pre></td></tr></table></figure><p>就可以看到如下的画面效果：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/localhost.png"></div><p>这个页面是hexo初始化的一个简单页面，主题是landscape。</p><hr><h2 id="3-托管GitPages"><a href="#3-托管GitPages" class="headerlink" title="3. 托管GitPages"></a>3. 托管GitPages</h2><p>上面的整个过程都是基于本地的，只能在本地给自己看，局限性很大，为了将博客部署到网站上，我们可以利用Github提供的GitPages免费站点服务，自带域名和1G免费空间，存放网页绰绰有余（图片不要放进去）。</p><p><strong>Step1 Windows端搭建Github环境</strong></p><p>这一部分内容可以见<a href="https://huangpiao.tech/2019/01/19/Windows%E5%B9%B3%E5%8F%B0%E4%B8%8BGithub%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E6%90%AD%E5%BB%BA/">我之前的博客</a>，从而完成Github注册和ssh环境配置。</p><p><strong>Step2 新建博客repository</strong></p><p>在github新建一个名为<code>你的用户名.github.io</code>的repository，这样做的好处是避免生成的域名太长,具体可执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"# nightmaredimple.github.io"</span> &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m <span class="string">"first commit"</span></span><br><span class="line">git remote add origin https://github.com/nightmaredimple/nightmaredimple.github.io.git</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><p>从而在你的github仓库中有了一个属于你博客站点文件存放的位置，一般设为master分支。</p><p><strong>Step3 Hexo配置</strong></p><p>首先安装部署依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>然后在站点配置文件blog/_config.yml中配置deploy为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: </span><br><span class="line">    github: https://github.com/nightmaredimple/nightmaredimple.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p><strong>Step4 完成部署</strong></p><p>在blog目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean;hexo g;hexo d</span><br></pre></td></tr></table></figure><p>执行完毕后，你就可以在你的github相应的博客repository下看到部署后的博客站点文件，通过链接可以看到属于你的博客网页，例如我的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://nightmaredimple.github.io</span><br></pre></td></tr></table></figure><p>也可以通过ping网站的方式观察，二者的ip一致~</p><hr><h2 id="4-托管CodingPages"><a href="#4-托管CodingPages" class="headerlink" title="4. 托管CodingPages"></a>4. 托管CodingPages</h2><p>Coding几乎算得上是Github的汉化版，缺点就是资源少，但是因为在国内，所以访问速度快。我在自己电脑上测试发现，ping自己的博客域名的延时是250+ms，很慢，所以我想同时将博客部署在Github和Coding上。既然我说Coding是Github的汉化版，那么也就是说我们之前在Windows端搭建的Github环境依然有效，并且部署过程几乎一样。</p><p><strong>Step1 Windows端搭建Coding环境</strong></p><p>首先进入<a href="https://coding.net/">Coding官网</a>注册,然后可以直接新建项目：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/coding-repository.png"></div>记得一定要初始化项目，不然无法开启pages服务。 同样地，我们还是保持项目名与用户名一致，这种创建方式可以直接替代上面github的方式，不过我还是贴出：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo "# nightmaredimple.coding.me" &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m "first commit"</span><br><span class="line">git remote add origin https://git.dev.tencent.com/nightmaredimple/nightmaredimple.git</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure>要注意的是，由于coding和腾讯云合作了，所以我们的coding项目会自动同步到腾讯云，也就是说上面的repository链接还可以写成：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://git.coding.net/nightmaredimple/nightmaredimple.git</span><br></pre></td></tr></table></figure>**Step2 部署项目公钥** 将我们在部署Github时生成的ssh公钥复制，进入控制台/设置/管理公钥，新建一个公钥复制进去,当然还可以新建一个个人公钥，以面向所有项目，直接点击头像，进入个人账户/SSH公钥即可。<div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/coding-ssh.png"></div><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/coding-ssh2.png"></div><p><strong>Step3 开启CodingPages服务</strong><br>进入控制台/代码/Pages服务,一键开启Coding Pages</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/coding-pages.png"></div><p><strong>Step4 Hexo配置</strong></p><p>这里跟Github一样，我们可以一起安装部署，写入如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: </span><br><span class="line">github: https://github.com/nightmaredimple/nightmaredimple.github.io.git</span><br><span class="line">    coding: https://git.coding.net/nightmaredimple/nightmaredimple.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>据其他人的踩坑记录，貌似需要在blog/source下创建一个名为Staticfile的空文件。</p><p><strong>Step5 完成部署</strong></p><p>在blog目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean;hexo g;hexo d</span><br></pre></td></tr></table></figure><p>执行完毕后，你就可以在你的coding.net或者腾讯云相应的博客repository下看到部署后的博客站点文件，通过链接可以看到属于你的博客网页，例如我的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://nightmaredimple.coding.me</span><br></pre></td></tr></table></figure><p>再ping一下，发现延迟只有40ms了。</p><hr><h2 id="5-自定义域名"><a href="#5-自定义域名" class="headerlink" title="5. 自定义域名"></a>5. 自定义域名</h2><p>完成以上操作之后，我们就拥有了两个博客域名，但是我还是想申请一个属于自己的域名，所以去阿里云绑定了一个域名，然后将域名解析到了自己的博客域名，具体过程如下：</p><p><strong>Step1 完成阿里云注册</strong></p><p>阿里云的域名申请是在<a href="https://wanwang.aliyun.com/">万网</a>，我们先根据要求完成注册和实名认证，很快。</p><p><strong>Step2 申请域名</strong></p><p>认证成功之后，我们通过查询域名看看那些域名可以用，并且根据合适的名称，合适的价格选择想要的后缀，然后进行域名的购买，不贵。</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/aliyun-wanwang.png"></div><p><strong>Step3 域名解析</strong></p><p>申请到我们的域名之后，就需要绑定我们的博客网站了，但是只能绑定其中一个，这里我绑定的coding的，因为访问更快。而域名解析的话，最好解析两个，一个是ip，一个是域名，对于我们博客网站的ip，可以通过ping自己的博客域名获取。</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/aliyun-yuming.png"></div><p>添加两条解析记录,其中A对应ip，CNAME对应博客原始域名：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/aliyun-jiexi.png"></div><p><strong>Step4 域名绑定</strong></p><p>首先在blog/source下新建一个CNAME文件，里面填写我们买的域名，然后相应的博客网站进行配置。对于GitPages，我们需要进入博客repository的setting中，添加GitPages的Custom Domain：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/github-settings.png"></div><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/github-domain.png"></div><p>而对于CodingPages，则是直接在Pages服务中添加新域名：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190121/coding-domain.png"></div><p>上述均有强制HTTPS访问选项，这个选项开启之后，我们的网站就变成了<code>https://xxxxx</code>，否则就是<code>http</code>。</p><p><strong>Step5 完成部署</strong><br>首先填写站点配置文件_config.yml中的<code>url</code>选项，填写你的域名。然后选做：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-cname</span><br></pre></td></tr></table></figure><p>这一项可以帮助你自动添加CNAME,接着在blog目录下执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean;hexo g;hexo d</span><br></pre></td></tr></table></figure><p>等待十几秒就能看直接通过我们绑定的域名访问博客啦~</p><p>PS:最近Coding时不时崩溃，所以暂时绑定GitPages。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;本人有做总结的习惯，以往都是写word文档，然后存放在qq里面，不过这样的话不是很方便管理，于是萌生了写博客的想法。既然写博客，那何不更有仪式感一点，所以就利用Hexo制作了一个属于自己的博客页面，然后托管至GitPages和CodingPages上，前者是国外的，后者是国内的，方便访问加速。最后呢，我在阿里云上注册了一个域名进行域名解析，于是乎，我就有了一个属于自己的博客网站。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Hexo" scheme="https://huangpiao.tech/tags/Hexo/"/>
    
      <category term="GitPages" scheme="https://huangpiao.tech/tags/GitPages/"/>
    
      <category term="Coding Pages" scheme="https://huangpiao.tech/tags/Coding-Pages/"/>
    
  </entry>
  
  <entry>
    <title>Windows平台下Github远程仓库的搭建</title>
    <link href="https://huangpiao.tech/2019/01/19/Windows%E5%B9%B3%E5%8F%B0%E4%B8%8BGithub%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    <id>https://huangpiao.tech/2019/01/19/Windows平台下Github远程仓库的搭建/</id>
    <published>2019-01-18T18:10:00.000Z</published>
    <updated>2019-01-18T18:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>Github是一个面向开源及私有软件项目的托管平台,拥有超过900万开发者用户，有众多的开源项目供研究者学习。还提供了很多项目管理功能，方便多终端同步管理项目。本文将介绍Windows平台下Github远程仓库的注册，部署和绑定,不介绍Github的使用！</p></blockquote><a id="more"></a><h2 id="1-Github注册"><a href="#1-Github注册" class="headerlink" title="1. Github注册"></a>1. Github注册</h2><p>进入<a href="https://github.com/">Github</a>网站进行注册，界面如下：</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190119/github-register.png" height="400"></div>输入用户名、邮箱和密码等相关信息后完成Github的注册和登录，对于修改用户名和密码等相关信息要求可以点击右上角用户图标-选中Settings:<div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190119/github-settings.png"></div>设置好相关配置之后就可以试着在右上角点击 ` + `，新建一个repository，设置名称，即可开始上传你的项目文件了。<div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190119/github-repository.png" height="600"></div><h2 id="2-Windows本地部署Github"><a href="#2-Windows本地部署Github" class="headerlink" title="2. Windows本地部署Github"></a>2. Windows本地部署Github</h2><h3 id="2-1-Github-for-Windows"><a href="#2-1-Github-for-Windows" class="headerlink" title="2.1. Github for Windows"></a>2.1. Github for Windows</h3><p><code>Github for Windows</code>是Github在Windows本地的客户端，点击<a href="https://desktop.github.com/">这里</a>可以下载安装。</p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190119/github-windows.png"></div><h3 id="2-2-Git-Bash"><a href="#2-2-Git-Bash" class="headerlink" title="2.2 Git Bash"></a>2.2 Git Bash</h3><p><code>Git Bash</code>是一款git的命令行工具，支持Mac、Linux和Windows，点击<a href="https://git-scm.com/download/win">这里</a>可以下载安装。最好选择添加至环境变量，或者手动添加git的bin路径至环境变量，这样就能在Windows命令行或者Git Bash工具中直接使用git命令。</p><h2 id="3-Windows本地绑定配置Github"><a href="#3-Windows本地绑定配置Github" class="headerlink" title="3. Windows本地绑定配置Github"></a>3. Windows本地绑定配置Github</h2><p><strong>Step1 配置Git账户</strong><br>在Git bash命令行输入：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">"你的GitHub用户名"</span></span><br><span class="line">git config --global user.email <span class="string">"你的GitHub注册邮箱"</span></span><br></pre></td></tr></table></figure><p></p><p><strong>Step2 生成新的ssh key</strong><br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">"你的GitHub注册邮箱"</span></span><br><span class="line">cat C:/Users/用户名/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p></p><p><strong>Step3 Github官网配置ssh key</strong><br>在Github官网个人主页的Settings中新建SSH Key，并将上一步打印的信息复制进其中:</p><p></p><div align="center"><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190119/github-ssh.png" height="400"></div><br>在Git Bash命令行输入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p></p><p>如果打印<code>You&#39;ve successfully authenticated!</code>，则说明验证成功。</p><h2 id="4-Windows本地托管项目"><a href="#4-Windows本地托管项目" class="headerlink" title="4. Windows本地托管项目"></a>4. Windows本地托管项目</h2><p><strong>Step1 新建仓库</strong><br>首先在自己电脑某个地方新建一个文件夹，即新建一个仓库，假设名字为Test,然后在git bash中进入该文件夹目录，执行<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p></p><p>从而Test目录下会多出一个.git目录<br><strong>Step2 将文件添加到版本库</strong><br>假设在该仓库下有一个待上传的文件夹project，该文件夹中有一些项目文件，同样是进入该文件夹，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin 项目git地址</span><br></pre></td></tr></table></figure><p>从而在你的用户下创建了一个名为project的repository。<br><strong>Step3 将版本库同步到本地</strong><br>如果之前已经创建过了，则需要将Github段的文件同步过来：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull 项目git地址</span><br></pre></td></tr></table></figure><p></p><p>与此同时，本地目录上会多出原本在github上的文件 。<br><strong>Step4 更新版本库</strong><br>首先利用 <code>git add 目录</code>（如果是一个点，则提交所有文件，否则指定文件），然后利用<code>git commit –m “版本2”</code> (-m后面跟提示信息，这个提示信息是一定要写的，记录我们提交的过程，写清晰为什么提交或修改了什么是非常有用的。)，最后将其推送至版本库：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><p></p><p>将当前分支推送到版本库master分支</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 14:29:20 GMT+0800 (中国标准时间) --&gt;&lt;hr&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Github是一个面向开源及私有软件项目的托管平台,拥有超过900万开发者用户，有众多的开源项目供研究者学习。还提供了很多项目管理功能，方便多终端同步管理项目。本文将介绍Windows平台下Github远程仓库的注册，部署和绑定,不介绍Github的使用！&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="软件安装与使用" scheme="https://huangpiao.tech/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Github" scheme="https://huangpiao.tech/tags/Github/"/>
    
  </entry>
  
</feed>
