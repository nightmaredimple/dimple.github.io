<!-- build time:Thu Mar 05 2020 18:08:44 GMT+0800 (中国标准时间) --><!DOCTYPE html><html class="theme-next gemini" lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.min.css"><link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="stylesheet" href="/css/main.css?v=6.7.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/128x128.png?v=6.7.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/32x32.png?v=6.7.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/16x16.png?v=6.7.0"><link rel="mask-icon" href="/images/logo2.svg?v=6.7.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"6.7.0",sidebar:{position:"left",display:"post",offset:12,b2t:!0,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!0,lazyload:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/8f5c5484.js","daovoice"),daovoice("init",{app_id:"8f5c5484"}),daovoice("update")</script><meta name="description" content="前言YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。"><meta name="keywords" content="目标检测,YOLO"><meta property="og:type" content="article"><meta property="og:title" content="详解YOLO系列"><meta property="og:url" content="https://huangpiao.tech/2019/02/09/详解YOLO系列算法/index.html"><meta property="og:site_name" content="见渊の博客"><meta property="og:description" content="前言YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/sayit.jpg"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220845247.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220928555.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549555635485.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549556050439.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/822124-20160902160437324-793316644.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180114183212429.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221342432.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549636582826.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221649338.jpg"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221547656.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4-lc.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645871959.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645895056.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/3.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549648009461.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549650071906.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549695294511.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/u=212751988,185815275&fm=173&s=0942F5122D5ED5CE18C595DA000050B3&w=459&h=414&img.JPEG"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697404615.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697673711.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697855804.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698204272.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698728695.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698809190.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702344040.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702478831.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181226152903109.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549703794254.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549705872475.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549704037718.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549723273927.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721649962.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721663939.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549724233482.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725186270.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/2018071722042637.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725539157.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/606386-20180327004340505-1572852891.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181213163753559.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-fbf72eec750dfd743a15f511872a0974_hd.jpg"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-400c7fed8265bf7e2a5b42a866c6f2a1_hd.jpg"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549729080541.png"><meta property="og:updated_time" content="2019-02-09T15:50:00.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="详解YOLO系列"><meta name="twitter:description" content="前言YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。"><meta name="twitter:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/sayit.jpg"><link rel="alternate" href="/atom.xml" title="见渊の博客" type="application/atom+xml"><link rel="canonical" href="https://huangpiao.tech/2019/02/09/详解YOLO系列算法/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>详解YOLO系列 | 见渊の博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">见渊の博客</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签<span class="badge">30</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类<span class="badge">12</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档<span class="badge">25</span></a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><a href="https://github.com/nightmaredimple" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://huangpiao.tech/2019/02/09/详解YOLO系列算法/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="黄飘"><meta itemprop="description" content="直到这一刻微笑着说话为止，我至少留下了一公升眼泪"><meta itemprop="image" content="/images/author.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="见渊の博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">详解YOLO系列</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-02-09 23:50:00" itemprop="dateCreated datePublished" datetime="2019-02-09T23:50:00+08:00">2019-02-09</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/目标检测/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2019/02/09/详解YOLO系列算法/#comments" itemprop="discussionUrl"><span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/02/09/详解YOLO系列算法/" itemprop="commentCount"></span> </a></span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span><span title="本文字数">10.9k字</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。</p></blockquote><a id="more"></a><h2 id="1-目标检测简介"><a href="#1-目标检测简介" class="headerlink" title="1.目标检测简介"></a>1.目标检测简介</h2><p><a href="https://pjreddie.com/darknet/yolo/">YOLO</a>(You Only Look Once)的作者非常萌，无论是写作风格、表情包还是Github风格，都表现出他是一个有趣的人。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/sayit.jpg" alt="img"></p><p>好了，言归正传，众所众知，目标检测算法的核心在于：</p><ul><li><p><strong>候选区域/框/角点等的确定</strong>。神经网络/深度学习本质是分类，那么对于目标检测问题，我们需要将其转化为分类问题，因此许多研究者发现需要先确定候选位置，然后对候选位置进行分类判断。这里，候选区域的选取从最初的滑窗方式。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220845247.png" alt="img"></p><p>慢慢演变到以<code>Selective Search</code>(过分割+分层聚类)为主的RCNN算法，为了更高效的生成候选区域，我们又利用卷积和池化过程近似滑窗从而有了Fast RCNN算法。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130220928555.png" alt="img"></p><p>再演变至以<code>anchor box</code>为代表的Faster RCNN、SSD和YOLO等系列算法，其原理在于可以对每一个ROI区域的中心，给定一个假设的长宽比，由此作为候选区域，再在后面利用回归层精修回归框。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549555635485.png" alt="1549555635485"></p><p>最后到现在的<code>Corner</code>为代表的CornerNet算法，不断地提升候选框提取效率、候选框有效率、候选框精准度以及与分类框架的融合。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549556050439.png" alt="1549556050439"></p></li><li><p><strong>判断目标属于什么类别</strong>。有了候选区域，那么就可以利用很简单的级联全连接层判定每个候选区域属于前景/背景的概率，以及属于各个目标类别的概率。</p></li><li><p><strong>目标精定位</strong>。目标框的描述包括目标的中心/角点位置和宽高，这些仅仅依赖候选区域是不够精确的，那么就需要合理的设计损失函数，利用多个全连接层进行进一步的回归，得到目标框的精修位置。当然CornerNet里面采用骨骼关键点检测里面的Hourglass结构作为backbone，再加上Corner Pooling层预测角点位置，不存在精修。</p></li></ul><h2 id="2-Darknet"><a href="#2-Darknet" class="headerlink" title="2.Darknet"></a>2.Darknet</h2><h3 id="2-1-Darknet网络框架"><a href="#2-1-Darknet网络框架" class="headerlink" title="2.1 Darknet网络框架"></a>2.1 Darknet网络框架</h3><p>目前来说，无论是在目标检测、目标识别还是目标分割、姿态分析等领域，都会用到各种各样的backbone网络，最常用的就是基于图像分类的backbone网络，因为深度学习本质是分类，而绝大多数分类网络都会在ImageNet竞赛中进行测试，我们从AlexNet，VGGNet到GoogleNet，再到ResNet/DenseNet等，已经见过很多优秀的骨干网络结构了，其中很多优秀的子模块也被用于其他网络结构，如：卷积+池化+BatchNorm+Relu的组合、Inception各个版本结构、残差模块、1x1卷积核等等。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/822124-20160902160437324-793316644.png" alt="img"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180114183212429.png" alt="img"></p><p>而Darknet实际上也是YOLO作者实现的一个backbone网络，其改进对象主要是GoogleNet，将GoogleNet中的Inception结构改成了串行的结构，从而使得网络速度更快，而效果仅仅损失了一点。可以看到下面的网络结构有24个卷积层，外加2个全连接层。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221342432.png" alt="img"></p><h3 id="2-2-Darknet训练框架"><a href="#2-2-Darknet训练框架" class="headerlink" title="2.2 Darknet训练框架"></a>2.2 Darknet训练框架</h3><p>除此之外呢，<a href="https://github.com/pjreddie/darknet">Darknet</a>也是一个深度学习框架，其框架设计与<a href="https://github.com/BVLC/caffe">caffe</a>基本一致，只不过是用C语言写的，其整体框架十分简洁，所以编译速度非常快。如果学习过caffe的话，应该很容易上手darknet。其中<code>darknet.h</code>就类似于caffe中的<code>caffe.proto</code>，定义了所有数据结构，而网络的构建是利用了<code>cfg</code>格式文件，即利用<code>key=value</code>方式搭建网络，这种方式的问题在于对于复杂网络的设计非常复杂，很难写。另外，由于darknet是纯C框架，所以要想增加自定义层的话会比较麻烦，主要是因为没有好的设计模式和面向对象设计，导致使用者需要完全读懂整个框架，而且很难实现共享内存和逐层不同学习率。</p><p>当然，darknet框架的安装也是很简单的，除开显卡驱动和CUDA、cudnn等配置之外，只需要从git上面clone下来源码，然后<code>make</code>即可，这里我们不考虑Windows版本的，github上面有相应的<a href="https://github.com/AlexeyAB/darknet">教程</a>。不仅可以利用原始的C接口，还能利用将其编译为动态链接库供C++接口调用，见<a href="https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp">这里</a>，不过我主要是利用python接口调用，这里呢就存在一个问题，即原始darknet数据结构是<code>image</code>，如果利用<code>ctypes</code>进行C/Python混合编程的话，需要设计到numpy数据结构与image数据结构的交互，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> darknet <span class="keyword">as</span> dn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMAGE</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"w"</span>, c_int),</span><br><span class="line">                (<span class="string">"h"</span>, c_int),</span><br><span class="line">                (<span class="string">"c"</span>, c_int),</span><br><span class="line">                (<span class="string">"data"</span>, POINTER(c_float))]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETECTION</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"bbox"</span>, BOX),</span><br><span class="line">                (<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"prob"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"mask"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"objectness"</span>, c_float),</span><br><span class="line">                (<span class="string">"sort_class"</span>, c_int)]</span><br><span class="line">    </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">array_to_image</span><span class="params">(arr)</span>:</span></span><br><span class="line">    arr = arr.transpose(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    c = arr.shape[<span class="number">0</span>]</span><br><span class="line">    h = arr.shape[<span class="number">1</span>]</span><br><span class="line">    w = arr.shape[<span class="number">2</span>]</span><br><span class="line">    arr = (arr/<span class="number">255.0</span>).flatten()</span><br><span class="line">    data = dn.c_array(dn.c_float, arr)</span><br><span class="line">    im = dn.IMAGE(w,h,c,data)</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><p>上面这种利用图像数据结构转化的方式，会占用很多时间，所以我们可以利用numpy的c接口实现数据结构转换，具体如下：</p><p>先在<code>src/image.c</code> line 558左右添加：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> NUMPY</span></span><br><span class="line"><span class="function">image <span class="title">ndarray_to_image</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span>* src, <span class="keyword">long</span>* shape, <span class="keyword">long</span>* strides)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h = shape[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> w = shape[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> c = shape[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span> step_h = strides[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> step_w = strides[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> step_c = strides[<span class="number">2</span>];</span><br><span class="line">    image im = make_image(w, h, c);</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">int</span> index1, index2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h; ++i)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k= <span class="number">0</span>; k &lt; c; ++k)&#123;</span><br><span class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; w; ++j)&#123;</span><br><span class="line"></span><br><span class="line">                    index1 = k*w*h + i*w + j;</span><br><span class="line">                    index2 = step_h*i + step_w*j + step_c*k;</span><br><span class="line">                    im.data[index1] = src[index2]/<span class="number">255.</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    rgbgr_image(im);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> im;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>然后在<code>src/image.h</code>19行左右添加：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> NUMPY</span></span><br><span class="line"><span class="function">image <span class="title">ndarray_to_image</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span>* src, <span class="keyword">long</span>* shape, <span class="keyword">long</span>* strides)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>再在<code>MakeFile</code>中加入：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(NUMPY)</span>, 1) </span><br><span class="line">COMMON+= -DNUMPY -I/usr/<span class="keyword">include</span>/python2.7/ -I/usr/lib/python2.7/dist-packages/numpy/core/<span class="keyword">include</span>/numpy/</span><br><span class="line">CFLAGS+= -DNUMPY</span><br><span class="line"><span class="keyword">endif</span></span><br></pre></td></tr></table></figure><p>并设置<code>Makefile</code></p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU=1</span><br><span class="line">CUDNN=1</span><br><span class="line">OPENCV=1</span><br><span class="line">OPENMP=0</span><br><span class="line">NUMPY=1</span><br><span class="line">DEBUG=0</span><br></pre></td></tr></table></figure><p>最后<code>python</code>接口为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nparray_to_image</span><span class="params">(self,img)</span>:</span></span><br><span class="line">    data = img.ctypes.data_as(POINTER(c_ubyte))</span><br><span class="line">    image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><p>这样的话，数据转换的速度大大提升,下面我附上我写的<code>darknet.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">c_array</span><span class="params">(ctype, values)</span>:</span></span><br><span class="line">    arr = (ctype*len(values))()</span><br><span class="line">    arr[:] = values</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BOX</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"x"</span>, c_float),</span><br><span class="line">                (<span class="string">"y"</span>, c_float),</span><br><span class="line">                (<span class="string">"w"</span>, c_float),</span><br><span class="line">                (<span class="string">"h"</span>, c_float)]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETECTION</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"bbox"</span>, BOX),</span><br><span class="line">                (<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"prob"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"mask"</span>, POINTER(c_float)),</span><br><span class="line">                (<span class="string">"objectness"</span>, c_float),</span><br><span class="line">                (<span class="string">"sort_class"</span>, c_int)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMAGE</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"w"</span>, c_int),</span><br><span class="line">                (<span class="string">"h"</span>, c_int),</span><br><span class="line">                (<span class="string">"c"</span>, c_int),</span><br><span class="line">                (<span class="string">"data"</span>, POINTER(c_float))]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">METADATA</span><span class="params">(Structure)</span>:</span></span><br><span class="line">    _fields_ = [(<span class="string">"classes"</span>, c_int),</span><br><span class="line">                (<span class="string">"names"</span>, POINTER(c_char_p))]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObjectDetect</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,cfg_path = None, wight_path = None, meta_path = None, ctx = None)</span>:</span></span><br><span class="line">        lib = CDLL(os.path.dirname(os.path.realpath(__file__))+<span class="string">"/libdarknet.so"</span>, RTLD_GLOBAL)</span><br><span class="line"></span><br><span class="line">        lib.network_width.argtypes = [c_void_p]</span><br><span class="line">        lib.network_width.restype = c_int</span><br><span class="line">        lib.network_height.argtypes = [c_void_p]</span><br><span class="line">        lib.network_height.restype = c_int</span><br><span class="line"></span><br><span class="line">        predict = lib.network_predict</span><br><span class="line">        predict.argtypes = [c_void_p, POINTER(c_float)]</span><br><span class="line">        predict.restype = POINTER(c_float)</span><br><span class="line"></span><br><span class="line">        self.set_gpu = lib.cuda_set_device</span><br><span class="line">        self.set_gpu.argtypes = [c_int]</span><br><span class="line">        <span class="keyword">if</span> ctx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.set_gpu(ctx)</span><br><span class="line"></span><br><span class="line">        make_image = lib.make_image</span><br><span class="line">        make_image.argtypes = [c_int, c_int, c_int]</span><br><span class="line">        make_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.get_network_boxes = lib.get_network_boxes</span><br><span class="line">        self.get_network_boxes.argtypes = [c_void_p, c_int, c_int, c_float, c_float, POINTER(c_int), c_int, POINTER(c_int)]</span><br><span class="line">        self.get_network_boxes.restype = POINTER(DETECTION)</span><br><span class="line"></span><br><span class="line">        make_network_boxes = lib.make_network_boxes</span><br><span class="line">        make_network_boxes.argtypes = [c_void_p]</span><br><span class="line">        make_network_boxes.restype = POINTER(DETECTION)</span><br><span class="line"></span><br><span class="line">        self.free_detections = lib.free_detections</span><br><span class="line">        self.free_detections.argtypes = [POINTER(DETECTION), c_int]</span><br><span class="line"></span><br><span class="line">        free_ptrs = lib.free_ptrs</span><br><span class="line">        free_ptrs.argtypes = [POINTER(c_void_p), c_int]</span><br><span class="line"></span><br><span class="line">        network_predict = lib.network_predict</span><br><span class="line">        network_predict.argtypes = [c_void_p, POINTER(c_float)]</span><br><span class="line"></span><br><span class="line">        self.load_net = lib.load_network</span><br><span class="line">        self.load_net.argtypes = [c_char_p, c_char_p, c_int]</span><br><span class="line">        self.load_net.restype = c_void_p</span><br><span class="line">        self.net = self.load_net(cfg_path, weight_path, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.do_nms_obj = lib.do_nms_obj</span><br><span class="line">        self.do_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]</span><br><span class="line"></span><br><span class="line">        do_nms_sort = lib.do_nms_sort</span><br><span class="line">        do_nms_sort.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]</span><br><span class="line"></span><br><span class="line">        self.free_image = lib.free_image</span><br><span class="line">        self.free_image.argtypes = [IMAGE]</span><br><span class="line"></span><br><span class="line">        letterbox_image = lib.letterbox_image</span><br><span class="line">        letterbox_image.argtypes = [IMAGE, c_int, c_int]</span><br><span class="line">        letterbox_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.load_meta = lib.get_metadata</span><br><span class="line">        lib.get_metadata.argtypes = [c_char_p]</span><br><span class="line">        lib.get_metadata.restype = METADATA</span><br><span class="line">        self.meta = self.load_meta(meta_path)</span><br><span class="line"></span><br><span class="line">        load_image = lib.load_image_color</span><br><span class="line">        load_image.argtypes = [c_char_p, c_int, c_int]</span><br><span class="line">        load_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        rgbgr_image = lib.rgbgr_image</span><br><span class="line">        rgbgr_image.argtypes = [IMAGE]</span><br><span class="line"></span><br><span class="line">        self.ndarray_image = lib.ndarray_to_image</span><br><span class="line">        self.ndarray_image.argtypes = [POINTER(c_ubyte), POINTER(c_long), POINTER(c_long)]</span><br><span class="line">        self.ndarray_image.restype = IMAGE</span><br><span class="line"></span><br><span class="line">        self.predict_image = lib.network_predict_image</span><br><span class="line">        self.predict_image.argtypes = [c_void_p, IMAGE]</span><br><span class="line">        self.predict_image.restype = POINTER(c_float)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nparray_to_image</span><span class="params">(self,img)</span>:</span></span><br><span class="line">        data = img.ctypes.data_as(POINTER(c_ubyte))</span><br><span class="line">        image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides)</span><br><span class="line">        <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detect</span><span class="params">(self,img, thresh=<span class="number">.5</span>, hier_thresh=<span class="number">.5</span>, nms=<span class="number">.45</span>)</span>:</span></span><br><span class="line">        im = self.nparray_to_image(img)</span><br><span class="line">        num = c_int(<span class="number">0</span>)</span><br><span class="line">        pnum = pointer(num)</span><br><span class="line">        self.predict_image(self.net, im)</span><br><span class="line">        dets = self.get_network_boxes(self.net, im.w, im.h, thresh, hier_thresh, <span class="keyword">None</span>, <span class="number">0</span>, pnum)</span><br><span class="line">        num = pnum[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> (nms): self.do_nms_obj(dets, num, self.meta.classes, nms);</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(meta.classes):</span><br><span class="line">                <span class="keyword">if</span> dets[j].prob[i] &gt; <span class="number">0</span>:</span><br><span class="line">                    b = dets[j].bbox</span><br><span class="line">                    res.append((self.meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))</span><br><span class="line">        res = sorted(res, key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line">        self.free_image(im)</span><br><span class="line">        self.free_detections(dets, num)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model = ObjectDetect(<span class="string">"cfg/yolo.cfg"</span>, <span class="string">"yolo.weights"</span>,<span class="string">"cfg/coco.data"</span>)</span><br><span class="line">    cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">    ret, img = cap.read()</span><br><span class="line">    r = model.detect(img)</span><br><span class="line">    print(r)</span><br></pre></td></tr></table></figure><h2 id="3-YOLOv1"><a href="#3-YOLOv1" class="headerlink" title="3.YOLOv1"></a>3.YOLOv1</h2><h3 id="3-1-YOLO网络框架"><a href="#3-1-YOLO网络框架" class="headerlink" title="3.1 YOLO网络框架"></a>3.1 YOLO网络框架</h3><p>作者为了让backbone网络具有更好的性能，取了上面提到的darknet版本的前20个卷积层，然后利用一个全局池化层和一个全连接层，搭建了一个预训练网络，其中全局池化层是指的将一个通道内的所有元素平均，这一点在YOLO系列版本中都有体现：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549636582826.png" alt="1549636582826"></p><p>可以看到YOLOv1才用的darknet版本(Darknet Reference)效果最差，其余的我们后面再说。有了预训练模型之后，我们再以上面提到的Darknet的完整框架（24卷积层+2全连接层）进行训练，其中网络输入大小固定为448x448。然后将最后一层的输出形状改为7x7x30。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221649338.jpg" alt="img"></p><p>对于网络的输出，我们可以这样理解，30=20+2+4x2，其中20指的是VOC数据集的类别数，即20个类别的概率，2指的是两个目标框的置信度，然后每个通道预测2个目标框，所以就是两个(x,y,w,h)即8个元素。那么作者在论文中所提到的网格划分是怎么体现的呢，这里要通过最后一个卷积层的输出来看，即7x7x1024，可以看到特征图的尺寸是7x7，那么根据卷积网络的特点，每一层的输出特征图上的每一个像素点都会对应着输入特征图的一个区域，也就是这个像素点的<strong>感受野</strong>，那么在最后一层卷积层输出特征图上，也是如此，所以我们可以认为是<strong>将原图划分成了7X7的网格区域，每个网格预测20个类别的概率，目标框置信度以及两个目标框信息，其中每个目标的中心位置都会转换至网格区域内</strong>。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20180130221547656.png" alt="img"></p><p>另外要说明的是，我在最新的github版本中发现，最后两个全连接层被替换成了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[local]</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=256</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[dropout]</span><br><span class="line">probability=.5</span><br><span class="line"></span><br><span class="line">[connected]</span><br><span class="line">output= 1715</span><br><span class="line">activation=linear</span><br></pre></td></tr></table></figure><p>其中的<code>connected</code>不用多说，就是全连接层，只不过节点数变成了1714,即7x7x35，那么这个35则说明每个网格区域会输出：20个类别概率，3个目标框置信度，3个目标框信息包含(x,y,w,h)。对于每个框所包含的物体判别方式则是采用了贝叶斯公式，将上述各个类别的概率作为条件概率。因此每个类别的真实置信度计算方式如下：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}
Confidenc{e_{object}} = P\left( {object} \right) \times IOU_{truth}^{pred}\\
Confidenc{e_{clas{s_i}}} = Confidenc{e_{object}} \times P\left( {\left. {clas{s_i}} \right|object} \right)
\end{array} \right.</script><p>也就是说<strong>每个目标框所输出的边框置信度，本身就包含了先验概率和IOU的乘积</strong>，这一点在YOLOv2论文中有所体现。另外我们还发现YOLOv1中是直接输出目标框的，而不是采用<code>anchor boxes</code>方式。</p><p>而<code>local</code>是用的<code>Locally Connected Layers</code>结构，这种结构跟1x1卷积方式不同，1x1卷积核是利用很多个1x1大小的卷积核遍历整个特征图，而<code>Locally Connected Layers</code>结构则是一个介于全连接和卷积网络之间的一个结构：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4-lc.png" alt="img"></p><p>可以看到，它也是利用卷积的方式进行计算的，不同的地方在于随着卷积的不断遍历，每个遍历位置的卷积核都不一样，即没有了卷积层所特有的共享内存。其好处在于更多的利用了空间相对区域特征以及整体特征，从而提升了一点效果。</p><h3 id="3-2-数据准备"><a href="#3-2-数据准备" class="headerlink" title="3.2 数据准备"></a>3.2 数据准备</h3><p>目前最常用的两个目标检测数据集分别是VOC和COCO，其中VOC数据集中的目标多为大目标,分为20个类别，而COCO数据集中有很多小而密集的目标，更加贴近实际，共80个类别，有几十万幅图像，几百万个目标实例。</p><p>对于YOLO的训练，我们需要将每个目标的信息进行转化，其中一个文件中包含所有目标信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class_label,center_x,center_y,width,height&gt;</span><br></pre></td></tr></table></figure><p>其中目标框信息都需要归一化，即除以对应的图像宽高，另一个文件中则是包含对应图像的地址。</p><h3 id="3-3-数据增强"><a href="#3-3-数据增强" class="headerlink" title="3.3 数据增强"></a>3.3 数据增强</h3><p>作者在训练中主要采用了 jittering 和 HSV 空间扰动两种数据增强方式，详细的过程比较复杂，我用 matlab 把过程复现了：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">clc;clear;close all;</span><br><span class="line"><span class="comment">%% 参数设置</span></span><br><span class="line">jitter = <span class="number">0.3</span>;<span class="comment">%抖动幅度</span></span><br><span class="line">hue = <span class="number">0.1</span>;<span class="comment">%色调变化幅度</span></span><br><span class="line">saturation = <span class="number">1.5</span>;<span class="comment">%饱和度变化幅度</span></span><br><span class="line">exposure = <span class="number">1.5</span>;<span class="comment">%曝光率变化幅度</span></span><br><span class="line">w = <span class="number">416</span>;<span class="comment">%网络输入的宽</span></span><br><span class="line">h = <span class="number">416</span>;<span class="comment">%网络输入的高</span></span><br><span class="line">filepath = <span class="string">'E:\201709\val\val\000e4adcf3a3a5a246351fd4a3e18ae9ac4d44a9.jpg'</span>;<span class="comment">%图片地址</span></span><br><span class="line"><span class="comment">%% jittering+resize</span></span><br><span class="line">I = imread(filepath);<span class="comment">%读取图像</span></span><br><span class="line">[oh,ow,~] = <span class="built_in">size</span>(I);<span class="comment">%读取原图宽高</span></span><br><span class="line">[dw,dh] = deal(<span class="built_in">floor</span>(ow*jitter),<span class="built_in">floor</span>(oh*jitter));<span class="comment">%计算抖动值上限</span></span><br><span class="line">pleft = <span class="built_in">floor</span>(-dw + <span class="number">2</span>*<span class="built_in">rand</span>*dw);<span class="comment">%随机化 left 抖动值</span></span><br><span class="line">pright = <span class="built_in">floor</span>(-dw + <span class="number">2</span>*<span class="built_in">rand</span>*dw);<span class="comment">%随机化 right 抖动值</span></span><br><span class="line">ptop = <span class="built_in">floor</span>(-dh + <span class="number">2</span>*<span class="built_in">rand</span>*dh);<span class="comment">%随机化 top 抖动值</span></span><br><span class="line">pbot = <span class="built_in">floor</span>(-dh + <span class="number">2</span>*<span class="built_in">rand</span>*dh);<span class="comment">%随机化 bot 抖动值</span></span><br><span class="line">swidth = ow - pleft - pright;<span class="comment">%计算抖动后的图像宽度</span></span><br><span class="line">sheight = oh - ptop - pbot;<span class="comment">%计算抖动后的图像高度</span></span><br><span class="line">sx = swidth/ow;<span class="comment">%计算 jittering 后图像宽度与原图的比例</span></span><br><span class="line">sy = sheight/oh;<span class="comment">%计算 jittering 后图像高度与原图的比例</span></span><br><span class="line"><span class="comment">%各向同性 crop</span></span><br><span class="line">crop_image = uint8(<span class="built_in">zeros</span>(sheight,swidth,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : sheight</span><br><span class="line">	<span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : swidth</span><br><span class="line">		<span class="keyword">for</span> k = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line">            r = <span class="built_in">max</span>(<span class="built_in">i</span> + ptop,<span class="number">1</span>);</span><br><span class="line">            r = <span class="built_in">min</span>(r,oh);</span><br><span class="line">            c = <span class="built_in">max</span>(<span class="built_in">j</span> + pleft,<span class="number">1</span>);</span><br><span class="line">            c = <span class="built_in">min</span>(c,ow);</span><br><span class="line">            crop_image(<span class="built_in">i</span>,<span class="built_in">j</span>,k) = I(r,c,k);</span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%图像大小调整 resize,双线性插值</span></span><br><span class="line">w_scale = (swidth<span class="number">-1</span>)/w;<span class="comment">%待变换宽尺度</span></span><br><span class="line">h_scale = (sheight<span class="number">-1</span>)/h;<span class="comment">%待变换高尺度</span></span><br><span class="line">resized_image = uint8(<span class="built_in">zeros</span>(w,h,<span class="number">3</span>));</span><br><span class="line">part = uint8(<span class="built_in">zeros</span>(w,sheight,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line">	<span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : sheight</span><br><span class="line">		<span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line">			<span class="keyword">if</span> k == w</span><br><span class="line">				part(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = crop_image(<span class="built_in">j</span>,swidth,<span class="built_in">i</span>);</span><br><span class="line">			<span class="keyword">else</span></span><br><span class="line">				sx = k*w_scale;</span><br><span class="line">				ix = <span class="built_in">floor</span>(sx);</span><br><span class="line">                  dx = sx-ix;</span><br><span class="line">                  part(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = (<span class="number">1</span>-dx)*crop_image(<span class="built_in">j</span>,ix,<span class="built_in">i</span>)+dx*crop_image(<span class="built_in">j</span>,ix+<span class="number">1</span>,<span class="built_in">i</span>);</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> :<span class="number">3</span></span><br><span class="line">	<span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : h</span><br><span class="line">        sy = <span class="built_in">j</span>*h_scale;</span><br><span class="line">        iy = <span class="built_in">floor</span>(sy);</span><br><span class="line">        dy = sy - iy;</span><br><span class="line">        <span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line">        	resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = (<span class="number">1</span>-dy)*part(iy,k,<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">j</span> &lt; h</span><br><span class="line">			<span class="keyword">for</span> k = <span class="number">1</span> : w</span><br><span class="line">				resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>) = resized_image(<span class="built_in">j</span>,k,<span class="built_in">i</span>)+dy*part(iy+<span class="number">1</span>,k,<span class="built_in">i</span>);</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%% 方式二 jittering+resizeing(随机)</span></span><br><span class="line">I = imread(filepath);<span class="comment">%读取图像</span></span><br><span class="line">[oh,ow,~] = <span class="built_in">size</span>(I);<span class="comment">%读取原图宽高</span></span><br><span class="line">[dw,dh] = deal(ow*jitter,oh*jitter);<span class="comment">%计算抖动值上限</span></span><br><span class="line">aspect_ratio = (ow-dw+<span class="number">2</span>*dw*<span class="built_in">rand</span>)/(oh-dh+<span class="number">2</span>*dh*<span class="built_in">rand</span>);<span class="comment">%计算 jittering 后的长宽比</span></span><br><span class="line">scale = <span class="number">0.25</span> + <span class="number">1.75</span>*<span class="built_in">rand</span>;<span class="comment">%对标准输入大小进行随机放缩，然后保证放缩后长宽比</span></span><br><span class="line"><span class="keyword">if</span> aspect_ratio &lt; <span class="number">1</span></span><br><span class="line">    nh = <span class="built_in">floor</span>(scale*h);</span><br><span class="line">    nw = <span class="built_in">floor</span>(nh*aspect_ratio);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    nw = <span class="built_in">floor</span>(scale*w);</span><br><span class="line">    nh = <span class="built_in">floor</span>(nw/aspect_ratio);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">[Dx,Dy] = deal(<span class="built_in">floor</span>((w-nw)*<span class="built_in">rand</span>),<span class="built_in">floor</span>((h-nh)*<span class="built_in">rand</span>));</span><br><span class="line">resized_image2 = uint8(<span class="number">0.5</span>*<span class="built_in">ones</span>(h,w,<span class="number">3</span>));</span><br><span class="line"><span class="keyword">for</span> c = <span class="number">1</span> : <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> y = <span class="number">1</span> : nh</span><br><span class="line">        <span class="keyword">for</span> x = <span class="number">1</span> : nw</span><br><span class="line">            frx = x/nw*ow;</span><br><span class="line">            fry = y/nh*oh;</span><br><span class="line">            rx = <span class="built_in">floor</span>(frx);</span><br><span class="line">            ry = <span class="built_in">floor</span>(fry);</span><br><span class="line">            dx = frx - rx;</span><br><span class="line">            dy = fry - ry;</span><br><span class="line">            val = (<span class="number">1</span>-dy)*(<span class="number">1</span>-dx)*get_pixel(I,ry,rx,c)+dy*(<span class="number">1</span>-dx)*get_pixel(I,ry+<span class="number">1</span>,rx,c)+... (<span class="number">1</span>-dy)*dx*get_pixel(I,ry,rx+<span class="number">1</span>,c)+dy*dx*get_pixel(I,ry+<span class="number">1</span>,rx+<span class="number">1</span>,c);</span><br><span class="line">			<span class="keyword">if</span> x+Dx&gt;<span class="number">0</span>&amp;&amp;x+Dx&lt;=w&amp;&amp;y+Dy&gt;<span class="number">0</span>&amp;&amp;y+Dy&lt;=h</span><br><span class="line">				resized_image2(y+Dy,x+Dx,c) = val;</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">1</span>)</span><br><span class="line">subplot(<span class="number">221</span>);imshow(I);title(<span class="string">'原图'</span>);</span><br><span class="line">subplot(<span class="number">222</span>);imshow(crop_image);title(<span class="string">'方式一 jittering'</span>);</span><br><span class="line">subplot(<span class="number">223</span>);imshow(resized_image);title(<span class="string">'方式一各向异性 resize'</span>);</span><br><span class="line">subplot(<span class="number">224</span>);imshow(resized_image2);title(<span class="string">'方式二 jittering+各向同性 resize'</span>);</span><br><span class="line"><span class="comment">%% 翻转</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">	J = <span class="built_in">fliplr</span>(resized_image2);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">2</span>)</span><br><span class="line">imshow(J);</span><br><span class="line"><span class="comment">%% HSV 空间扰动</span></span><br><span class="line">dhue = -hue+<span class="number">2</span>*<span class="built_in">rand</span>*hue;<span class="comment">%随机化色调偏差</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">	dsat = <span class="number">1</span> + <span class="built_in">rand</span>*(saturation<span class="number">-1</span>);<span class="comment">%随机化饱和度偏差</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	dsat = <span class="number">1</span>/(<span class="number">1</span> + <span class="built_in">rand</span>*(saturation<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> randi([<span class="number">0</span> <span class="number">1</span>])</span><br><span class="line">	dexp = <span class="number">1</span> + <span class="built_in">rand</span>*(exposure<span class="number">-1</span>);<span class="comment">%随机化曝光率偏差</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	dexp = <span class="number">1</span>/(<span class="number">1</span> + <span class="built_in">rand</span>*(exposure<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">J = <span class="built_in">rgb2hsv</span>(J);<span class="comment">%将 RGB 空间转换到 HSV 空间</span></span><br><span class="line">temp = J(:,:,<span class="number">1</span>)+dhue;<span class="comment">%调整色调</span></span><br><span class="line">temp(temp&gt;<span class="number">1</span>) = temp(temp&gt;<span class="number">1</span>)<span class="number">-1</span>;</span><br><span class="line">temp(temp&lt;<span class="number">0</span>) = temp(temp&lt;<span class="number">0</span>)+<span class="number">1</span>;</span><br><span class="line">J(:,:,<span class="number">1</span>) = temp;</span><br><span class="line">J(:,:,<span class="number">2</span>) = J(:,:,<span class="number">2</span>)*dsat;<span class="comment">%调整饱和度</span></span><br><span class="line">J(:,:,<span class="number">3</span>) = J(:,:,<span class="number">3</span>)*dexp;<span class="comment">%调整曝光率/亮度</span></span><br><span class="line">J = <span class="built_in">hsv2rgb</span>(J);<span class="comment">%返回 RGB 空间</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">3</span>)</span><br><span class="line">imshow(J);title(<span class="string">'HSV 空间扰动'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">pixel</span> = <span class="title">get_pixel</span><span class="params">(image,i,j,c)</span></span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">i</span> &lt; <span class="number">1</span>||<span class="built_in">i</span>&gt;<span class="built_in">size</span>(image,<span class="number">1</span>)||<span class="built_in">j</span>&lt;<span class="number">1</span>||<span class="built_in">j</span>&gt;<span class="built_in">size</span>(image,<span class="number">2</span>)</span><br><span class="line">	pixel = uint8(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	pixel = image(<span class="built_in">i</span>,<span class="built_in">j</span>,c);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">%Matlab 中使用工具箱函数 imresize 会更快：</span></span><br><span class="line"><span class="comment">%imresize(I,[w,h],’bilinear’)%双线性插值</span></span><br><span class="line"><span class="comment">%imresize(I,[w,h],’bicubic’)%双三次线性插值</span></span><br></pre></td></tr></table></figure><p>作者在 YOLO 和 YOLOv2 分别用了两种实现方法，第一种将 jittering 和 resize 分开了，采用双线性插值的方式，第二种则是将二者结合了，先在原图获取一定比例的亚像素值，然后再进行随机双线性插值，效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645871959.png" alt="1549645871959"></p><p>可以看到，方式一的 jittering 会将边界像素进行复制扩充，并且不会内部像素会进行重排，所以是各向异性 resize。而方式二就好像在保证原始图像比例的前提下，通过填充 0 像素达到规定尺寸，所以是各向同性 resize。然后随机将图像进行左右翻转，最后就是 HSV 空间扰动，具体原理还是直接看代码，由于每次随机的值都不一样，所以下面的图可能与上面的不是一致的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549645895056.png" alt="1549645895056"></p><h3 id="3-4-训练技巧"><a href="#3-4-训练技巧" class="headerlink" title="3.4 训练技巧"></a>3.4 训练技巧</h3><p>YOLO训练过程中采用了很多技巧，具体如下：</p><ul><li><p><strong>采用Leaky Relu激活函数</strong></p><script type="math/tex;mode=display">leaky(x) = \left\{ \begin{array}{l}
  x,\;\;\;\;\;x > 0\\
  0.1x,x \le 0
  \end{array} \right.</script></li><li><p><strong>损失函数</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/3.png" alt="img"></p><p>损失函数整体分为定位误差和分类误差（图中的中心位置x,y部分有错误），其中</p><p>(1)第一部分表示当区域内存在目标，且也检测到了匹配目标的前提下，计算目标框中心的均方误差，定位权重为5；</p><p>(2)第二部分就是当区域内存在目标，且也检测到了匹配目标的前提下，计算的目标框宽高的均方误差，定位权重为5，实际上作者训练的时候输出的就是宽高开平方后的结果。这里注意用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大。举个例子，原来 w=10，h=20，预测出来 w=8，h=22，跟原来 w=3，h=5，预测出来其实前者的误差要比后者下，但是如果不加开根号，那么损失都是一样：4+4=8，但是加上根号后，变成 0.15和 0.7；</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/4.png" alt="img"></p><p>(3)第三部分是分别计算当区域内真实目标和与之匹配的预测目标同时存在和不同时存在的情况下，边框置信度的均方误差，其中${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}<br>\over C} _i}$表示的是真实目标框与预测目标框的IOU值。然而，大部分边界框都没有物体，积少成多，造成loss的不平衡，所以同时存在状态下的分类权重为1，不同时存在状态下的分类权重为0.5；</p><p>(4)第四部分计算的是当该区域存在目标时，计算目标类别均方误差。对于每个格子而言，作者设计只能包含同种物体。若格子中包含物体，我们希望希望预测正确的类别的概率越接近于1越好，而错误类别的概率越接近于0越好。</p><p><strong>其中对于预测目标和groundtruth的匹配，作者除了利用IOU进行匹配之外，对于无匹配对象的ground，则是取与其(x,y,w,h)均方误差最小的目标框作为匹配对象</strong></p></li><li><p><strong>非极大值抑制</strong></p><p>非极大值抑制(NMS)算法是目标检测领域中不可或缺的一个算法，其主要功能在于目标去重。而主要依据在于目标框之间的IOU以及每个目标框的置信度，即保证在IOU较大的目标群中选择置信度最高的目标框作为该目标群唯一的预测输出。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549648009461.png" alt="1549648009461"></p><p>YOLO所采用的NMS算法流程如下：</p><ul><li><p>Step1 在网络输出结果之后，会得到7x7x2=98个目标框，首先会根据阈值将prob不合格(大概率属于背景)的目标框置信度置为0；</p></li><li><p>Step2 对于每一个类别分开处理，先根据每个目标框在该类别下的prob置信度进行从大到小排序；</p></li><li><p>Step3 对于排序好的第一个置信度不为0的目标框，依次计算与其他置信度不为0的目标框的IOU，如果IOU大于阈值，则将该目标框置信度置为0，且其对应的所有类别prob都置为0；</p></li><li><p>Step4 转移至下一个置信度不为0的目标框，重复Step3，直到下一步没有置信度非0的目标框为止；</p></li><li><p>Step5 重复Step2</p></li><li><p>Step5 输出所有置信度非0的目标框，并根据阈值筛选有效目标。</p></li></ul></li></ul><p>部分C代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">nms_comparator</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *pa, <span class="keyword">const</span> <span class="keyword">void</span> *pb)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    detection a = *(detection *)pa;</span><br><span class="line">    detection b = *(detection *)pb;</span><br><span class="line">    <span class="keyword">float</span> diff = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(b.sort_class &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        diff = a.prob[b.sort_class] - b.prob[b.sort_class];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        diff = a.objectness - b.objectness;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(diff &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(diff &gt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_nms_sort</span><span class="params">(detection *dets, <span class="keyword">int</span> total, <span class="keyword">int</span> classes, <span class="keyword">float</span> thresh)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    k = total<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= k; ++i)&#123;</span><br><span class="line">        <span class="keyword">if</span>(dets[i].objectness == <span class="number">0</span>)&#123;</span><br><span class="line">            detection swap = dets[i];</span><br><span class="line">            dets[i] = dets[k];</span><br><span class="line">            dets[k] = swap;</span><br><span class="line">            --k;</span><br><span class="line">            --i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    total = k+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; classes; ++k)&#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; total; ++i)&#123;</span><br><span class="line">            dets[i].sort_class = k;</span><br><span class="line">        &#125;</span><br><span class="line">        qsort(dets, total, <span class="keyword">sizeof</span>(detection), nms_comparator);</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; total; ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(dets[i].prob[k] == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            box a = dets[i].bbox;</span><br><span class="line">            <span class="keyword">for</span>(j = i+<span class="number">1</span>; j &lt; total; ++j)&#123;</span><br><span class="line">                box b = dets[j].bbox;</span><br><span class="line">                <span class="keyword">if</span> (box_iou(a, b) &gt; thresh)&#123;</span><br><span class="line">                    dets[j].prob[k] = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549650071906.png" alt="1549650071906"></p><p>这里要注意的是YOLOv1中并没有用到softmax，所以可能同一个目标框的多个类别的置信度都很高。</p><ul><li><p><strong>dropout</strong></p><p>为了减少过拟合概率，YOLOv1中采用的是dropout方式，即在第一个全连接层/局部连接层后利用dropout随机将特征图中的一部分特征丢失。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549695294511.png" alt="1549695294511"></p><p>关于dropout的实现，其主要依据的是<code>drop_probability</code>，即丢失比例/概率，不同框架的实现方式不同，例如caffe：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> DropoutLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span>* mask = rand_vec_.mutable_cpu_data();</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;phase_ == TRAIN) &#123;</span><br><span class="line">    <span class="comment">// Create random numbers</span></span><br><span class="line">    caffe_rng_bernoulli(count, <span class="number">1.</span> - threshold_, mask);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">      top_data[i] = bottom_data[i] * mask[i] * scale_;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    caffe_copy(bottom[<span class="number">0</span>]-&gt;count(), bottom_data, top_data);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其在测试环节直接将输入映射到输出，而在训练环节则是利用伯努利分布按照<code>1-drop_probability</code>的比例选取特征图中的特征，得到一个mask，然后乘以一定比例：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}
	mask \sim Bernoulli\left( {1 - drop\_probability,N} \right)\\
	{f_i} = {f_i} \times mas{k_i} \times \frac{1}{{1 - drop\_probability}}
	\end{array} \right.</script><p>而在Darknet中，其实现方式是：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_dropout_layer</span><span class="params">(dropout_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">if</span> (!net.train) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch * l.inputs; ++i)&#123;</span><br><span class="line">        <span class="keyword">float</span> r = rand_uniform(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        l.rand[i] = r;</span><br><span class="line">        <span class="keyword">if</span>(r &lt; l.probability) net.input[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> net.input[i] *= l.scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现原理是一样的，不过其实现方式更加清晰，缺点在于需要重复生成很多无效随机数，以及没有向量加速。</p></li><li><p><strong>学习率</strong></p><p>YOLOv1采用的是<code>multistep</code>变化方式，即在特定的迭代次数更新学习率，论文中的YOLOv1采用的是学习率逐渐衰减的方式，但有意思的是最新的YOLOv1中学习率变化过程不同于传统的逐渐衰减方式，而是类似于当前新兴的<code>warmup/warmrestart</code>变化方式。我们知道传统的训练模式下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/u=212751988,185815275&amp;fm=173&amp;s=0942F5122D5ED5CE18C595DA000050B3&amp;w=459&amp;h=414&amp;img.JPEG" alt="img"></p><p>不断衰减的学习率可以稳定收敛，但是现在发现在某些模型训练过程中收敛效果并不好，所以就有研究者提出了“热重启”策略，当然也有类似的“循环”策略：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697404615.png" alt="1549697404615"></p><p>那么在YOLOv1中的学习率设定是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=0.0005</span><br><span class="line">policy=steps</span><br><span class="line">steps=200,400,600,20000,30000</span><br><span class="line">scales=2.5,2,2,.1,.1</span><br></pre></td></tr></table></figure><p>可以看到，初始学习率只有0.0005，先慢慢增大，然后逐渐变小。</p></li></ul><h3 id="3-5-测试效果"><a href="#3-5-测试效果" class="headerlink" title="3.5 测试效果"></a>3.5 测试效果</h3><p>先放一张原论文中的实验效果图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697673711.png" alt="1549697673711"></p><p>我们可以看到YOLOv1这个典型的<code>one-stage</code>目标检测算法，在速度大幅领先的前提下，只损失了7%的精度，并且值得注意的是YOLO相对于Fast RCNN将目标误识别为背景的概率小很多，所以作者做了一个小尝试，即以YOLO本身预测为主，逐个对比Fast RCNN和YOLO预测的目标框，根据置信度进行选择，也就是下面<code>Fast R-CNN + YOLO</code>的embedding组合。那么对于VOC各个类别的定位精度：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549697855804.png" alt="1549697855804"></p><p>其对于稀疏大目标的定位效果还是能接受的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698204272.png" alt="1549698204272"></p><h3 id="3-6-优缺点"><a href="#3-6-优缺点" class="headerlink" title="3.6 优缺点"></a>3.6 优缺点</h3><p>YOLO的优点不用多说，其作为当时最快的实时目标检测算法横空出世，SSD是在其后出来的，正式打开了<code>one-stage</code>目标检测算法的大门，虽然精度仍然不如Faster RCNN等，但是其速度很高，适合工程界研究改造。</p><p>当然其缺点也有很多：</p><ul><li><p>1.YOLOv1采用了7x7的网格划分模式，每个网格只能预测两个同类别的目标框，那么就无法预测密集场景下的目标位置，如：拥挤人群；</p></li><li><p>2.YOLOv1的网格划分方式会影响每个目标的边界定位准确度，因为目标一般是跨网格区域的，如果目标只有一小部分在某个网格，那么可能就会被忽略；</p></li><li><p>3.NMS本身漏洞，NMS会将相邻的目标框去重，那么就会出现下面的情况：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698728695.png" alt="1549698728695"></p><p>另外，由于置信度和IOU并不是强相关的，那么对于下面的情况，则不得不选择更差的目标框：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549698809190.png" alt="1549698809190"></p></li><li><p>4.固定分辨率，由于YOLOv1中存在全连接层，所以输入的分辨率必须固定，那么对于YOLOv1所固定的448x448大小分辨率，很多大分辨率图像中的目标会变得很小，另外许多非正方形分辨率的图像目标会失真。</p></li></ul><h2 id="4-YOLOv2"><a href="#4-YOLOv2" class="headerlink" title="4.YOLOv2"></a>4.YOLOv2</h2><p>YOLOv2也被称作YOLO9000，其相对于YOLOv1提升了很多，正如作者所说：<code>Better、Faster、Stronger</code>。其行文很容易理解，我们直接通过创新点来了解其与YOLOv1的区别。</p><h3 id="4-1-网络结构改变"><a href="#4-1-网络结构改变" class="headerlink" title="4.1 网络结构改变"></a>4.1 网络结构改变</h3><p>YOLOv2的网络结构做了较大的改变，主要有:</p><ul><li><p><strong>Training for classfication—Darknet19</strong></p><p>YOLOv2在YOLOv1中的backbone网络基础上借鉴了VGG网络中的卷积方式，利用多个小卷积核替代大卷积核，并且利用1x1卷积核代替全连接层，这样做的好处的特征图每个位置共享参数，然后利用卷积核个数弥补参数组合多样性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   256 x 256 x   3   -&gt;   256 x 256 x  32  0.113 BFLOPs</span><br><span class="line">    1 max          2 x 2 / 2   256 x 256 x  32   -&gt;   128 x 128 x  32</span><br><span class="line">    2 conv     64  3 x 3 / 1   128 x 128 x  32   -&gt;   128 x 128 x  64  0.604 BFLOPs</span><br><span class="line">    3 max          2 x 2 / 2   128 x 128 x  64   -&gt;    64 x  64 x  64</span><br><span class="line">    4 conv    128  3 x 3 / 1    64 x  64 x  64   -&gt;    64 x  64 x 128  0.604 BFLOPs</span><br><span class="line">    5 conv     64  1 x 1 / 1    64 x  64 x 128   -&gt;    64 x  64 x  64  0.067 BFLOPs</span><br><span class="line">    6 conv    128  3 x 3 / 1    64 x  64 x  64   -&gt;    64 x  64 x 128  0.604 BFLOPs</span><br><span class="line">    7 max          2 x 2 / 2    64 x  64 x 128   -&gt;    32 x  32 x 128</span><br><span class="line">    8 conv    256  3 x 3 / 1    32 x  32 x 128   -&gt;    32 x  32 x 256  0.604 BFLOPs</span><br><span class="line">    9 conv    128  1 x 1 / 1    32 x  32 x 256   -&gt;    32 x  32 x 128  0.067 BFLOPs</span><br><span class="line">   10 conv    256  3 x 3 / 1    32 x  32 x 128   -&gt;    32 x  32 x 256  0.604 BFLOPs</span><br><span class="line">   11 max          2 x 2 / 2    32 x  32 x 256   -&gt;    16 x  16 x 256</span><br><span class="line">   12 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   13 conv    256  1 x 1 / 1    16 x  16 x 512   -&gt;    16 x  16 x 256  0.067 BFLOPs</span><br><span class="line">   14 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   15 conv    256  1 x 1 / 1    16 x  16 x 512   -&gt;    16 x  16 x 256  0.067 BFLOPs</span><br><span class="line">   16 conv    512  3 x 3 / 1    16 x  16 x 256   -&gt;    16 x  16 x 512  0.604 BFLOPs</span><br><span class="line">   17 max          2 x 2 / 2    16 x  16 x 512   -&gt;     8 x   8 x 512</span><br><span class="line">   18 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   19 conv    512  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x 512  0.067 BFLOPs</span><br><span class="line">   20 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   21 conv    512  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x 512  0.067 BFLOPs</span><br><span class="line">   22 conv   1024  3 x 3 / 1     8 x   8 x 512   -&gt;     8 x   8 x1024  0.604 BFLOPs</span><br><span class="line">   23 conv   1000  1 x 1 / 1     8 x   8 x1024   -&gt;     8 x   8 x1000  0.131 BFLOPs</span><br><span class="line">   24 avg                        8 x   8 x1000   -&gt;  1000</span><br><span class="line">   25 softmax                                        1000</span><br></pre></td></tr></table></figure><p>我们可以看到其中有19个卷积层和5个<code>max pooling</code>层，所以称其为<code>Darknet19</code>。作者利用该网络重新再Imagenet上训练了，相对于yolov1中的backbone网络，参数量更少，计算速度更快，效果更好。</p></li><li><p><strong>Training for Detection</strong></p><p>有了backbone骨干网络之后，作者剔除了<code>Darknet19</code>的最后一个卷积层，然后额外添加了几个卷积层：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers=-9</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=64</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[reorg]</span><br><span class="line">stride=2</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers=-1,-4</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=425</span><br><span class="line">activation=linear</span><br></pre></td></tr></table></figure><p>我们可以看到其中出现了两个新层<code>reorg</code>和<code>route</code>，这两个层的意义在于：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702344040.png" alt="1549702344040"></p><p>可以也就是说，<code>route</code>的层的作用就是将选定层按照通道拼接在一起，而<code>reorg</code>层的作用就是将特征图均匀划分为 4 份，从而使得两组特征图可以拼接。大致原理如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549702478831.png" alt="1549702478831"></p><p>最终的网络结构为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   416 x 416 x   3   -&gt;   416 x 416 x  32</span><br><span class="line">    1 max          2 x 2 / 2   416 x 416 x  32   -&gt;   208 x 208 x  32</span><br><span class="line">    2 conv     64  3 x 3 / 1   208 x 208 x  32   -&gt;   208 x 208 x  64</span><br><span class="line">    3 max          2 x 2 / 2   208 x 208 x  64   -&gt;   104 x 104 x  64</span><br><span class="line">    4 conv    128  3 x 3 / 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    5 conv     64  1 x 1 / 1   104 x 104 x 128   -&gt;   104 x 104 x  64</span><br><span class="line">    6 conv    128  3 x 3 / 1   104 x 104 x  64   -&gt;   104 x 104 x 128</span><br><span class="line">    7 max          2 x 2 / 2   104 x 104 x 128   -&gt;    52 x  52 x 128</span><br><span class="line">    8 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">    9 conv    128  1 x 1 / 1    52 x  52 x 256   -&gt;    52 x  52 x 128</span><br><span class="line">   10 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256</span><br><span class="line">   11 max          2 x 2 / 2    52 x  52 x 256   -&gt;    26 x  26 x 256</span><br><span class="line">   12 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   13 conv    256  1 x 1 / 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   14 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   15 conv    256  1 x 1 / 1    26 x  26 x 512   -&gt;    26 x  26 x 256</span><br><span class="line">   16 conv    512  3 x 3 / 1    26 x  26 x 256   -&gt;    26 x  26 x 512</span><br><span class="line">   17 max          2 x 2 / 2    26 x  26 x 512   -&gt;    13 x  13 x 512</span><br><span class="line">   18 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   19 conv    512  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   20 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   21 conv    512  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 512</span><br><span class="line">   22 conv   1024  3 x 3 / 1    13 x  13 x 512   -&gt;    13 x  13 x1024</span><br><span class="line">   23 conv   1024  3 x 3 / 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   24 conv   1024  3 x 3 / 1    13 x  13 x1024   -&gt;    13 x  13 x1024</span><br><span class="line">   25 route  16                                                             </span><br><span class="line">   26 reorg              / 2    26 x  26 x 512   -&gt;    13 x  13 x2048       </span><br><span class="line">   27 route  26 24                                                      </span><br><span class="line">   28 conv   1024  3 x 3 / 1    13 x  13 x3072   -&gt;    13 x  13 x1024</span><br><span class="line">   29 conv    425  1 x 1 / 1    13 x  13 x1024   -&gt;    13 x  13 x 425</span><br><span class="line">   30 detection</span><br></pre></td></tr></table></figure><p><strong>这里输出的425指的是：每个网格输出5个目标框，每个目标框包含COCO的80个类别概率，一个边框置信度，以及(tx,ty,tw,th)，即5x(80+1+4)。</strong></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181226152903109.png" alt="img"></p></li><li><p><strong>全卷积网络</strong></p><p>综上可知，YOLOv2是一个全卷积网络，与YOLOv1相同，利用感受野的概念，我们可以认为是将原图划分为了多个网格区域，且区域半径为32，所以说输入大小必须是32的倍数。另外全卷积网络的好处在于可以有任意分辨率的输入，因为全连接层参数依赖前后两层的尺寸，而卷积层参数只有卷积核，与前后层尺寸无关，所以更为方便了。</p></li></ul><h3 id="4-2-batch-normalization"><a href="#4-2-batch-normalization" class="headerlink" title="4.2 batch normalization"></a>4.2 batch normalization</h3><p>相对于YOLOv1，YOLOv2将<code>dropout</code>替换成了效果更好的<code>batch normalization</code>，在每个卷积层计算之前利用<code>batch normalization</code>进行批归一化：</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}
{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
\over x} }^k} = \frac{{{x^k} - E\left[ {{x^k}} \right]}}{{\sqrt {Var\left[ {{x^k}} \right]} }}\\
{y^k} = {\gamma ^k}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
\over x} }^k} + {\beta ^k}
\end{array} \right.</script><h3 id="4-3-Multi-Scale-Training"><a href="#4-3-Multi-Scale-Training" class="headerlink" title="4.3 Multi-Scale Training"></a>4.3 Multi-Scale Training</h3><p>为了让网络能适应不同分辨率的输入，在训练过程中，每个10个batches会随机选择一种分辨率输入，即利用图像插值对图像进行放缩，由于训练速度的要求以及分辨率必须是32倍数，所以训练过程中选择的分辨率分别为：320, 352, …, 608。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549703794254.png" alt="1549703794254"></p><h3 id="4-4-Anchor-boxes"><a href="#4-4-Anchor-boxes" class="headerlink" title="4.4 Anchor boxes"></a>4.4 Anchor boxes</h3><p>YOLOv2相对于YOLOv1的定位架构最大的改变在于剔除了<code>anchor boxes</code>概念，具体见第一章，从直接预测目标相对网格区域的偏移量到预测<code>anchor box</code>的修正量，有了先验长宽比的约束，可以减少很多不规则的目标定位。</p><script type="math/tex;mode=display">\left\{ \begin{array}{l}
x = {t_x} \times {w_a} + {x_a}\\
y = {t_y} \times {h_a} + {y_a}\\
w = {t_w} \times img\_width\\
h = {t_h} \times img\_height
\end{array} \right. \Rightarrow \left\{ \begin{array}{l}
{b_x} = \sigma \left( {{t_x}} \right) + {c_x}\\
{b_y} = \sigma \left( {{t_y}} \right) + {c_y}\\
{b_w} = {p_w}{e^{{t_w}}}\\
{b_h} = {p_h}{e^{{t_h}}}\\
Pr\left( {object} \right) * IOU\left( {b,object} \right) = \sigma \left( {{t_o}} \right)
\end{array} \right.</script><p>其中，<code>p</code>代表的是<code>anchor box</code>的先验值，<code>c</code>表示每个网格区域的左上角顶点，<code>t</code>表示网络输出的目标框的5个参数(tx,ty,tw,th,to),<code>b</code>表示真实预测定位信息。而最后一个关于先验概率的等价关系我们可以知道，与YOLOv1相同，这里目标预测的边框置信度包含了IOU先验值。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549705872475.png" alt="1549705872475"></p><p>其中对于部分输出进行了logistic转换：</p><script type="math/tex;mode=display">\sigma \left( x \right) = \frac{1}{{1 + x}}</script><p>对于先验<code>anchor boxes</code>的确定，作者通过 K-means 的方法对 VOC 和 COCO 数据集所有框的标签进行聚类，最后发现anchor box 在仅有 5 种 aspect ratio 的情况下就能达到足够的效果，当然，作者也试着将 K 提升到 9 个，发现效果更好。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549704037718.png" alt="1549704037718"></p><h3 id="4-5-其他训练技巧"><a href="#4-5-其他训练技巧" class="headerlink" title="4.5 其他训练技巧"></a>4.5 其他训练技巧</h3><ul><li><p><strong>softmax</strong></p><p>YOLOv2中对于每个类别的概率输出进行了softmax归一化。</p></li><li><p><strong>学习率</strong></p><p>YOLOv2的学习率变化方式与YOLOv1类似：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=0.001</span><br><span class="line">burn_in=1000</span><br><span class="line">max_batches = 500200</span><br><span class="line">policy=steps</span><br><span class="line">steps=400000,450000</span><br><span class="line">scales=.1,.1</span><br></pre></td></tr></table></figure><p>只不过多了一个<code>burn_in</code>参数，那么上面的参数设置对应的变化方式为：</p><script type="math/tex;mode=display">learning\_rat{e_{iter}} = \left\{ \begin{array}{l}
	base\_learningrate \times {\left( {iter/burn\_in} \right)^{power}},if\;iter < burn\_in\\
	learningrat{e_{iter - 1}} \times scal{e_j},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;elif\;iter = step{s_j}\\
	learningrat{e_{iter - 1}},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;otherwise
	\end{array} \right.</script></li><li><p><strong>损失函数</strong></p><p>YOLOv2的损失函数类似于YOLOv1，也有所不同，我阅读源码之后总结如下：</p><script type="math/tex;mode=display">\begin{array}{l}
	Loss=\\
	{\lambda _{prior}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{iter < 12800}\left[ {{{\left( {\sigma ({t_{{x_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{y_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{w_{ij}}}}) - {p_{{w_k}}}} \right)}^2} + {{\left( {\sigma ({t_{{h_{ij}}}}) - {p_{{h_k}}}} \right)}^2}} \right]} } } \\
	 + {\lambda _{coord}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{obj}{{\left( {{x_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over x} }_{ijk}}} \right)}^2} + {{\left( {{y_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over y} }_{ijk}}} \right)}^2} + {{\left( {{w_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over w} }_{ijk}}} \right)}^2} + {{\left( {{h_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over h} }_{ijk}}} \right)}^2}} } } \\
	 + {\lambda _{obj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over C} }_{ij}}} \right)^2} + {\lambda _{noobj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{noobj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over C} }_{ij}}} \right)^2}\\
	 + {\lambda _{class}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}\sum\limits_{c \in classes} {{{\left( {{p_{ij}}\left( c \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} 
	\over p} }_{ij}}\left( c \right)} \right)}^2}} } } 
	\end{array}</script><p>可以发现只有定位误差部分的损失函数变化了，其中${\lambda <em>{prior}=0.01},{\lambda </em>{coord}=1},{\lambda <em>{class}=1},{\lambda </em>{obj}=5,{\lambda _{noobj}=1}}$，其中第一部分的意思是当训练的batch数不超过12800个时，尽量让预测目标框靠近每个网格中心，且尺寸与先验anchor box相同。</p></li></ul><h3 id="4-6-联合训练分类和检测"><a href="#4-6-联合训练分类和检测" class="headerlink" title="4.6 联合训练分类和检测"></a>4.6 联合训练分类和检测</h3><p>作者在论文最后提出，可以将分类和检测数据集放在一起训练网络，在遇到分类问题时，就只调用分类部分损失函数，否则调用检测分布损失函数。而对于两类数据集中存在的，类别相互包含的情况，作者则是剔除了<code>Word Tree</code>的概念：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549723273927.png" alt="1549723273927"></p><p>其原理实际上就是，预先构建好所有类别的关系树，然后利用联合概率分布和条件概率等，进行组合，相当于YOLO中对于分类概率和目标置信度的关系。</p><h3 id="4-7-测试效果"><a href="#4-7-测试效果" class="headerlink" title="4.7 测试效果"></a>4.7 测试效果</h3><p>YOLOv2在VOC和COCO上的测试结果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721649962.png" alt="1549721649962"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549721663939.png" alt="1549721663939"></p><p>我们可以看到，此时的YOLOv2的效果已经与Faster RCNN以及新出现的<code>one-stage</code>算法<code>SSD</code>持平，不过YOLOv2依旧保持着遥遥领先的速度优势。</p><h3 id="4-8-优缺点"><a href="#4-8-优缺点" class="headerlink" title="4.8 优缺点"></a>4.8 优缺点</h3><p>YOLOv2相对来说在每个网格内预测了更多的目标框，并且每个目标框可以不用为同一类，而每个目标都有着属于自己的分类概率，这些使得预测结果更加丰富。另外，由于<code>anchor box</code>的加入，使得YOLOv2的定位精度更加准确。不过，其对于YOLOv1的许多问题依旧没有解决，当然那些也是很多目标检测算法的通病。那么随着<code>anchor box</code>的加入所带来的新问题是：</p><ul><li><p><code>anchor box</code>的个数以及参数都属于超参数，因此会影响训练结果；</p></li><li><p>由于<code>anchor box</code>在每个网格内都需要计算一次损失函数，然而每个正确预测的目标框才能匹配一个比较好的先验anchor，也就是说，对于YOLOv2中的5种<code>anchor box</code>，相当于强行引入了4倍多的负样本，在本来就样本不均衡的情况下，加重了不均衡程度，从而使得训练难度增大；</p></li><li><p>由于IOU和NMS的存在，会出现下面的情况：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549724233482.png" alt="1549724233482"></p><p>我们可以看到，当两个人很靠近或重叠时，检测框变成了中间的矩形框，其原因在于对于两个候选框（红，绿），其中红色框可能更加容易受到目标1的影响，而绿色框会同时收到目标1和目标2的影响，从而导致最终定位在中间。然后由于NMS存在，其他的相邻的框则会被剔除。要想避免这种情况，就应该在损失函数中加入相关的判定。</p></li></ul><h2 id="5-YOLOv3"><a href="#5-YOLOv3" class="headerlink" title="5.YOLOv3"></a>5.YOLOv3</h2><h3 id="5-1-Darknet-53"><a href="#5-1-Darknet-53" class="headerlink" title="5.1 Darknet-53"></a>5.1 Darknet-53</h3><p>YOLOv3中又提出了一种新的backbone网络——Darknet-53，其效果还是见第二章的表格图片。其架构如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725186270.png" alt="1549725186270"></p><p>可以看到，新增了<code>Residual</code>模块，不同于原本的Resnet中的残差模块：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/2018071722042637.png" alt="img"></p><p>我怎么感觉作者就是为了加深网络，所以才不得不引入残差模块的…可以看到明显的效果变化：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549725539157.png" alt="1549725539157"></p><h3 id="5-2-网络多尺度输出"><a href="#5-2-网络多尺度输出" class="headerlink" title="5.2 网络多尺度输出"></a>5.2 网络多尺度输出</h3><p>YOLOv3增加了top down 的多级预测，解决了yolo颗粒度粗，对小目标无力的问题。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/606386-20180327004340505-1572852891.png" alt="img"></p><p>可以看到，不仅在不同的感受野范围输出了三种尺度的预测结果，每种预测结果中每个网格包含3个目标框，一共是9个目标框。而且，相邻尺度的网络还存在着级联：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/20181213163753559.png" alt="img"></p><p><strong>DBL</strong>: conv+BN+Leaky relu。</p><p><strong>resn</strong>：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。</p><p><strong>concat</strong>：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</p><p><strong>upsample</strong>：终于把原来的<code>reorg</code>改成了<code>upsample</code>，这里的upsample很暴力，很像克罗内克积，即：</p><script type="math/tex;mode=display">\left[ {\begin{array}{*{20}{c}}
1&2\\
3&4
\end{array}} \right] \Rightarrow \left[ {\begin{array}{*{20}{c}}
1&1&2&2\\
1&1&2&2\\
3&3&4&4\\
3&3&4&4
\end{array}} \right]</script><p>可以看到每个输出的深度都是255，即3x(80+5)。这种多尺度预测的方式应该是参考的FPN算法。</p><h3 id="5-3-Anchor-Boxes改进"><a href="#5-3-Anchor-Boxes改进" class="headerlink" title="5.3 Anchor Boxes改进"></a>5.3 Anchor Boxes改进</h3><p>YOLOv2中是直接预测了目标框相对网格点左上角的偏移，以及<code>anchor box</code>的修正量，而在YOLOv3中同样是利用K-means聚类得到了9组<code>anchor box</code>，只不过YOLOv2中用的是相对比例，而YOLOv3中用的是绝对大小。那么鉴于我们之前提到的<code>anchor box</code>带来的样本不平衡问题，以及绝对大小可能会出现超出图像边界的情况，作者加入了新的判断条件，即对于每个目标预测结果只选择与groundtruth的IOU最大/超过0.5的<code>anchor</code>，不考虑其他的<code>anchor</code>，从而大大减少了样本不均衡情况。</p><h3 id="5-4-分类函数"><a href="#5-4-分类函数" class="headerlink" title="5.4 分类函数"></a>5.4 分类函数</h3><p>YOLOv3中取消了对于分类概率的联合分布softmax，而是采用了logistic函数，因为有一些数据集的中的目标存在多标签，而softmax函数会让各个标签相互抑制。</p><h3 id="5-5-测试效果"><a href="#5-5-测试效果" class="headerlink" title="5.5 测试效果"></a>5.5 测试效果</h3><p>YOLOv3的泛化性能更好了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-fbf72eec750dfd743a15f511872a0974_hd.jpg" alt="img"></p><p>在加入了多尺度预测之后，小尺度目标检测效果更好：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/v2-400c7fed8265bf7e2a5b42a866c6f2a1_hd.jpg" alt="img"></p><p>与其他算法的对比效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20190209/1549729080541.png" alt="1549729080541"></p><h3 id="5-6-展望"><a href="#5-6-展望" class="headerlink" title="5.6 展望"></a>5.6 展望</h3><p>感觉YOLOv3的提升已经很大了，不过一些固有问题还是没有解决，有意思的是，在加入了多尺度预测后，拥挤场景下的目标检测效果更好了。不过基于<code>anchor box</code>的目标检测算法始终都有着瓶颈，寻求更好的出路才是最好的。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://lanbing510.info/2017/08/28/YOLO-SSD.html">http://lanbing510.info/2017/08/28/YOLO-SSD.html</a></li><li><a href="https://pjreddie.com/darknet/">https://pjreddie.com/darknet/</a></li><li><a href="https://blog.csdn.net/m0_37192554/article/details/81092514">https://blog.csdn.net/m0_37192554/article/details/81092514</a></li><li><a href="https://blog.csdn.net/leviopku/article/details/82660381">https://blog.csdn.net/leviopku/article/details/82660381</a></li><li><a href="https://github.com/pjreddie/darknet">https://github.com/pjreddie/darknet</a></li><li>Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</li><li>Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. 2017.</li><li>Redmon J, Farhadi A. YOLOv3: An Incremental Improvement[J]. 2018.</li></ol></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.jpg" alt="黄飘 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="黄飘 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>黄飘</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://huangpiao.tech/2019/02/09/详解YOLO系列算法/" title="详解YOLO系列">https://huangpiao.tech/2019/02/09/详解YOLO系列算法/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/目标检测/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a> <a href="/tags/YOLO/" rel="tag"><i class="fa fa-tag"></i> YOLO</a></div><div class="post-widgets"><div class="social_share"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/02/04/剖析KCF/" rel="next" title="剖析KCF"><i class="fa fa-chevron-left"></i> 剖析KCF</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2019/02/15/矩阵求导/" rel="prev" title="矩阵求导">矩阵求导 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/author.jpg" alt="黄飘"><p class="site-author-name" itemprop="name">黄飘</p><p class="site-description motion-element" itemprop="description">直到这一刻微笑着说话为止，我至少留下了一公升眼泪</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">25</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">12</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">30</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/nightmaredimple" title="GitHub &rarr; https://github.com/nightmaredimple" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/nightmare_dimple" title="CSDN &rarr; https://blog.csdn.net/nightmare_dimple" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="/huangpiao2985@163.com" title="E-Mail &rarr; huangpiao2985@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/huang-piao-72/posts" title="ZhiHu &rarr; https://www.zhihu.com/people/huang-piao-72/posts" rel="noopener" target="_blank"><i class="fa fa-fw fa-zhihu"></i>ZhiHu</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-目标检测简介"><span class="nav-text">1.目标检测简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Darknet"><span class="nav-text">2.Darknet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Darknet网络框架"><span class="nav-text">2.1 Darknet网络框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Darknet训练框架"><span class="nav-text">2.2 Darknet训练框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-YOLOv1"><span class="nav-text">3.YOLOv1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-YOLO网络框架"><span class="nav-text">3.1 YOLO网络框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-数据准备"><span class="nav-text">3.2 数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-数据增强"><span class="nav-text">3.3 数据增强</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-训练技巧"><span class="nav-text">3.4 训练技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-测试效果"><span class="nav-text">3.5 测试效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-优缺点"><span class="nav-text">3.6 优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-YOLOv2"><span class="nav-text">4.YOLOv2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-网络结构改变"><span class="nav-text">4.1 网络结构改变</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-batch-normalization"><span class="nav-text">4.2 batch normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Multi-Scale-Training"><span class="nav-text">4.3 Multi-Scale Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Anchor-boxes"><span class="nav-text">4.4 Anchor boxes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-其他训练技巧"><span class="nav-text">4.5 其他训练技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-联合训练分类和检测"><span class="nav-text">4.6 联合训练分类和检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-测试效果"><span class="nav-text">4.7 测试效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-8-优缺点"><span class="nav-text">4.8 优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-YOLOv3"><span class="nav-text">5.YOLOv3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Darknet-53"><span class="nav-text">5.1 Darknet-53</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-网络多尺度输出"><span class="nav-text">5.2 网络多尺度输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Anchor-Boxes改进"><span class="nav-text">5.3 Anchor Boxes改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-分类函数"><span class="nav-text">5.4 分类函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-测试效果"><span class="nav-text">5.5 测试效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-展望"><span class="nav-text">5.6 展望</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></ol></div></div></div><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">黄飘</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="post.totalcount">65.4k字</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量">总访客量: <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量">总访问数: <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script src="/lib/reading_progress/reading_progress.js"></script><script src="/js/src/utils.js?v=6.7.0"></script><script src="/js/src/motion.js?v=6.7.0"></script><script src="/js/src/affix.js?v=6.7.0"></script><script src="/js/src/schemes/pisces.js?v=6.7.0"></script><script src="/js/src/scrollspy.js?v=6.7.0"></script><script src="/js/src/post-details.js?v=6.7.0"></script><script src="/js/src/bootstrap.js?v=6.7.0"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick";guest=guest.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#comments",verify:!1,notify:!1,appId:"YGYUrzPLBQI94NtQWvTjzrE5-gzGzoHsz",appKey:"aQseSeSwrta45i3gUrcNK2QQ",placeholder:"ヾﾉ≧∀≦)o 来呀！吐槽一番吧！",avatar:"mm",meta:guest,pageSize:"10",visitor:!1})</script><script>function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var e=$("#local-search-input");e.attr("autocapitalize","none"),e.attr("autocorrect","off"),e.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(e){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(e,t,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:e,dataType:isXml?"xml":"json",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():e,r=document.getElementById(t),s=document.getElementById(o),a=function(){var e=r.value.trim().toLowerCase(),t=e.split(/[\s\-]+/);t.length>1&&t.push(e);var o=[];if(e.length>0&&n.forEach(function(n){function r(t,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===e&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(e,t){var o="",n=t.start;return t.hits.forEach(function(t){o+=e.substring(n,t.position);var r=t.position+t.length;o+='<b class="search-keyword">'+e.substring(t.position,r)+"</b>",n=r}),o+=e.substring(n,t.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url).replace(/\/{2,}/g,"/"),d=[],g=[];if(""!=l&&(t.forEach(function(e){function t(e,t,o){var n=e.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(t=t.toLowerCase(),e=e.toLowerCase());(s=t.indexOf(e,r))>-1;)a.push({position:s,word:e}),r=s+n;return a}d=d.concat(t(e,h,!1)),g=g.concat(t(e,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(e){e.sort(function(e,t){return t.position!==e.position?t.position-e.position:e.word.length-t.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;w<0&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hits.length!==t.hits.length?t.hits.length-e.hits.length:e.start-t.start});var T=parseInt("3");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(e){b+="<a href='"+f+'\'><p class="search-result">'+s(p,e)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===t.length&&""===t[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x"></i></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>';else{o.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hitCount!==t.hitCount?t.hitCount-e.hitCount:t.id-e.id});var a='<ul class="search-result-list">';o.forEach(function(e){a+=e.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(e){e.stopPropagation()}),$(document).on("keyup",function(e){var t=27===e.which&&$(".search-popup").is(":visible");t&&onPopupClose()})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0';
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
        extensions: ['[mhchem]/mhchem.js'],
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.MathJax_Display{overflow:auto hidden}</style><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="box",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="box",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><style>.copy-btn{display:inline-block;padding:6px 12px;font-size:13px;font-weight:700;line-height:20px;color:#333;white-space:nowrap;vertical-align:middle;cursor:pointer;background-color:#eee;background-image:linear-gradient(#fcfcfc,#eee);border:1px solid #d5d5d5;border-radius:3px;user-select:none;outline:0}.highlight-wrap .copy-btn{transition:opacity .3s ease-in-out;opacity:0;padding:2px 6px;position:absolute;right:4px;top:8px}.highlight-wrap .copy-btn:focus,.highlight-wrap:hover .copy-btn{opacity:1}.highlight-wrap{position:relative}</style><script>$(".highlight").each(function(e,t){var n=$("<div>").addClass("highlight-wrap");$(t).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(e){var t=$(this).parent().find(".code").find(".line").map(function(e,t){return $(t).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=document.createRange(),a=window.getSelection(),i=window.pageYOffset||document.documentElement.scrollTop;n.style.top=i+"px",n.style.position="absolute",n.style.opacity="0",n.value=t,n.textContent=t,n.contentEditable=!0,n.readOnly=!1,document.body.appendChild(n),o.selectNode(n),a.removeAllRanges(),a.addRange(o),n.setSelectionRange(0,t.length);var d=document.execCommand("copy");d?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(e){var t=$(this).find(".copy-btn");setTimeout(function(){t.text("复制")},300)}).append(t)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html><script type="text/javascript" src="/js/src/clicklove.js"></script><!-- rebuild by neat -->