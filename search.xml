<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CVPR2020 | 基于图卷积GNN的多目标跟踪算法解析]]></title>
    <url>%2F2020%2F06%2F26%2F%E5%9F%BA%E4%BA%8EGNN%E7%9A%84%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言随着这两年GNN的发展，其对于关系的建模特性也被引入了多目标跟踪领域，这次我通过对这两年基于GNN的MOT算法的介绍来分析其特点。相关MOT和数据关联的基础知识可以去我的专栏查看。1.EDA_GNN论文题目： Graph Neural Based End-to-end Data Association Framework for Online Multiple-Object Tracking作者：Xiaolong Jiang, Peizhao Li, Yanjing Li, Xiantong Zhen论文链接：https://arxiv.org/pdf/1907.05315.pdf从算法的示意图可以看到，作者通过一个孪生网络求得了观测目标j与当前目标轨迹i的表观相似度，然后取目标轨迹的历史位置为输入，通过LSTM得到预测的位置，计算该位置与观测目标位置的运动相似度，两个相似度结合构建相似度矩阵。至此，所有目标轨迹与观测目标的相似度构成了一个二部图，以目标和观测信息作为节点，相似度作为边权，表观特征和位置信息拼接作为节点属性特征。然后基于消息传递机制，作者通过GNN的网络框架实现对节点特征的更新：邻接矩阵的normalization采用的是row-wise softmax，即对相似度矩阵进行逐行softmax，可以用注意力的方式来理解，W为待学习的权重。最后通过一个激活函数ReLU实现特征的更新。对于边权关系的更新则是简单地利用MLP将两个节点特征的差转换为标量。在训练的时候，损失函数由三部分组成：其中第一部分是预测得到的关联矩阵的分类损失，第二部分则是将groundtruth中的关联对取出，计算分类损失，第三部分是将新出/消失的目标单独取出，计算MSE损失。2.DAN论文题目： Deep association: End-to-end graph-based learning for multiple object tracking with conv-graph neural network作者：Cong Ma, Yuan Li, Fan Yang, Ziwei Zhang, Yueqing Zhuang, Huizhu Jia, Xiaodong Xie备注信息：ICMR2019论文链接：https://dl.acm.org/doi/pdf/10.1145/3323873.3325010这里的DAN并非我之前提过的DAN，其整理流程跟EDA_GNN基本一样：都是先提取表观和运动特征，由此构建网络图，通过GNN得到最终的关联矩阵。首先相似度矩阵怎这里用的是IOU信息：IOU后面的部分是帧间差，如果目标存在跨帧链接，那么间隔越久，相似度越低。不过DAN与EDA_GNN不同的是，并没有将图结构构建为二部图，而是将跟踪节点和观测节点统一为节点集合，因此邻接矩阵/相似度矩阵就变成了(m+n)x(m+N)，这就是最基础的GNN网络结构了。所以节点特征的更新就是：邻接矩阵的更新为：损失函数为Graph Loss，即对正负链接边权的交叉熵损失函数：结果如下:3.GNMOT论文题目：Graph Networks for Multiple Object Tracking作者：Jiahe Li, Xu Gao, Tingting Jiang备注信息：WACV2020论文链接：https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9093347代码链接：https://github.com/yinizhizhu/GNMOT首先我们看看算法流程：可以看到，GNMOT的不同在于表观和运动部分分别采用了GNN网络，二者结合得到的是相似度矩阵，由此输入数据关联部分。关于GNN网络的更新流程，作者设计了4步：其中第一次边和节点的更新都是通过两层FC进行更新的。第三次的全局更新这里，作者引入了一个全局变量u，先计算所有节点的特征均值和边权均值，再通过两层FC进行更新。这里的u会在出现在所有更新过程中，作为一个调节量。最后一次的边权更新则是在两层FC之后再加了一层softmax层。4.MPN Tracker论文题目：Learning a Neural Solver for Multiple Object Tracking作者：Brasó G, Leal-Taixé L备注信息：CVPR2020论文链接：https://arxiv.org/abs/1912.07515代码链接：https://github.com/selflein/GraphNN-Multi-Object-Tracking我之前也介绍过这篇文章，但是之前不懂GNN，所以只能做搬运工，现在学习了GNN，所以就再次分析一下。首先是图的构建，图节点由所有帧的所有目标构成，直接将观测信息作为节点，没有跟踪，只有关联。节点属性特征由训练得到的表观特征和几何特征构成，其中几何特征为位置和形状。并且定义表观特征距离用欧氏距离度量，几何特征距离用下面的公式度量：时间特征自然就是帧数，这几个特征通过一个MLP网络得到最终的特征表达。边的连接自然就是跨帧节点存在连接，而同一帧节点不存在连接，边权的设定就是上面的距离度量。也就是说，这相当于一个端到端的离线跟踪框架。消息传递机制中，对于边权的更新和节点的更新方式如下：其中对于边的更新就是由节点特征和原始边权通过MLP过程更新的。对于节点的更新，由于一个节点连接有多条边，所以需要进行聚合，聚合方式可以求和、取平均，还可以是取最大值。而更新的代数L自然也就决定了图卷积网络的感受野，当L越大时，与之相关的节点在时间跨度上越大。上面这个图从左往右是不同时间帧的节点，这里举的例子是一个相邻三帧的节点连接。原始的更新机制中，对于节点的更新会将周围边的影响通过求和的方式聚合。而这里作者考虑了时间因素，将时间分为了过去和未来两个部分：然后通过拼接的方式聚合，最后利用MLP结构实现特征降维。可以看下消息传递代数的影响：我们发现在3代的时候就已经达到了性能上限，不过为了保证鲁棒性，作者还是选了12.不得不说离线的方法在IDF1指标上的表现很好：PS：大家可能对于第一张图中的Edge Classification有疑惑，即如何实现的边的稀疏化。这里由于每条边权都经过了一个sigmoid层，因此作者直接利用固定阈值0.5进行了裁剪。5.GNN3DMOT论文题目：Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning作者：Xinshuo Weng, Yongxin Wang, Yunze Man, Kris Kitani备注信息：CVPR2020论文链接：https://arxiv.org/abs/2006.07327代码链接：https://github.com/xinshuoweng/GNN3DMOT从这篇开始，接下来的全是这个组的文章。这篇文章所涉及的算法框架很完整，值得一读：算法流程通过这张图可以很容易得到，运动特征是通过LSTM网络回归得到的，表观特征是通过CNN网络得到的，二者相拼接。其中3D部分的输入则是点云信息和3D位置信息，o表示物体，d表示检测。将2D和3D特征拼接/相加得到每个节点的特征。而节点自然就是上一帧存在的目标和当前帧的观测。对于边权/相似度矩阵的确定，作者尝试了三种方式，余弦距离、欧氏距离还有网络回归，即上图中的Edge Regression：当然这个也是边权更新的机制，而对于节点 消息传递机制，作者也设计了四种：方法很明了，我就不细讲了，可以看到聚合方式都是求和。另外由算法框架示意图可知，每次消息传递都会计算关联矩阵的损失，那么为什么会采用batch triplet loss呢？作者把关联矩阵中的每条边权看作了N对匹配，三元组损失中，首先选取相邻帧中的一对连接i,j，然后分别选取不同id的两帧节点r,s，计算上述损失。即要保证不同帧间不同id身份的边权的最小距离越大越好。而对于相似度损失，则是采用了两种交叉熵损失：效果如下：可以看到利用网络回归的方式得到的相似度度量方式要比余弦距离和欧氏距离好，2D和3D特征融合的方式更优，结合了关联矩阵和节点差异的聚合方式，即Table7中的type4更优。6.GNNTrkForecast论文题目：Joint 3D Tracking and Forecasting with Graph Neural Network and Diversity Sampling作者：Xinshuo Weng, Ye Yuan, and Kris Kitani论文链接：https://arxiv.org/abs/2003.07847代码链接：https://github.com/xinshuoweng/GNNTrkForecast这篇论文里面，作者通过GNN将3D MOT和轨迹预测结合在一起了。其中对于GNN网络的构建以及关联矩阵的获取跟之前的论文几乎一致，具体我们就不介绍了：那么3D MOT分支实际上就是GNN模型中的一部分，是根据GNN的边权矩阵进行数据关联：而对于轨迹预测分支，作者基于条件自编码器的形式，设计的流程图如下，由于这块我不熟悉，所以我就不细讲了。效果如下：7. JDMOT_GNN论文题目：Joint Detection and Multi-Object Tracking with Graph Neural Networks作者：Yongxin Wang，Xinshuo Weng，and Kris Kitani论文链接：https://arxiv.org/abs/2006.13164也许是看到最近联合检测和跟踪的框架很热门，作者团队又给加入了GNN模块，所以我们简单提一下：最开始的表观和运动特征部分就不提了，一个是LSTM/MLP回归，一个是Darknet53回归得到的。图的构建依旧是以检测框和目标作为节点，节点特征的更新则是：这里面要注意的是两个head，其中检测head的是根据各节点特征利用MLP降维得到用于分类和回归的特征。而数据关联head则是边权，它的确定是依据节点特征的差异，通过三层全连接得到的：最终效果如下：可以看到单纯用GNN做数据关联的提升并不大，当然，这里并没有做消融实验，也不能妄下评论。参考文献[1] Jiang X, Li P, Li Y, et al. Graph Neural Based End-to-end Data Association Framework for Online Multiple-Object Tracking[J]. arXiv preprint arXiv:1907.05315, 2019.[2] Ma C, Li Y, Yang F, et al. Deep association: End-to-end graph-based learning for multiple object tracking with conv-graph neural network[C]//Proceedings of the 2019 on International Conference on Multimedia Retrieval. 2019: 253-261.[3] Jiahe L, Xu G, Tingting J.Graph Networks for Multiple Object Trackin[C]//The IEEE Winter Conference on Applications of Computer Vision (WACV).2020.[4] Brasó G, Leal-Taixé L. Learning a neural solver for multiple object tracking[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 6247-6257.[5] Weng X, Wang Y, Man Y, et al. GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning[J]. arXiv preprint arXiv:2006.07327, 2020.[6] Weng X, Yuan Y, Kitani K. Joint 3d tracking and forecasting with graph neural network and diversity sampling[J]. arXiv preprint arXiv:2003.07847, 2020.[7] Wang Y, Weng X, Kitani K. Joint Detection and Multi-Object Tracking with Graph Neural Networks[J]. arXiv preprint arXiv:2006.13164, 2020.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[arXiv 04.17 MOT论文解读]]></title>
    <url>%2F2020%2F04%2F17%2FarXiv04.17%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[前言最近MOT相关的论文出来得很多，4.17又出来了三篇，各有各的特点吧，其中最后一篇我觉得很有意思，我这里一起介绍一些各自的创新之处。1.Siamese Track-RCNN论文题目：Multiple Object Tracking with Siamese Track-RCNN作者团队：亚马逊云服务识别实验室论文链接：https://arxiv.org/abs/2004.07786这篇论文的思路很直接，其更多的创新还是基于SOT任务中的孪生结构和MOT中的联合检测和跟踪的框架，将SOT、Detection和ReID利用多任务分支的方式合并为一个整体框架。论文的整体其实通过上图就能猜到：SOT分支，作者借助GOTURN的SOT孪生结构，基于上一帧目标位置在当前帧扩展区域进行搜索，这里与原始SOT任务不同的是：预测的是目标相对位移，而不是响应图，并且预测的分类信息中的前景背景信息，作者解释成目标是否可见，这一点在MOT数据集中存在目标可视度的标注。但是这里依旧是对每个目标做了一次SOT，所以不得不用GOTURN这类效率高的网络；Detection分支，这个很简单，我之前的多篇博客已经介绍过了， 这里作者采用的就是Tractor++的框架；ReID分支，这里仅仅是使用了共享特征，通过triplet loss进行训练。总的来说，论文思路就是让多个任务共享特征。效果如下：2.ArTIST论文标题：ArTIST: Autoregressive Trajectory Inpainting and Scoring for Tracking作者团队：澳大利亚国立大学机器视觉中心&amp;EPFL CVLab论文链接：https://arxiv.org/abs/2004.07482通过我们之前对于联合检测和跟踪的框架的讨论，我们可以知道的是这类框架效果好的本质在于检测器的性能，主要体现在跟踪精度上，进而影响ID Sw.。但是如果不结合运动和表观等其他信息的话，ID Sw.和FP会大大增加，这篇论文就是从运动信息从层面对Tracktor++进行了改进。上图是对比Tracktor++对于遮挡的鲁棒性，论文框架如下：从公式我们大致可以理解为对于每个即将加入跟踪轨迹的候选框，通过条件概率模型计算其属于该轨迹的概率。从图中我们能看到的是作者采用的框架式基于LSTM的，并且这里面涉及到了目标位置和形状的回归估计。从论文中作者介绍的来看，作者通过K-means的方式得到了K类运动模式(△x,△y,△w,△h)，由此得到近似最优的运动估计。如上图所示，对于每个目标（假设有n个），都会存在k种运动模型和m种可能的观测框，也就是每一次都要进行nkm次估计，利用概率模型选择最优的估计。这一步作者称之为Tracklet Scoring。然后考虑到目标丢失所造成了轨迹缺失，作者直接采用丢失之前的运动模式进行估计，称之为Tracklet inplainting。当然，最后还是通过匈牙利算法进行了数据关联。所以这篇文章的创新点就在于基于LSTM的离散运动状态估计。效果如下：这里提一下，上面一栏是不基于图像信息的算法。3.SQE论文标题：SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking作者团队：清华大学&amp;旷视论文链接：https://arxiv.org/abs/2004.07472这篇论文我觉得很有意思，作者团队设计了一种不需要GT的MOT评价指标SQE，借此可以实现与原始评价体系相近的结果，这对于现实场景中的算法调优很有帮助。在看这篇论文之前我们可以自己先思考一下，在计算MOT相关的评价指标时，一般分为跟踪精度和跟踪轨迹一致性，对于跟踪精度而言，如果没有GT，无法估计。而对于跟踪轨迹的一致性，原始方式是通过GT计算跟踪轨迹中的ID变换情况，这一点可以近似估计。这篇论文就是通过分析相同身份目标和不同身份目标之间的特征距离来估计ID的变换情况的，可是问题在于常规MOT任务中也有很多利用ReID来实现这种效果的，但是如何保证这里的估计一定准确呢？我们具体看论文：作者首先分析了同一条轨迹内部的特征距离，可以看到ID1的轨迹中由于不存在其他身份的轨迹段，所以距离分布单一，而ID2中由于存在其他身份轨迹片段，所以存在了两种距离分布。而对于不同轨迹之间的特征距离，因为ID1和ID2轨迹中目标身份全无交集，所以也只存在一种距离分布，而ID2和ID3中存在轨迹交互，所以存在两种距离分布。为了度量上面所说的距离分布，作者引入了高斯混合模型，由于描述目标特征，距离度量模式采用欧氏距离：借助目标特征的高斯混合模型和欧氏距离公式，得到了距离分布模型如上图所示，并且特征距离标准化后服从卡方分布，这一点有点类似于马氏距离，具体可以去看我前面介绍Kalman滤波器相关的文章。当然，作者也说了，由于是采用的统计信息提到了均值和方差，另外ReID特征各个维度并不是独立，所以分布假设也会存在一定误差。为了验证这种方式的效果，作者做了相关实验：可以看到，对于同一身份的目标，无论其位于同一条轨迹还是不同轨迹，其大多数的距离分布都偏向于均值较小的部分。而不同身份的轨迹之间则是大多服从均值大的距离分布。评价指标公式和算法如下：其中n表示轨迹数量，L表示轨迹平均长度，对于轨迹内部，FP的判定是轨迹长度小于一定阈值和轨迹内距离分布标准差大于一定阈值。对于dif，这里我理解是轨迹内身份变化程度，作者通过计算2-高斯混合模型的均值距离来判定是否存在多个身份。对于轨迹之间，如果特征距离分布均值存在不同两种分布，则说明两条轨迹存在交叉，即sim误差增加。在实验环节，作者采用的是ReID领域经典的PCB算法，感觉好像是直接用PCB进行多目标跟踪。可以看到，随着ReID阈值的调整，IDF1和SQE的值变化情况接近要注意的是上面的ReID阈值是根据SQE评价指标提前设好的，而不是根据GT结果调整的，可以看到两种度量方式的差异接近。参考文献[1]Multiple Object Tracking with Siamese Track-RCNN.[2]ArTIST: Autoregressive Trajectory Inpainting and Scoring for Tracking.[3] SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的多目标跟踪算法（上）——端到端的数据关联]]></title>
    <url>%2F2020%2F04%2F15%2F%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%8A%EF%BC%89%E2%80%94%E2%80%94%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%2F</url>
    <content type="text"><![CDATA[前言最近基于深度学习的多目标跟踪算法越来越多，有用于特征提取的，有改进单目标跟踪器的，也有提升数据关联的。如果真的要总结的话那就太多了，所以我准备分类别进行介绍，这次我主要介绍端到端的数据关联方法。其中ICCV2019Tracktor++的作者团队在CVPR2020上被录用的DeepMOT和MPN Tracker两篇就是专门研究端到端数据关联算法的，这次我们结合近两年的顶会文章进行讲解。后续的部分也会继续在我的专栏更新~1.Deep affinity network for multiple object tracking（DAN)作者：ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah备注信息：PAMI2019论文链接：https://arxiv.xilesou.top/pdf/1810.11780.pdf代码链接： https://github.com/shijieS/SST.git这是一篇创作于2017.9，并刊登于PAMI2019的端到端多目标跟踪框架，对于后续几篇文章都有着启示作用。DAN框架[1]很简洁新颖：作者提出这个框架的目的是将表观特征和数据关联算法结合形状端到端的联合框架，在 NVIDIA GeForce GTX Titan上的效率为6.7FPS，下面我结合文章内容具体地解析算法。1.1 数据准备与增强要实现端到端的训练，如何将多目标跟踪数据集整理出来是第一大难关。这里作者采用了单目标跟踪算法中常用的跨帧匹配方式，允许相隔1~30帧的的视频帧进行跨帧数据关联，这样就可以解决MOT样本数量不足的问题。然而，视频类数据集还存在一个问题，即相邻帧的信息几乎一样，使得样本缺乏多样性。因此作者采用了几类数据增强方法：作者在文中提到了三种数据增强方法：光学扰动。即先将RGB图像转换到HSV格式，然后将S通道的每个值随机放缩[0.7，1.5]，最后转换到RGB格式，并将值域约束到0~255或者0~1；图像扩展放缩。本质上就是图像随机放缩，但是为了保证输入尺寸大小一致，作者在图像四周增加[1,1.2]倍的padding，padding像素值为图像均值；随机裁剪。基于裁剪比例[0.8，1]在原图上裁剪，不过要求无论怎么裁剪，图像上所有目标框的中心点一定不能丢失，最后再放缩至指定尺寸。而作者在代码中实际上使用了更复杂的数据增强方法：12345678910111213self.augment = Compose([ ConvertFromInts(), PhotometricDistort(), Expand(self.mean), RandomSampleCrop(), RandomMirror(), ToPercentCoords(), Resize(self.size), SubtractMeans(self.mean), ResizeShuffleBoxes(), FormatBoxes(), ToTensor() ])除去一些常规的数据类型转换和归一化操作，我们可以看到还多了一个随机水平镜像的操作RandomMirror,并且以上操作的概率为0.3。除此之外，熟悉MOT Challenge数据集的人应该知道，在该数据集中，无论目标是否可见都会标注，因此作者将可视度低于0.3的目标视为不可见，即认为不存在。另外，端到端的框架要保证输入输出的尺寸不变，因此作者设定了每一帧中出现的目标数量上限Nm，如80。1.2特征提取器特征提取器作为网络的第一个阶段，由上面的图示可以知道，作者利用孪生网络结构，通过特征提取器处理指定两帧中出现的所有目标框。特征提取器的结构为VGG网络+自定义扩展的结构组成：为了利用多尺度多感受野信息，作者取了以上9层的特征图信息，得到了长度为520的特征向量，即60+80+100+80+60+50+40+30+20，可以看到520这个数据就是9层特征通道之和。不过文中作者并没有介绍是如何处理这些特征的，我们通过代码来观察：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def forward_feature_extracter(self, x, l): ''' extract features from the vgg layers and extra net :param x: :param l: :return: the features ''' s = list() x = self.forward_vgg(x, self.vgg, s) x = self.forward_extras(x, self.extras, s) x = self.forward_selector_stacker1(s, l, self.selector) return xdef forward_vgg(self, x, vgg, sources): for k in range(16): x = vgg[k](x) sources.append(x) for k in range(16, 23): x = vgg[k](x) sources.append(x) for k in range(23, 35): x = vgg[k](x) sources.append(x) return xdef forward_extras(self, x, extras, sources): for k, v in enumerate(extras): x = v(x) #x = F.relu(v(x), inplace=True) #done: relu is unnecessary. if k % 6 == 3: #done: should select the output of BatchNormalization (-&gt; k%6==2) sources.append(x) return xdef forward_selector_stacker1(self, sources, labels, selector): ''' :param sources: [B, C, H, W] :param labels: [B, N, 1, 1, 2] :return: the connected feature ''' sources = [ F.relu(net(x), inplace=True) for net, x in zip(selector, sources) ] res = list() for label_index in range(labels.size(1)): label_res = list() for source_index in range(len(sources)): # [N, B, C, 1, 1] label_res.append( # [B, C, 1, 1] F.grid_sample(sources[source_index], # [B, C, H, W] labels[:, label_index, :] # [B, 1, 1, 2 ).squeeze(2).squeeze(2) ) res.append(torch.cat(label_res, 1)) return torch.stack(res, 1)从代码可以看到，9层不同分辨率的特征图都通过采样操作降维了，其中`source表示的是特征图BxCxHxW，labels表示的是一个框中心坐标BxNx1x1x2，也就是说每个特征图都会采样每个框的映射位置。因此最后通过得到就是Bx(CxN)的输出，也就是图中的Nmx520，这个过程的确挺出乎我意料的。1.3亲和度估计亲和度估计实际上就是特征相似度计算和数据关联过程的集成，第一步我们可以看到作者将两个520xNm大小的特征图变成了一个1040xNmxNm大小的亲和度矩阵，比较难理解：这里我们观察一下作者的代码：12345678910111213def forward_stacker2(self, stacker1_pre_output, stacker1_next_output): stacker1_pre_output = stacker1_pre_output.unsqueeze(2).repeat(1, 1, self.max_object, 1).permute(0, 3, 1, 2) stacker1_next_output = stacker1_next_output.unsqueeze(1).repeat(1, self.max_object, 1, 1).permute(0, 3, 1, 2) stacker1_pre_output = self.stacker2_bn(stacker1_pre_output.contiguous()) stacker1_next_output = self.stacker2_bn(stacker1_next_output.contiguous()) output = torch.cat( [stacker1_pre_output, stacker1_next_output], 1 ) return output可以看到，作者的处理方式也比较特征先分别将两个520xNm大小的特征图分别扩展维度变成了520x1xNm和520xNmx1的矩阵，然后分别复制Nm次，得到两个520xNmxNm矩阵，最后将通道合并得到1040xNmxNm大小的特征矩阵。随后利用一系列1x1卷积降维至NmxNm大小。作者在这里考虑了实际过程中出现的FP和FN情况，即目标轨迹无对应观测和观测无对应目标轨迹的场景，分别为关联矩阵的行列增加了一个x单元，用于存储这些情况，即L：(Nm+1)x(Nm+1)。其中最后一行和最后一列都填充为固定值，文中作者设定的是10.1.4训练为了方便损失函数的设计，作者将L拆解为row-wise何column-wise两种形式：M1：Nmx(Nm+1)和M2：(Nm+1)xNm。并分别采用行softmax和列softmax求得各自的联合概率分布A1和A2两种分配矩阵。这里做一个解释，对于增加了一列的M1矩阵，其针对的是第t-n帧的目标轨迹和第t帧观测的匹配，通过行softmax可以得到每个目标属于每个观测的概率，这是一个前向的匹配过程。反之M2就是一个反向的匹配过程。基于这种假设，作者设计了四种损失函数，分别是前向/反向损失、前向反向一致性损失和联合损失：其中L1和L2是为了保证L和A1/A2矩阵尺寸一致所裁剪行列得到的矩阵。训练算法方面，作者采用的是multi-step的学习率下降策略，batchsize=8，epochs=120.1.5推理通过以上分析我们可以知道的是，该网络通过输入两帧的目标信息可以得到亲和度矩阵A=max(A1, A2)，然而这个矩阵并不是关联矩阵，更像是相似度矩阵，并不能直接得到跟踪结果。然而作者并没有将相似度矩阵转换为代价矩阵输入匈牙利算法：从文章可以看到，作者将目标轨迹i的历史帧信息与当前帧第j个目标的亲和度求和了，但是如何使用的还是没说清，这一点我们在代码里面找到了：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def get_similarity_uv(self, t, frame_index): res_similarity = [] res_uv = [] for i, f in enumerate(t.f): if len(t.f) == TrackerConfig.max_track_node and i == 0: continue all_iou = self.recorder.all_iou[frame_index][f] all_similarity = self.recorder.all_similarity[frame_index][f] selected_box_index = t.uv[i, i] if selected_box_index == -1: # cannot find box in f frame. res_similarity += [0] res_uv += [-1] continue # combine the similarity with the iou selected_similarity = np.copy(all_similarity[selected_box_index, :]) delta_f = frame_index - f if delta_f in TrackerConfig.min_iou_frame_gap: iou_index = TrackerConfig.min_iou_frame_gap.index(delta_f) selected_iou = (all_iou[selected_box_index, :] &gt;= TrackerConfig.min_iou[iou_index]).astype(float) selected_iou = np.append(selected_iou, 1.0) selected_similarity = selected_similarity * selected_iou max_index = np.argmax(selected_similarity) max_value = all_similarity[selected_box_index, max_index] if max_index == all_similarity.shape[1] - 1: # new node max_index = -1 res_uv += [int(max_index)] res_similarity += [float(max_value)] # get the representation box of this frame. res = &#123;&#125; for uv, s in zip(res_uv, res_similarity): # if s &lt; 0.5: # continue if uv not in res: res[uv] = [s] else: res[uv] += [s] if len(res.keys()) &gt; 0: max_uv = max(res.keys(), key=(lambda k: np.sum(res[k]))) else: max_uv = -1 res_similarity += [1] res_uv += [max_uv] if max_uv == -1: t.age += 1 else: t.age = 0 return res_similarity, res_uv可以看到的是，除了亲和度矩阵，作者还加入了iou信息作为mask，然后利用iou_tracker的方式，采用贪婪算法，逐行取最大相似度的匹配，这种方式以及其变种我在之前的博客详细介绍过。没有引入其他的模块，如运动信息等，效果也不错：2.A Differentiable Framework for Training Multiple Object Trackers（DeepMOT)作者：Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taixé, Xavier Alameda-Pineda备注信息：ICCV2019Tracktor++的作者团队，CVPR2020论文链接：https://arxiv.org/pdf/1906.06618.pdf代码链接：https://gitlab.inria.fr/yixu/deepmot这篇DeepMOT[2]文章作者团队与ICCV2019的Tracktor++[3]团队一致，不过这篇文章的创新性很高，内容也很精彩，所以我这里也详细介绍一下。2.1匈牙利算法形式演化作者一开始就交代了原始匈牙利算法的算法形式：其中D表示代价矩阵，这里叫距离矩阵，A表示0-1分配矩阵，匈牙利算法的核心就是在保证每个轨迹最多只有一个观测，每个观测最多只有一个目标轨迹对应的前提下，是的代价最小。紧接着，作者以MOT中常用的两个指标MOTA和MOTP为例进行微分化：对于MOTP，作者这里没有给出原始的形式，不过我们将距离矩阵看做iou信息就可以啦，即MOTP统计的是所有跟踪正确的轨迹的IOU平均值。这里我们不妨直接介绍DeepMOT的损失函数：可以看到，作者直接借助MOTA和MOTP指标，通过将其微分化得到了损失函数。其中要微分化就要将FP、FN、TP和ID Sw.等四个指标先微分化：这里做一个解释，作者在设计关联矩阵的时候跟DAN一样，增加了一行一列，所以FP就是新增的目标观测信息被误匹配到了已存在的目标轨迹上，即最后一列的数值之和，同理可得FN。对于IDS，其意义在于目标轨迹中的身份切换次数，因此作者通过上一帧的关联0-1矩阵和当前帧估计的匹配矩阵C进行相乘。示例如下：其中距离矩阵D的计算是根据关联结果，利用iou和欧氏距离计算而来：从上图可以得到一个有意思的信息，即关联矩阵大小可以不固定，不用预设目标数量。2.2DHN网络设计DHN网络是一个单独设计的网络框架，其输入是目标轨迹和观测量的距离矩阵，在训练的时候作者直接采用了public detections作为目标轨迹，gt作为观测。可以看到的是，作者通过两个阶段的特征变换，使得两两之间的相似度信息得到了充分交互。这里作者设计了三种流程，一种是串行的Sequential模式，一种是并行的Parallel模式，最后一种是基于U-Net的形式，利用卷积网络进行交互。其中row-wise的意思是一行行的从NxM的距离矩阵中取向量，得到1x(MN)的向量。而对于向量中各单元的交互，作者采用了Seq2Seq的BiRNN结构：但是我从文中不明白的是，作者如何将一个1x(MN)的向量，通过Seq2Seq转换为2HxNxM的矩阵的。这里我去看了一下代码：123456789101112self.lstm_row = nn.GRU(element_dim, hidden_dim, bidirectional=biDirenction, num_layers=2)self.lstm_col = nn.GRU(512, hidden_dim, bidirectional=biDirenction, num_layers=2)# The linear layer that maps from hidden state space to tag spaceif biDirenction: # *2 directions * 2 ways concat self.hidden2tag_1 = nn.Linear(hidden_dim * 2, 256) self.hidden2tag_2 = nn.Linear(256, 64) self.hidden2tag_3 = nn.Linear(64, target_size)else: # * 2 ways concat self.hidden2tag_1 = nn.Linear(hidden_dim, target_size)备注：element_dim为1，hidden_dim为256。也就是说输入一个长为NM的序列，得到NMx512的输出。可以看到基于Sequential流程和GRU结构的框架效果最好，其中WA指的是准确率，MA指的是未被分配到观测信息的比例，SA指的是分配超过一个观测信息的比例。2.3实现细节对于填充行列的固定值，作者采用的是0.5，另外，由于关联矩阵是离散且稀疏的，所以作者首先通过sigmoid离散化0-1矩阵，然后基于关联矩阵中的0-1数量比例进行自适应的损失加权。采用RMSprop优化器，初始学习率为3e-4，每2w iter降低5%学习率，训练20epochs，只用6小时。gt是通过匈牙利算法生成的。作者的底层多目标跟踪框架是Tracktor++构建的，在此基础上加入了ReID head，从而实现多任务的训练模式。其中Re-ID分支采用ResNet50网络结构，通过triplet loss训练。除此之外，作者还尝试利用单目标跟踪算法SiamRPN和GOUTRN作为底层跟踪器进行实现，可以发现，DeepMOT均有所帮助。为了降低过拟合风险，作者还将标注的目标框信息进行一定尺度的放缩和平移，增大误差，模拟实际观测误差。3.Online multi-object tracking with dual matching attention networks(DMAN)作者：Ji Zhu，Hua Yang，Nian Liu，Minyoung Kim，Wenjun Zhang ，and Ming-Hsuan Yang备注信息：ECCV2018论文链接：http://openaccess.thecvf.com/content_ECCV_2018/papers/Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper.pdf代码链接：https://github.com/jizhu1023/DMAN_MOTDMAN[4]和FAMNet[5]我本来准备放到下一讲SOT专题来讲的，但是考虑到篇幅平衡，并且这两个都可以扯到端到端的SOT+Data Association框架，就还是加进来了。DMAN算法是基于以下四个方向展开的讨论：通过空间注意力模块让网络更加注意前景目标：基于循环网络，利用时间注意力机制分析目标轨迹中各个历史信息的权重：利用SOT跟踪器，缓解MOT中由于遮挡、观测信息质量不高导致的目标丢失等情况，增强鲁棒性。其中空间注意力模块很简单，就是深度学习中常见的mask，利用mask与原特征图进行相乘。而时间注意力网络，则是将目标轨迹中的所有的历史信息与当前观测信息进行匹配，最后加权判断是否匹配成功。而SOT跟踪器的底层算法是单目标跟踪中知名的ECO算法，不过作者考虑到背景和前景的样本不平衡问题，引入代价敏感参数q(t)对其进行了进一步的改进：之所以我要将DMAN算做端到端的数据关联框架，是因为这个框架实现的是1：N的数据关联，这里有两层含义：一个观测对应N个轨迹历史信息；一个观测对应N条目标轨迹，这里是以batch的形式实现的，勉强沾边吧。4.Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking(FAMNet)作者：Peng Chu, Haibin Ling备注信息：ICCV2019论文链接：http://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdf相比于DMAN，FAMNet[4]对于端到端数据关联算法的集成更加紧密：通过观察网络可以看到，FAMNet的流程是SOT-&gt;Affinity-&gt;Data Association-&gt;post process，其中它的核心在于通过局部的关联形式完成了K帧的联合数据关联训练。4.1Affinity Subnet作者将SOT孪生网络和亲和度网络合并，SOT backbone作为特征提取器，SOT输出的置信图作为亲和度依据。下面我们详细分析一下这个网络结构。作者这里提出了一个新概念anchor candidate，这里指的是当前帧的目标，对应上图中中间部分k=1的分支，至于途中所提到的数字2，这里指的是当前帧中的第二个目标。对于指定目标，通过对当前帧和前后帧扩展区域的裁剪，基于两个孪生网络进行两次SOT跟踪流程，从而得到上下两个置信图输出。这其中还有三个Detection Attention Mask，初步理解就是将观测信息变成mask，而观测信息的确定是通过目标序号来确定的，所以可以看到观测信息并非与当前目标对应。最终得到的亲和度就是两个SOT置信图得到的信息和detection mask得到的亲和度之和，在上面的例子里面存在3x3x3对亲和度，但是由于两个非`anchor frame并没有直接参与匹配，所以图中显示的是C222。这里作者借鉴的是IJCAI2019的一篇论文的策略[6]：4.2 R1TA Power Iteration Layer对于关联假设的生成，FAMNet采用了与前文一致的策略：那么从亲和度矩阵到数据关联的集成，作者同样借鉴了IJCAI2019的那篇论文的策略[6]，首先将匈牙利算法进行了重构：即将整体的数据关联n:m,变成了n份小型的数据关联的联合作用,对于这个关系的转换，可以参照论文[6]：其中z_212指的是第一帧中的第二个目标、第二帧中的第一个目标和第三帧中的第二个目标的整体关联结果是否为真，其中任意一对不成立都会变成0。然后基于R1TA Power Iteration过程，将这种多次SOT过程组成一个完整的数据关联过程：经过我们上面的讨论，这幅图里面的大多数的示意应该能懂，但是对于Rank-1 tensor approximation可能还有疑惑，文中也给出了解释，我感觉这两篇几乎就是理论和实践分开投了两个顶会…可以看到，作者通过一个连续的外积公式来近似k阶关联矩阵，从而转换了匈牙利算法问题形式。作者也给出了迭代算法的前向反向公式：可以注意到的是，这里面有一个除法因子，用作L1 norm，作者也给出了它的求导转换，这里我就不再放了。结果如下：5.Learning a Neural Solver for Multiple Object Tracking(MPNTracker)作者：Brasó G, Leal-Taixé L备注信息：MOT Challenge上前列，CVPR2020论文链接：https://arxiv.xilesou.top/pdf/1912.07515代码链接：https://github.com/selflein/GraphNN-Multi-Object-TrackingMPNTracker在MOT Challenge各个榜单上都位于前列，不过近期有个别算法超过去了，但是它的各项指标性能依旧很高。这个算法框架很简洁，直接借助了图卷积网络中的消息传递机制，基于Re-ID和位置形状信息进行特征建模，不过我暂时对于图卷积不是很了解，所以就不做详细的介绍：对于当前帧的每个目标，其消息传递方式的节点更新部分考虑了时间信息，即针对前面的帧和后面的帧，分开计算边的累加影响，利用链接的方式结合，而非累加的方式。可以看到，整体结构很简单，就是网络的构建和特征的建模，其中网络构建部分我在之前的博客中进行了详细的介绍。其效果也很不错：参考文献[1] Sun S, Akhtar N, Song H, et al. Deep affinity network for multiple object tracking[J]. IEEE transactions on pattern analysis and machine intelligence, 2019.[2] Xu Y, Ban Y, Alameda-Pineda X, et al. DeepMOT: A Differentiable Framework for Training Multiple Object Trackers[J]. arXiv preprint arXiv:1906.06618, 2019.[3] Bergmann P, Meinhardt T, Leal-Taixe L. Tracking without bells and whistles[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 941-951.[4] Zhu J, Yang H, Liu N, et al. Online multi-object tracking with dual matching attention networks[C]. in: Proceedings of the European Conference on Computer Vision (ECCV). 2018. 366-382.[5] Chu P, Ling H. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 6172-6181.[6]X. Shi, H. Ling, Y. Pang, W. Hu, P. Chu, and J. Xing. Rank-1 tensor approximation for high-order association in multi-target tracking. IJCV, 2019.[7] Brasó G, Leal-Taixé L. Learning a Neural Solver for Multiple Object Tracking[J]. arXiv preprint arXiv:1912.07515, 2019.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>数据关联</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的多目标跟踪算法（中）——从UMA Tracker(CVPR2020)出发谈谈SOT类MOT算法]]></title>
    <url>%2F2020%2F04%2F08%2F%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%AD%EF%BC%89%E2%80%94%E2%80%94%E4%BB%8EUMA%20Tracker(CVPR2020)%E5%87%BA%E5%8F%91%E8%B0%88%E8%B0%88SOT%E7%B1%BBMOT%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言之前的博客中我们介绍了联合检测和跟踪的多目标跟踪框架，这类框架最大优势在于可以利用优秀的检测器平衡不同观测输入的质量。随之又介绍了端到端的数据关联类算法，这类算法的优势在于可以利用MOT数据信息缓解人工提取特征和计算特征距离的弊端。这次我们要介绍的是基于单目标跟踪（SOT）算法的MOT算法，这类算法的优缺点可以看我下面的介绍。1.前情回顾(FAMNet、DMAN)1.1 DMAN论文题目：Online multi-object tracking with dual matching attention networks备注信息：ECCV2018论文链接：http://openaccess.thecvf.com/content_ECCV_2018/papers/Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper.pdf代码链接：https://github.com/jizhu1023/DMAN_MOTDMAN算法我不小心放在了数据关联部分，这次我们简单回顾一下（具体可见上次的博客）：我觉得DMAN算法的主要特点在于：利用Bi-LSTM网络实现了观测框与目标轨迹历史特征序列的端到端特征提取与比对；将基于改进版ECO的SOT模块嵌入了网络中，其主要利用的是响应图信息，而响应图中包含有目标的定位和分类信息；在数据关联部分，我们可以注意到存在两个识别部分，作者称之为时空注意力，其中时间注意力就是第一点中的verfication任务，而空间注意力就对应图中的identification任务 ，这里利用SOT输出的响应图作为注意力mask，分别基于特征预测了目标身份信息。对于第一点，其实通过图就可以明白，是通过对于历史轨迹特征的质量进行自适应评估，并对特征自动融合。而对于第二点，关于SOT如何融入网络，可以自行搜索CFNet等SOT网络。而对于ECO算法，作者考虑到相似表观目标中容易出现的多峰问题进行了改进：即将处于目标附近的hard samples的惩罚权重变大：1.2FAMNet论文题目：Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking备注信息：ICCV2019论文链接：http://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdfFAMNet的结构我们也介绍了，这里也总结他的特征：对于相邻帧中的每个目标，利用Siamese网络进行单目标跟踪，由此隐式获取到目标的表观和位置信息，并基于响应图进行特征比对；利用其提出的R1TA Power Iteration Layer降低连续多帧数据关联的复杂度，并实现连续多帧的跟踪训练。2.STAM论文标题：Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism作者：Qi Chu，Wanli Ouyang，Hongsheng Li，Xiaogang Wang， Bin Liu，and Nenghai Yu备注：ICCV2017论文链接：http://openaccess.thecvf.com/content_ICCV_2017/papers/Chu_Online_Multi-Object_Tracking_ICCV_2017_paper.pdfSTAM算得上是一篇经典的多目标跟踪算法，而且仔细阅读之后还会发现一个亮点。其大致流程如下：可以简单看出这里面涵盖有运动模型、目标特征提取、目标空间注意力，以及目标轨迹时间注意力等等模块。看完整个流程我惊了，尤其是ROI Pooled Features那一部分，比Tracktor++提出得还早。通过将不同目标映射到特征图上进行进一步特征提取和位置回归，只不过作者当时并没有从检测入手，所以效果不突出。其中运动模型其实就是一个在线更新的带动量的匀速模型：对于空间注意力，作者主要考虑了遮挡问题，通过训练可视度响应图，由此作为特征的mask，突出前景目标特征：对于时间注意力，则是轨迹层面的质量考虑：其通过triplet loss训练，既包含当前帧内的neg和pos，还包含历史帧的：其中注意力计算如下：对于具体的实验细节，推荐大家去看作者的博士论文《基于深度学习的视频多目标跟踪算法研究》。3.LSST论文标题：Multi-Object Tracking with Multiple Cues and Switcher-Aware Classification作者：Weitao Feng，Zhihao Hu，Wei Wu，Junjie Yan，and Wanli Ouyang论文链接：https://arxiv.org/abs/1901.06129LSST中作者的出发点也是针对遮挡问题：由于遮挡导致轨迹特征出现残缺，甚至身份漂移。而作者的基础跟踪器则是SiamRPN，因为快而准。。。最左边就是就RPN框架的SiamRPN框架，作者称之为短期线索，这部分的质量是通过下面的公式计算的：而对于长期线索，则自然是ReID所提取的表观信息了。作者通过ResNet18设计了一个质量评估网络，从而在目标轨迹中选择K个最好质量的特征进行比对，当然每个特征间保留了间距：这样就得到了K组相似度，基于以上的短期和长期线索，作者利用regularized Newton boosting decision tree训练了一个分类器，由此进行数据关联。4.KCF论文标题：Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment作者：Peng Chu, Heng Fan, Chiu C Tan, and Haibin Ling备注：WACV2019论文链接：https://arxiv.xilesou.top/pdf/1902.08231此KCF并非单目标跟踪中的核相关滤波算法，只是名字巧合罢了（论文里面没说简称，但是MOT官网写的KCF）。我们可以看到这篇论文的流程十分复杂：整体来看包含了：综合前/背景相应和SOT设计Instance-aware SOT跟踪器：这两个响应图是直接基于岭回归算法叠加的：然后利用KCF的求解方式对联合模型进行求解。基于检测的校正，即对SOT结果和Detection信息利用multicut进行数据关联，对于这类图模型的构建可以参照我之前写的博客[7]。有了目标实际上就有了图节点，那么SOT模型就是为边权而服务的：其中X表示目标轨迹，O表示的是预测的目标位置和观测位置的集合，g就是上面的联合损失函数。即如果是相邻帧之间的边权，则用SOT中的联合损失函数值。如果是上一帧中目标间的边，则设置一个固定值。如果是当前帧节点间的边，则直接使用IOU代替。模型更新作者考虑到场景中可能存在的噪声信息，导致SOT跟踪结果不准，所以通过一个CNN网络判断当前SOT结果是否需要利用观测信息进行更新，如果需要，则采用观测框。有意思的是作者采用了强化学习的策略在线训练分类器。当观测框比预测框更精准，但是没有更新，那么观测框的特征和预测框的特征会被当作positive samples。当预测框比观测框更精准，但是却更新了，那么就视为negtive samples，样本与部分训练集合并组成在线训练集进行更新。特征是通过ROI Pooling进行提取的。当然，如果当前更新的权重并不适用于接下来的跟踪，权重还会恢复如初。目标的管理为了保证目标从遮挡状态恢复，作者做了一个强假设，即如果目标因遮挡而丢失，那么在出现的那一帧的数据关联中也没有与之匹配的目标。因此就可以跨帧匹配：利用时间距离、位置形状、IOU、直方图等信息作为特征，通过SVM进行分类判别。效果如下：5.UMA论文标题：A Unified Object Motion and Affinity Model for Online Multi-Object Tracking作者：Junbo Yin, Wenguan Wang, Qinghao Meng, Ruigang Yang, and Jianbing Shen备注：CVPR2020论文链接：https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.11291代码链接：https://github.com/yinjunbo/UMA-MOT这篇文章实际上跟前面我所介绍的DMAN算法很像，都是想利用SOT实现表观特征和运动信息的获取，进而实现在线的匹配关联：整体流程也很相似，那么UMA Tracker所基于的单目标跟踪器是SiamFC：其中的特征提取都是采用的AlexNet，从图中可以看到：对于正样本对则采用SOT进行跟踪比对，从而得到SOT部分的损失。对于每个目标样本，还存在一个embedding模块，提取了256维的特征信息，进而进行iidentification的分类任务；利用SENet的机制，实现verification任务。可以看到，这个整体就是基于SENet的变种，结合256个通道注意力而设计的，可以看到训练得到的特征图可视化效果还不错：其中第2行是跟踪任务中的响应图（网络第一行分支），第3行是相似度度量任务中的响应图（网络第三行分支），所以SOT的任务跟偏向于定位和周围环境信息的提取，而Affinity部分更偏向于前景目标的部位。对于跟踪流程，作者同样考虑了遮挡情况：这里作者直接通过affinity相似度和IOU的变化情况估计了遮挡情况。另外，为了保证表观特征部分的信息更准确，作者利用ROI Align模块，将特征图上SOT预测出来的位置区域的目标特征单独获取出来作为表观特征的输入。最后在数据关联部分，作者同样考虑了跟踪轨迹的历史特征，不过使用方式比较简单：通过均匀采样，计算K组特征相似度，然后取平均作为最终的相似度。6.总结在MOT场景中，由于Siamese结构的存在，使得SOT任务本身就自带了定位和识别等信息，所以利用SOT替代运动模型和表观模型的算法相继涌现。另外，SOT本身对于观测缺乏的问题有一定的鲁棒性，可以通过区域搜索得到暂时的目标定位信息。如果SOT本身的定位能力强，比如SiamRPN这种，甚至都相当于额外做了检测，所以基于SOT的算法理论上是可以跟基于检测的框架一较高下的。但问题在于，基于SOT的MOT目前都是针对每个目标进行一次跟踪，效率方面问题太大了，希望有后续研究可以解决这一点。参考文献[1]Zhu J, Yang H, Liu N, et al. Online multi-object tracking with dual matching attention networks[C]. in: Proceedings of the European Conference on Computer Vision (ECCV). 2018. 366-382.[2]Chu P, Ling H. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 6172-6181.[3] Chu Q, Ouyang W, Li H, et al. Online multi-object tracking using CNN-based single object tracker with spatial-temporal attention mechanism[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2017. 4836-4845.[4]Feng W, Hu Z, Wu W, et al. Multi-object tracking with multiple cues and switcher-aware classification[J]. arXiv preprint arXiv:1901.06129, 2019.[5]Chu P, Fan H, Tan C C, et al. Online multi-object tracking with instance-aware tracker and dynamic model refreshment[C]. in: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019. 161-170.[6]Yin J, Wang W, Meng Q, et al. A Unified Object Motion and Affinity Model for Online Multi-Object Tracking[J]. arXiv preprint arXiv:2003.11291, 2020.[7] https://zhuanlan.zhihu.com/p/111397247]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>SOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2020 | 多目标跟踪（车辆）与检测框架 RetinaTrack]]></title>
    <url>%2F2020%2F04%2F08%2FCVPR2020%20%20%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%EF%BC%88%E8%BD%A6%E8%BE%86%EF%BC%89%E4%B8%8E%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%20RetinaTrack%2F</url>
    <content type="text"><![CDATA[前言今天经群友提醒，发现漏掉了一篇CVPR2020的MOT论文，同样是基于检测和跟踪一体的框架，只不过它是以车辆跟踪为背景而写的。这里我们也凑个整，Tracktor++（就叫它FrcnnTrack吧，哈哈）、CenterTrack、FairMOT、JDE(YoloTrack。。。) ，以及这次的RetinaTrack开始神仙打架。不过也为MOT领域担忧，在public赛道上基于检测的跟踪框架把baseline刷得太高了，有点不利于后续发展呀。RetinaTrack论文题目：RetinaTrack: Online Single Stage Joint Detection and Tracking作者团队：谷歌备注：Waymo 39.12MOTA,14FPS虽然RetinaTrack也是同之前的联合检测和跟踪的算法一样的框架（感兴趣的可以在我放在参考文献中的链接里面去看看），从名字也知道是基于RetinaNet的，但是论文中是以自动驾驶为背景进行介绍的，没有在MOT Challenge赛道比拼，倒是跟Tracktor++进行了比较。首先我们回顾一下RetinaNet的结构：整体来看，我们可以讲其归纳为三个特点：FPN、focal loss、回归和分类的两个分支（在我之前介绍目标检测中的特征冲突中提到了）。然后我们看看RetinaTrack的架构：直接从图上看的话我们可以得到的信息是，RetinaNet在分类和回归的分支上分别预测了k个anchor下的分类和回归信息。而RetinaTrack与JDE和FairMOT一样，都增加了一个256维的特征信息embeddings分支：我们都知道，在MOT场景中需要解决严重遮挡问题，这个问题对于检测的影响也很大，比如：上图中两辆车的中心重合，二者的检测框如果都是基于同一个anchor点进行预测的，则很难得到具有分辨力的embeddings。另外，我们之前的博客讨论过，reid和目标检测在特征方面的需求不同，以行人检索为例，目标检测中分类要求同类目标特征一致，而ReID则是要求在保证类内距离尽可能小的同时，确保类间距离大，但是这里的类间指的是不同身份的人，但是对于目标检测而言都是人。所以这里将ReID和分类的共享特征减少是最好的选择，作者这里实际上隐含着用了三种方式改进这一点：通过将分类、回归和特征提取设为三个分支任务，除了FPN之前的部分，三者的特征共享部分含有m1个3x3卷积；对于每层特征图上每个特征点的k个anchor，全部预测分类、回归和特征，增加区分度。；对于检测任务，分类和回归分支都包含m2个3x3卷积，而embedding分支则为m3个1x1卷积。对于训练部分，不同于JDE和FairMOT采用的identification模式，RetinaTrack采用的是verification模式，采用基于batch-hard的triplet loss进行训练，其中margin为0.1。以上任务是在一堆TPU上训练的，基于Momentum SGD算法，每个batch还有128个clips，每个clip含两个相隔8帧的样本（对于10Hz的Waymo数据集而言就是相隔0.8s），图像输入是1024x1024，并采用bfloat16式的混合精度训练模式。其中去除embeddings分支的部分是在COCO数据集上预训练的，然后采用warmup和余弦退火学习策略训练。实验效果如下：由消融实验可知，anchor类型数量越多效果越好，其中RetinaNet部分是直接通过IOU进行数据关联的。紧接着作者又做了几组对比实验：对比MOT Challenge中表现良好的Tracktor++算法，RetinaTrack效果更好；基于IOU，不采用triplet loss（这是直接做成identification了？），或者将特征分支单独利用resnet50训练这两种方法都不如RetinaTrack。在Waymo v1.1数据集上MOTA可达44.92，mAP可达45.70，推理速度为70ms参考资源[1] RetinaTrack: Online Single Stage Joint Detection and Tracking[2] https://zhuanlan.zhihu.com/p/125395219[3] https://zhuanlan.zhihu.com/p/126558285[4] https://zhuanlan.zhihu.com/p/126359766]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOT开源实时新SOTA |A Simple Baseline for Multi-Object Tracking]]></title>
    <url>%2F2020%2F04%2F07%2FMOT%E5%BC%80%E6%BA%90%E5%AE%9E%E6%97%B6%E6%96%B0SOTA%20A%20Simple%20Baseline%20for%20Multi-Object%20Tracking%2F</url>
    <content type="text"><![CDATA[前言今天又开源了一篇MOT的新SOTA，也是实时的，也是CenterNet为底层的，估计是看到CenterTrack开源了。emmm….看来我近期看的几篇都在今年某顶会扎堆了，噗。这里我还是把这篇文章给介绍一下吧，有意思的是其中的大部分论点我都在之前的博客([2]、[3])说过了。FairMOT论文题目：A Simple Baseline for Multi-Object Tracking作者团队：华科&amp;微软亚研院备注：MOT15~20(private)：59.0、68.7、67.5、58.7 MOTA代码链接：https://github.com/ifzhang/FairMOT这篇论文的立意是两部分，一个是类似于CenterTrack的基于CenterNet的联合检测和跟踪的框架，一个是类似于JDE，但是却又不同的，探讨了检测框架与ReID特征任务的集成问题。作者称这类框架为one-shot MOT框架，论文一开始作者讨论了检测框架和ReID任务的关系：作者的意思是anchor-based的检测框架中存在anchor和特征的不对齐问题，所以这方面不如anchor-free框架，emmm…指出的问题的确是对的，不过详细的讨论建议各位看看我之前对这个问题的详细讨论[3] [4]。作者因为这个问题而选择了anchor-free算法——CenterNet，不过其用法并不是类似于CenterTrack[2]中采取的类似于D&amp;T的孪生联合方式，而是采用的Tracktor++的方式。我们知道原始的anchor-free框架的大多数backbone都是采用了骨骼关键点中的hourglass结构：后面我会单独开一个Re-ID和MOT的专题，这里呢作者就谈到了Re-ID网络中典型的多尺度问题，所以就提出要将hourglass结构改成上图中的多尺度融合的形式。最后通过两个分支完成了检测和Re-ID任务的集成，那么接下来的部分就是如何训练。在训练部分呢，同样地，考虑到正负样本不均衡问题，作者采用了focal loss的形式：其中M(x,y)表示的是heatmap在(x,y)处存在目标的概率，而对于box size和offset则采用L1 loss：最后对于Re-ID分支而言，作者采用了identification式的分类框架，这里面的L就是不同的ID的one-hot表示，p就是网络预测的分类置信度。在实验部分，作者先是通过实验证明anchor-free的框架比anchor-based框架更适合reid：紧接着论证了多尺度融合框架对于Re-ID的影响：的确，从特征空间来讲，各ID的特征距离更大了。而对于Re-ID的特征维度，作者通过实验表明128维即可，这里我就不细说了。最后放一下结果，下面都是private赛道的：参考资源[1] A Simple Baseline for Multi-Object Tracking[2] https://zhuanlan.zhihu.com/p/125395219[3] https://zhuanlan.zhihu.com/p/114700229[4] https://zhuanlan.zhihu.com/p/126359766]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2020 商汤|再谈目标检测中的分类和定位冲突问题]]></title>
    <url>%2F2020%2F04%2F06%2FCVPR2020%20%E5%95%86%E6%B1%A4%E5%86%8D%E8%B0%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB%E5%92%8C%E5%AE%9A%E4%BD%8D%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言前段时间我在专栏里详细分析了目标检测中的特征冲突与不对齐问题，今天无意间又看到了商汤在CVPR2020的一篇相关论文，分析角度跟我之前所说的类似，但是解决方案增加了一些技巧，论文中提到其对于各类backbone都有~3%mAP的提升，该算法也用到了OpenImage比赛中，是对Decoupling Head框架的详细分析。这里我们一起来分析下论文内容。一、背景介绍前段时间我写了两篇相关的博客《谈谈CNN中的位置和尺度问题》[2]和《目标检测中的特征冲突与不对齐问题》[3]。其中其第一篇里面我详细分析了CNN中平移和尺度不变性和相等性问题，并介绍了padding对于位置估计的影响。在第二篇中我们从回归和分类任务对于平移/尺度不变性和相等性要求的冲突展开了一系列讨论，并结合anchor和特征的不对齐介绍了two-stage和one-stage检测算法的相关解决方案。因此强烈建议在阅读本篇内容之前先看看上两篇。论文一开始就给出了一副直观的示意图，图中展示的是分类和定位任务对于特征空间的敏感性对比，其中分类任务只对特定区域敏感，而定位任务对于目标整体，也可以说是目标边界都很敏感，这一点就衬托出二者对于特征的要求不同。之前的博客里我们所介绍的解决方案是尽可能减少定位和分类分支的特征共享部分。比如原始的Double Head的处理方式是在ROI Pooling之后进行解耦：而商汤之前《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》一文中所提出的Decoupling Head算法则是更为复杂的方式，我之前的博客中好像说漏了一部分，而关于这一点的详细介绍可以在这篇文章中得到解答。二、 Task-aware spatial disentanglement learning论文所提出的算法名为TSD，我们重点要关注的是图中的三个部分，我们先看Spatial disentanglement和TSD部分：首先作者定义了新的损失函数形式：$$ \begin{array}{l} L = L_{cls}^D\left( {H_1^D\left( {{F_l},{{\hat P}_c}} \right),y} \right) + L_{loc}^D\left( {H_2^D\left( {{F_l},{{\hat P}_r}} \right),B} \right)\\ \left\{ \begin{array}{l} H_1^D\left( \cdot \right) = \left\{ {{f_c}\left( \cdot \right),C\left( \cdot \right)} \right\}\\ H_2^D\left( \cdot \right) = \left\{ {{f_r}\left( \cdot \right),R\left( \cdot \right)} \right\} \end{array} \right. \end{array} $$跟传统的方式不同，作者想通过特征变换的方式让两个任务执行得更好，关于这一点可以在[3]中的Guided Anchoring和AlignDet等算法中看到类似的思路，即基于预测的anchor框offset，通过deform conv的方式重新提取特征进行分类。这里作者令分类任务的特征变换形式为point-wise，令回归任务特征变换的形式为proposal-wise，也就是说作者让网络特征通过两种变换形式变成了任务驱动型的特征输入。下面我们来看看两种变换具体是怎么执行的。$$ \left\{ \begin{array}{l} {{\hat P}_c} = {\tau _c}\left( {P,\Delta C} \right),\Delta C = \gamma {F_c}\left( {F;{\theta _c}} \right) \cdot \left( {w,h} \right)\\ {{\hat P}_r} = {\tau _r}\left( {P,\Delta R} \right),\Delta R = \gamma {F_r}\left( {F;{\theta _r}} \right) \cdot \left( {w,h} \right) \end{array} \right. $$对于分类部分，这一部分我们比较熟悉，作者通过第一阶段的回归结果，预测了256x256x(kxkx2)大小的偏移量，其中kxk指的是ROI Pooling之后的特征图大小。通过这个offset，基于deformable conv的方式对特征重采样，重采样的特征用于分类，之前已经讨论过了。我们再看回归部分，作者通过第一阶段的回归结果，在此基础上预测了框特征的偏移量，即256x256x2，不同的是，这个偏移量是proposal-wise的，也就是说框的整体平移，通过平移进行特征采样。综上，TSD部分的创新在于对回归和分类都进行了特征变换，不同的是回归采用特征平移，而分类则采用我们熟悉的模式。三、 Progressive constraint如果我们观察了Decoupling Head框架，会发现除了回归和分类两个分支，作者还保留了原有的ROI-Pooling主干。关于这一点，作者在这篇文章也解释了，就是上文中的Sibling head+PC部分。这一部分呢主要还是为了提升TSD性能而提出来的，我们可以总结为一致性约束，即令TSD和传统ROI Pooling主干结果保持一致。四 、实验结果这里我简单罗列部分实验结果：这里作者主要想表明TSD+PC的策略对于不同的backbone都有提升，这里我再给出COCO的结果：可见本文的效果的确不错。五、总结综上呢，其实我们可以看到，绝大多数的内容都在之前的博客中讲到了，不同的地方只有回归部分特征的平移策略，以及模型的ensemble策略。这篇博客的主要目的是对之前内容的一点补充，以上这些主要讲的还是特征层面的应对策略，对于我们之前所提及的关于NMS过程中分类置信度的“不可信”问题并没有进行说明。而之前有知友在评论里面提到了ICCV2019的Guassian YOLOv3，其代码链接为：https://github.com/jwchoi384/Gaussian_YOLOv3这个算法通过将回归任务改进为高斯分布参数的预测，从侧面预测了回归框的可靠性，进而利用这部分参数修正回归框和分类置信度，不失为一种改进策略：$$ prob = {P_{object}} \times {P_{clas{s_i}}} \times \left( {1 - \frac{{{\delta _x} + {\delta _y} + {\delta _w}{\rm{ + }}{\delta _h}}}{4}} \right) $$不过这方面的研究还没结束，期待以后有更简洁的特征层面的解决策略~参考资源[1] Revisiting the Sibling Head in Object Detector[2] https://zhuanlan.zhihu.com/p/113443895[3] https://zhuanlan.zhihu.com/p/114700229[4]1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation.[5]Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomo]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[联合检测和跟踪的MOT算法解析（含MOT17 No.1等多个榜前算法!）]]></title>
    <url>%2F2020%2F04%2F03%2F%E8%81%94%E5%90%88%E6%A3%80%E6%B5%8B%E5%92%8C%E8%B7%9F%E8%B8%AA%E7%9A%84MOT%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言最近一年里，随着Tracktor++这类集成检测和多目标跟踪算法框架的出现，涌现了很多相关的多目标跟踪算法变种，基本都位列MOT Challeng榜单前列，包括刚刚开源的榜首CenterTrack。这里我就对集成检测和跟踪的框架进行分析，相关MOT和数据关联的基础知识可以去我的专栏查看，后期我也会针对基于深度学习的数据关联、ReID2MOT和SOT2MOT等进行专题介绍。1.Detect to Track and Track to Detect（D&amp;T)作者：Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman备注信息：ICCV2017论文链接：http://openaccess.thecvf.com/content_ICCV_2017/papers/Feichtenhofer_Detect_to_Track_ICCV_2017_paper.pdf代码链接：https://github.com/feichtenhofer/Detect-Track当前的多目标跟踪算法主流是基于检测的框架，即Detection based Tracking(DBT)，所以检测的质量对于跟踪的性能影响是很大的。那么在MOT Challenge上也分别设置了两种赛道，一种是采用官方提供的几种公共检测器的结果，即public赛道，一种是允许参赛者使用自己的检测器，即private赛道。这篇D&amp;T就属于private类跟踪框架，并初步将检测与跟踪框架进行了结合：从图中可以清晰看到，作者通过改进版的R-FCN检测网络实现了主线的检测任务，然后基于两阶段目标检测的特点，将第一阶段所获得的多尺度特征图进行交互。这种方式借鉴了单目标跟踪中经典的Siamese网络框架，不同之处在于原本的Siamese网络做的是1:1的相关滤波，而D&amp;T框架做的是n:n的相关滤波。其中两个分支中所包含的目标数量也是不定的，那么为什么作者要用R-FCN网络呢，可以发现，R-FCN的网络结构起到了很好的作用，正是因为其独特的position-sensitive ROI Pooling模块：不同于传统两阶段目标检测框架利用全连接网络分支预测分类和回归的情况，R-FCN通过全卷积的方式将分类得分转化到特征图通道上，使得特征图保持了一定的平移不变性（这个可以看我之前的博客），有利于跟踪任务的相关滤波。那么这里D&amp;T在传统目标检测的分类和回归任务上，增加了一个跟踪分支，作者巧妙地将跟踪任务转化成了预测相邻两帧各目标位置相对偏移量的回归任务。当然，跟踪分支只考虑与gt的IOU&gt;0.5的预测框，并且目标要同时出现在这两帧。多任务损失函数如下：最后我们谈一下最重要的一点，如何做ROI Tracking，即在不丢失相对位置关系的前提下，执行多个区域的相关滤波：提到相关滤波，我们可能容易想到单目标跟踪中的CF类传统方法，比如KCF（详细原理可以看我的解析）。KCF算法中就是通过循环移位的方式，利用相关滤波估计目标在图像中的位置变化。但是这种方式并不适合多目标的相关滤波，我们基于相邻两帧变化幅度不大的假设，更希望的是每块局部区域单独做类似于循环移位之类的操作。对此，作者借鉴了FlowNet的Corr操作，因为光流任务也是估计相邻帧像素的偏移量，所以用在这里很合适。Corr的公式是：可以看到，这里的滤波不是对卷积核的，而是将两幅特征图的多个kxk的区域分别做相关滤波，从而保持了相对位置。最后对于多目标跟踪的部分，作者对于两个目标的连接代价设置如下：其中p表示的相邻两帧的检测置信度，最后一项指的是相邻两帧的目标框与预测到的位置的IOU&gt;0.5时为1，否则为0。至此我们就可以得到跟踪预测位置和代价矩阵了，后面就是常规的多目标跟踪算法操作了。2.Real-time multiple people tracking with deeply learned candidate selection and person re-identification（MOTDT)作者：Long Chen, Haizhou Ai, Zijie Zhuang, Chong Shang备注信息：ICME2018论文链接：https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification代码链接：https://github.com/longcw/MOTDT这篇论文表面看上基于R-FCN检测框架的private多目标跟踪算法，不过与上一篇不同的是，作者只利用R-FCN对观测框进行进一步的前景/背景分类，即用于目标框的分类过滤，而且MOTDT将检测和跟踪框架分离了。作者的框架也是由现在多目标跟踪算法的通用模块组成的，即检测、外观模型和运动模型。这里我们就只关注他的算法流程：从算法流程可以清晰地看到，MOTDT的流程是：利用Kalman Filter完成目标的运动估计；将观测框和跟踪框合并，并做NMS操作，其中每个目标框的置信度得到了修正：这里面L表示的长度，通过上面两个公式，作者将检测置信度和跟踪轨迹置信度结合在一起了。提取ReID特征，先基于ReID相似度进行匹配，再对剩余的利用IOU进行关联。MOTDT这个算法框架很经典，对于后续的一些多目标跟踪算法也起到了启发作用。3.Tracking without bells and whistles(Tracktor++)作者：Philipp Bergmann，Tim Meinhardt，Laura Leal-Taixe备注信息：ICCV2019，MOT15~17: 46.6, 56.2. 56.3 MOTA(public）论文链接：https://arxiv.org/pdf/1903.05625.pdf代码链接：https://github.com/phil-bergmann/tracking_wo_bnwTracktor++算法是去年出现的一类全新的联合检测和跟踪的框架，这类框架与MOTDT框架最大的不同在于，检测部分不仅仅用于前景和背景的进一步分类，还利用回归对目标进行了进一步修正，因此关于这类框架属于public还是private得争论也存在，这里我们就不做过多的讨论了。只要熟悉两阶段目标检测算法的应该都能理解这个算法，其核心在于利用跟踪框和观测框代替原有的RPN模块，从而得到真正的观测框，最后利用数据关联实现跟踪框和观测框的匹配。流程图如下：有了检测模块的加持，自然对于检测质量进行了增强，所以效果也得到了大幅提升：可以看到，DPM、FRCNN和SDP三种检测器输入下的性能差距不大，然而DPM检测器的性能是很差的，所以Tracktor++这类算法对于平衡检测输入的效果提升很大。4.Multiple Object Tracking by Flowing and Fusing(FFT)作者：Jimuyang Zhang, Sanping Zhou, Xin Chang, Fangbin Wan, Jinjun Wang, Yang Wu, Dong Huang备注信息：MOT15~17: 46.3, 56.5. 56.5 MOTA(public）论文链接：https://arxiv.org/abs/2001.11180这篇文章也是基于Tracktor++的模式，做了很直接的一步操作，即直接增加一个光流预测分支，将Tracktor++中的跟踪框+观测框变成了光流预测框+观测框不过好处在于光流网络和Faster RCNN可以联合训练，在训练的时候RPN保留，不过从论文来看光流部分好像是固定权重的，其效果相对来说的确更好了：5.Towards Real-Time Multi-Object Tracking(JDE)作者：Zhongdao Wang，Liang Zheng，Yixuan Liu，Shengjin Wang备注信息：MOT16 74.8 MOTA(private), 22FPS!!论文链接：https://arxiv.org/pdf/1909.12605v1.pdf代码链接：https://github.com/Zhongdao/Towards-Realtime-MOTJDE这篇跟这次的主题不是很相符，但是考虑到这也是近期比较热门的实时多目标跟踪算法，我们也一起讲。它的框架出发点是为了增加特征的复用性，基于检测算法（作者采用的是YOLOv3），在原本的分类和回归分支上增加了一个表观特征提取的分支。文中作者重点介绍了多任务网络框架的训练方式，首先分析了三种Loss：对于triplet loss，这个在表观模型的metric learning任务中很常见，作者采用了batch hard模式，并提出了triplet loss的上界，推导很简单，关键在于多的那个1。为了更好地跟交叉熵损失函数进行比较，作者将上界进行了平滑。那么区别就在于g，g表示的正负样本的权重。在交叉熵损失函数中，所有的负样本都会参与计算，然而在triplet loss中，负样本是采样出来的，所以：作者通过实验也论证了上面的结论，所以在metric learning中作者采用了交叉熵损失函数。最后关于各个任务的损失函数的权重，作者提出了一种自适应平衡的加权方式：其中的s是一种度量不同任务下个体损失的不确定性因子，详细的原理可参见CVPR2018的《 Multi-task learning using uncertainty to weigh losses for scene geometry and semantics》关于方差不确定性对于多任务权重的影响分析。效果和速度都很诱人~6.Refinements in Motion and Appearance for Online Multi-Object Tracking(MIFT)作者：Piao Huang, Shoudong Han, Jun Zhao, Donghaisheng Liu, HongweiWang, En Yu, and Alex ChiChung Kot备注信息：MOT15~17: 60.1, 60.4, 48.1 MOTA(public）论文链接：https://arxiv.org/abs/2003.07177代码链接：https://github.com/nightmaredimple/libmot这篇也是我们团队基于Tracktor++框架做的一个框架，主要关注的是运动模型、表观模型和数据关联部分的改进，由于某些原因，我这里不能细讲。代码会慢慢开源，暂时没有完全开源。其中对于运动模型部分，我们将Kalman和ECC模型集成在一起，而不是将Kalman和ECC模型独立执行，实验证明融合的版本比分开的提升了1.4 MOTA。对于表观模型，我们考虑到特征对齐的因素，做了一点小改进，结合可视度预测设计了多任务的表观模型：并在观测框和跟踪轨迹特征比对的时候，考虑了跟踪轨迹历史信息，来进行自适应加权：通过上面的分析，我们可以知道的是，数据关联部分的特征相似度计算，不仅要进行n:m的Kalman更新过程（为了求马氏距离），还要进行m:(nxk)的表观特征比对，这个过程很耗时。所以我们利用3-D integral image快速将空间区域分配，使得特征相似度计算过程的复杂度降至O(m+n)。方法很巧妙，就是将每个观测框利用one-hot编码映射到特征图，这种方式比基于iou的要快很多：我后期又做了一些实验，效果比论文中的更好一些，MOT15~17: 48.1、60.4、60.1 MOTA(public）7.Tracking Objects as Points(CenterTrack)作者：Xingyi Zhou(CenterNet的作者), Vladlen Koltun, and Philipp Krähenbühl备注信息：同时实现了2D/3D多目标跟踪，包含人和车辆，MOT17：61.4(public）、67.3(private) MOTA, 22FPS!!!KITTI：89.4MOTA论文链接：http://arxiv.org/abs/2004.01177代码链接：https://github.com/xingyizhou/CenterTrackCenterTrack是CenterNet作者基于Tracktor++这类跟踪机制，通过将Faster RCNN换成CenterNet实现的一种多目标跟踪框架，因此跟踪框也就变成了跟踪中心点。通过上图我们可以大致分析出算法框架，除了对相邻两帧利用CenterNet进行检测之外，还利用了上文中提到的D&amp;T框架的策略，预测同时存在于两帧中目标的相对位移，由此进行跟踪预测。对于提供的观测框，作者通过将这些观测框的中心点映射到一张单通道的heatmap上，然后利用高斯模糊的方式将点的附近区域也考虑进去。因此CenterTrack相对于CenterNet的不同之处在于，输入维度增加了（两幅3维图像和一张观测位置heatmap），输出变成了两张图像的目标中心位置、大小和相对偏移。对于测试环节的数据关联部分，作者直接通过中心点的距离来判断是否匹配，是一种贪婪的方式，并非匈牙利算法那种全局的数据关联优化。在训练过程中，作者并非只用相邻帧进行训练，允许跨3帧。CenterTrack在MOT、KITTI和nuScenes等数据集上的2D/3D多行人/车辆跟踪任务上均取得了SOTA的成绩。参考文献[1] Feichtenhofer C, Pinz A, Zisserman A. Detect to track and track to detect[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2017. 3038-3046.[2] Chen L, Ai H, Zhuang Z, et al. Real-time multiple people tracking with deeply learned candidate selection and person re-identification[C]. in: 2018 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2018. 1-6.[3] Bergmann P, Meinhardt T, Leal-Taixe L. Tracking without bells and whistles[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 941-951.[4] Multiple Object Tracking by Flowing and Fusing[5] Towards Real-Time Multi-Object Tracking[6] Refinements in Motion and Appearance for Online Multi-Object Tracking[7] Tracking Objects as Points]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多目标跟踪中的相机运动模型]]></title>
    <url>%2F2020%2F03%2F27%2F%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E7%9B%B8%E6%9C%BA%E8%BF%90%E5%8A%A8%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言之前的博客中我介绍了Kalman滤波器，这个算法被广泛用于多目标跟踪任务中的行人运动模型。然而实际场景中存在有很多相机运动，仅仅依赖行人运动模型是不够的。这次我主要介绍下相机运动模型，以对极几何和ECC为主。完整的代码和示例我都放在了github。1 多目标跟踪中的相机运动在多目标跟踪场景中往往存在有复杂的运动模式，这些模式除了行人这类非刚性运动，还有相机这类刚性运动。以MOT Challenge数据集为例，其中就存在大量相机运动场景，甚至超过了静态相机场景数。比如MOT17-13号视频中车载相机在车辆转弯时对于两个运动速度较慢行人的视角：我们从示意图可以看到，由于车辆转弯速度很快，上一帧的行人位置映射到下一帧就变成了另一个的位置。因此相机运动对于多目标跟踪的影响很大，尤其是仅依赖运动信息的模型，相机的运动会严重干扰运动模型。2对极几何2.1 对极几何模型关于相机运动方面的知识，我在之前介绍单目深度估计中的无监督模型时介绍过，即将变化差异不剧烈的两帧画面近似看作不同相机视角下同一场景的画面，也就是对极几何，这一点可以看看《计算机视觉中的多视几何》中关于相机几何方面的知识：不过这里我需要先解释一下一些概念，以方便后续模型的讲解：基线[baseline]：直线CC’为基线。对极平面束[epipolar pencil]：以基线为轴的平面束。对极平面[epipolar plane]：任何包含基线的平面都称为对极平面。对极点[epipole]：摄像机的基线与每幅图像的交点。比如，上图中的点x和x’。对极线[epipolar line]：对极平面与图像的交线。5点共面：点x，x’，摄像机中心C、C’，空间点X是5点共面的。极线约束：两极线上点的对应关系。接下来，我们首先看一篇ACM MM2019的论文TNT[1]，这是一篇研究端到端运动信息和表观信息结合框架的论文：不过这里我们要讲的是其提出来的相机运动模型：我们可以看到，作者将行人运动和相机运动结合了，其中目标函数的第一部分是利用了对极几何中本质矩阵F的性质，相关的理论推导可以看下图：其中x表示的目标框的四个顶点的坐标信息，第二部分中作者则是假设两帧中的同一目标的形状近似不变。因此我们只需要求得本质矩阵F，即可根据上一帧目标框信息，利用最小二乘法求得下一帧目标框信息。关于本质矩阵F的求解，作者提到基于SURF特征点提取和Ransac采样进行估计。不过作者也没有给出详细的实现过程，我这里试着做一下理论推导。首先由于作者在目标函数中要求了目标框形状的一致性，那么我们不妨直接把下一帧目标框的形状信息看做已知的。其次，我们先假设本质矩阵F已经被估计出来了，这个矩阵是3x3的形状，那么为了推导方便，我这里做一个假设：对于第t帧的任意一个目标框的每一个节点$x$，这里由于是三维的几何信息，所以添加一个z轴坐标，令$x^TF$为一个已知的三维向量，那么一个目标框就存在四个这样的三维向量，不妨看作一个4x3的矩阵M那么就可以将目标函数展开，这里面的(w,h)为已知信息，(x,y)为下一帧目标框的左上角坐标：$$ \begin{array}{l} \left\{ \begin{array}{l} {M_{11}}x + {M_{12}}y + {M_{13}} = 0\\ {M_{21}}\left( {x + w} \right) + {M_{22}}y + {M_{23}} = 0\\ {M_{31}}x + {M_{32}}\left( {y + h} \right) + {M_{33}} = 0\\ {M_{41}}\left( {x + w} \right) + {M_{42}}\left( {y + h} \right) + {M_{43}} = 0 \end{array} \right.\\ \Rightarrow \left[ {\begin{array}{*{20}{c}} {{M_{11}}}&{{M_{12}}}\\ {{M_{21}}}&{{M_{22}}}\\ {{M_{31}}}&{{M_{32}}}\\ {{M_{41}}}&{{M_{42}}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ y \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{M_{13}}}\\ {{M_{23}}{\rm{ + }}w{M_{21}}}\\ {{M_{33}}{\rm{ + }}h{M_{32}}}\\ {{M_{43}}{\rm{ + }}w{M_{41}} + h{M_{42}}} \end{array}} \right] \end{array} $$很明显这就是一个典型的Ax=b问题，后面的问题就迎刃而解了。2.2 实验分析为了保证效率，我这里采用ORB特征提取策略，然后采用brute force的匹配策略：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Epipolar(object): def __init__(self, feature_method = 'orb', match_method = 'brute force', metric = cv2.NORM_HAMMING, n_points = 50, nfeatures = 500, scaleFactor = 1.2, nlevels = 8): """Using Epipolar Geometry to Estimate Camara Motion Parameters ---------- feature_method : str the method of feature extraction, the default is ORB, more methods will be added in the future match_method : str the method of feature matching, the default is brute force, more methods will be added in the future metric: metrics in cv2 distance metric for feature matching n_points: int numbers of matched points to be considered nfeatures: int numbers of features to be extract scaleFactor: float scale factor for orb nlevels: float levels for orb """ self.metric = metric if feature_method == 'orb': self.feature_extractor = cv2.ORB_create(nfeatures = nfeatures, scaleFactor = scaleFactor, nlevels = nlevels) if match_method == 'brute force': self.matcher = cv2.BFMatcher(metric, crossCheck=True) self.n_points = n_points def FeatureExtract(self, img): """Detect and Compute the input image's keypoints and descriptors Parameters ---------- img : ndarray of opencv An HxW(x3) matrix of img Returns ------- keypoints : List of cv2.KeyPoint using keypoint.pt can see (x,y) descriptors: List of descriptors[keypoints, features] keypoints: keypoints which a descriptor cannot be computed are removed features: An Nx32 ndarray of unit8 when using "orb" method """ if img.ndim == 3: img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # find the keypoints with ORB keypoints = self.feature_extractor.detect(img, None) # compute the descriptors with ORB keypoints, descriptors = self.feature_extractor.compute(img, keypoints) return keypoints, descriptors那么对于本质矩阵的估计和最小二乘法的应用，都可以直接利用已有的工具箱opencv和numpy搞定：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108def GetFundamentalMat(self, keypoints1, descriptors1, keypoints2, descriptors2): """Estimate FunfamentalMatrix using BF matcher and ransac [p2;1]^T K^(-T) E K^(-1) [p1;1] = 0, T means transpose, K means the intrinsic matrix of camera F = K^(-T) E K^(-1) Parameters ---------- keypoints : List of cv2.KeyPoint using keypoint.pt can see (x,y) descriptor : ndarray An Nx32 matrix of descriptors Returns ------- F: ndarray A 3x3 Matrix of Fundamental Matrix mask: ndarray A Nx1 Matrix of those inline points pts1: List of cv2.KeyPoint keypoints matched pts2: List of cv2.KeyPoint keypoints matched matches : List of matches distance - distance of two points, queryIdx - query image's descriptor id, default is the second image trainIdx - train image's descriptor id, default is the second image imageIdx - train image's id, default is 0 """ # matching points matches = self.matcher.match(descriptors1, descriptors2) matches = sorted(matches, key=lambda x: x.distance) pts1 = [] pts2 = [] for i, match in enumerate(matches): if i &gt;= self.n_points: break pts1.append(keypoints1[match.queryIdx].pt) pts2.append(keypoints2[match.trainIdx].pt) pts1 = np.int32(pts1) pts2 = np.int32(pts2) matches = matches[:self.n_points] ## Estimate Fundamental Matrix by ransac, distance_threshold = 1, confidence_threshold = 0.99 F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC, 1, 0.99) return F, mask, pts1, pts2, matchesdef EstimateBox(self, boxes, F): """Estimate box in target image by Fundamental Matrix Parameters ---------- boxes : array like A Nx4 matrix of boxes in source images (x,y,w,h) F : ndarray A 3x3 Fundamental Matrix Returns ------- aligned_boxes: ndarray A Nx4 matrix of boxes in source images (x,y,w,h) Method ------- L = ||Bi^T F Ai||2 + ||(A2-A0)+(B2-B0)||2 A is the four corner of box in source image B is the four corner of aligned box in target image A0,B0:top left corner of box, [x;y;1] A1,B1:top right corner of box A2,B2:bottom left corner of box A3,B3:bottom right corner of box the height and width of boxes and aligned boxes are assumed to be same we can use greedy strategy: make M = A^T F^T then: M11 x1 + M12 y1 + M13 = 0 M21 (x1+w) + M22 y1 + M23 = 0 M31 x1 + M32 y1+h + M33 = 0 M41 (x1+w) + M42 (y1+h) + M43 = 0 =&gt; M[:2][x;y] + M[:3]+[0;M21w;M32h;M41w+M42h] = 0 -&gt;Ax = b x = (pseudo inverse of A )b """ boxes = np.asarray(boxes) if boxes.ndim == 1: boxes = boxes[np.newaxis, :] aligned_boxes = np.zeros(boxes.shape) for i, bbox in enumerate(boxes): w = bbox[2] h = bbox[3] AT = np.array([[bbox[0] , bbox[1] , 1], [bbox[0] + w, bbox[1] , 1], [bbox[0] , bbox[1] + h, 1], [bbox[0] + w, bbox[1] + h, 1]]) M = AT @ F.T b = -M[:, 2] - np.array([0, M[1][0]*w, M[2][1]*h, M[3][0]*w+M[3][1]*h]) aligned_tl = np.linalg.pinv(M[:,:2]) @ b aligned_boxes[i, 0] = aligned_tl[0] aligned_boxes[i, 1] = aligned_tl[1] aligned_boxes[i, 2] = w aligned_boxes[i, 3] = h return aligned_boxes.astype(np.int32)具体效果如下：上面极线的法线也正好是车载相机的方向所在，可以看到第一章的示例问题被很大缓解了：3ECC3.1原理介绍第二章所介绍的对极几何方法，由于我们只是根据二维信息对三维信息的估计，所以也会存在误差。这一张我们也讲一个简单有效的方案，那就是“仿射变换”。当然，并不是我们所理解的那种仿射变换，具体细节我将慢慢介绍。第一次看到ECC算法，我是在ICCV2019的Tracktor++[3]中，不过作者只是一笔带过，没有提及如何实现。ECC算法全名是增强相关系数算法[2]，来自于PAMI2008的一篇论文，这个算法适用于图像配准任务的：也就是对于两张内容差异小，但是存在光照、尺度、颜色、平移等变换影响的图像，将二者对齐。ECC算法本质是一个目标函数：$$ ECC = {\left\| {\frac{{{x_i}}}{{\left\| {{x_i}} \right\|}} - \frac{{{y_i}}}{{\left\| {{y_i}} \right\|}}} \right\|^2},y = warp(x) $$当然这只是一个原始形式，在求解过程中有所调整，我就不细讲这里的理论了。可以注意到的是y=warp(x)这个函数，所以这个算法假设两帧图像之间存在某种变换，不一定是仿射变换，可能有以下几种：其中最后一种透视变换的矩阵形式是：$$ \left[ {\begin{array}{*{20}{c}} {x'}\\ {y'}\\ {z'} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{a_{11}}}&{{a_{12}}}&{{t_x}}\\ {{a_{21}}}&{{a_{22}}}&{{t_y}}\\ {{a_{23}}}&{{a_{24}}}&{{t_z}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ y\\ z \end{array}} \right] $$前三种变换则不考虑最后一行信息，即2x3的矩阵形式。3.2实验分析opencv中正好提供了ECC相关的功能函数，这里我们只需要再次封装，以方便多目标跟踪。可以知道的是ECC算法的核心在于变换矩阵的求解：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889def ECC(src, dst, warp_mode = cv2.MOTION_EUCLIDEAN, eps = 1e-5, max_iter = 100, scale = None, align = False): """Compute the warp matrix from src to dst. Parameters ---------- src : ndarray An NxM matrix of source img(BGR or Gray), it must be the same format as dst. dst : ndarray An NxM matrix of target img(BGR or Gray). warp_mode: flags of opencv translation: cv2.MOTION_TRANSLATION rotated and shifted: cv2.MOTION_EUCLIDEAN affine(shift,rotated,shear): cv2.MOTION_AFFINE homography(3d): cv2.MOTION_HOMOGRAPHY eps: float the threshold of the increment in the correlation coefficient between two iterations max_iter: int the number of iterations. scale: float or [int, int] scale_ratio: float scale_size: [W, H] align: bool whether to warp affine or perspective transforms to the source image Returns ------- warp matrix : ndarray Returns the warp matrix from src to dst. if motion model is homography, the warp matrix will be 3x3, otherwise 2x3 src_aligned: ndarray aligned source image of gray """ assert src.shape == dst.shape, "the source image must be the same format to the target image!" # BGR2GRAY if src.ndim == 3: # Convert images to grayscale src = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY) dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY) # make the imgs smaller to speed up if scale is not None: if isinstance(scale, float) or isinstance(scale, int): if scale != 1: src_r = cv2.resize(src, (0, 0), fx = scale, fy = scale,interpolation = cv2.INTER_LINEAR) dst_r = cv2.resize(dst, (0, 0), fx = scale, fy = scale,interpolation = cv2.INTER_LINEAR) scale = [scale, scale] else: src_r, dst_r = src, dst scale = None else: if scale[0] != src.shape[1] and scale[1] != src.shape[0]: src_r = cv2.resize(src, (scale[0], scale[1]), interpolation = cv2.INTER_LINEAR) dst_r = cv2.resize(dst, (scale[0], scale[1]), interpolation=cv2.INTER_LINEAR) scale = [scale[0] / src.shape[1], scale[1] / src.shape[0]] else: src_r, dst_r = src, dst scale = None else: src_r, dst_r = src, dst # Define 2x3 or 3x3 matrices and initialize the matrix to identity if warp_mode == cv2.MOTION_HOMOGRAPHY : warp_matrix = np.eye(3, 3, dtype=np.float32) else : warp_matrix = np.eye(2, 3, dtype=np.float32) # Define termination criteria criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, max_iter, eps) # Run the ECC algorithm. The results are stored in warp_matrix. (cc, warp_matrix) = cv2.findTransformECC (src_r, dst_r, warp_matrix, warp_mode, criteria, None, 1) if scale is not None: warp_matrix[0, 2] = warp_matrix[0, 2] / scale[0] warp_matrix[1, 2] = warp_matrix[1, 2] / scale[1] if align: sz = src.shape if warp_mode == cv2.MOTION_HOMOGRAPHY: # Use warpPerspective for Homography src_aligned = cv2.warpPerspective(src, warp_matrix, (sz[1],sz[0]), flags=cv2.INTER_LINEAR) else : # Use warpAffine for Translation, Euclidean and Affine src_aligned = cv2.warpAffine(src, warp_matrix, (sz[1],sz[0]), flags=cv2.INTER_LINEAR) return warp_matrix, src_aligned else: return warp_matrix, None这里面我添加了一个技巧，由于ECC算法针对的是两幅图，所以图像的尺寸对于算法求解速度的影响很大。因此这里我根据变换矩阵的形式，设计了一种可以根据尺度放缩自动调节的简易算法。效果如下：效果也很好，值得一提的是，ECC算法只需要大约几毫秒的时间，但是由于它的求解效率跟变换的难度相关，所以间隔越久越慢，而对极几何的方法效率比较稳定，不过就很慢了。4 其他近似方案4.1光流上面我介绍的都是近两年关于相机运动的针对性解决方案，那么实际上在有一些算法模型中，如果场景变化不剧烈，并不特别需要用到运动模型。比如基于光流法的多目标跟踪算法，这里众所周知的就是ICCV2015的NOMT[5]算法。作者用的是一种简化版的快速光流法，那么更形象的可以看今年刚出的一篇论文《Multiple Object Tracking by Flowing and Fusing》，具体我就不说了，就是简单的在Tracktor++框架上加了一个光流预测分支：可以看到的是，光流也是在捕捉相邻帧中相似的像素信息，这一点跟第二章中提出的两种相机运动模型有点类似，所以不需要显式使用相机运动模型。4.2SOT而基于SOT的方法，无论是使用传统的相关滤波算法还是使用Siamese类深度学习框架，都会在上一帧目标周围1.5~2.5倍区域搜索下一帧的目标，这里面会显式或者隐式用到特征的比对。只不过不同于上面的像素比对，这里是更加高层的特征比对。参考资料[1] Wang G, Wang Y, Zhang H, et al. Exploit the connectivity: Multi-object tracking with trackletnet[C]. in: Proceedings of the 27th ACM International Conference on Multimedia. 2019. 482-490.[2] Evangelidis G D, Psarakis E Z. Parametric image alignment using enhanced correlation coefficient maximization[J]. IEEE transactions on pattern analysis and machine intelligence, 2008, 30(10): 1858-1865.[3] Bergmann P, Meinhardt T, Leal-Taixe L. Tracking without bells and whistles[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 941-951.[4] Choi W. Near-online multi-target tracking with aggregated local flow descriptor[C]. in: Proceedings of the IEEE international conference on computer vision. 2015. 3029-3037.[5] Feng W, Hu Z, Wu W, et al. Multi-object tracking with multiple cues and switcher-aware classification[J]. arXiv preprint arXiv:1901.06129, 2019.[6] https://blog.csdn.net/ssw_1990/article/details/53355572]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>相机运动</tag>
        <tag>对极几何</tag>
        <tag>ECC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测中的特征冲突与不对齐问题]]></title>
    <url>%2F2020%2F03%2F20%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%86%B2%E7%AA%81%E4%B8%8E%E4%B8%8D%E5%AF%B9%E9%BD%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言昨天看到一篇商汤的刷榜文《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》，里面的每个技巧我们都见过，还有很多依靠大量计算资源的参数搜索和模型集成。不过其中关于回归和分类的冲突勾起了我的回忆，去年整理了一些相关的文章。我准备在简要介绍这片文章的同时，谈谈目标检测（two-stage和one-stage）中特征的冲突和不对齐问题，以及现有的改进方案。1 Two-stage目标检测中的特征/任务冲突问题1.1 Two-stage目标检测的流程与原理说起两阶段目标检测算法，大家耳熟能详的就是Faster RCNN系列了，目前的大多数两阶段算法也都是在其基础上进行的改进。不过现在新出的很多“N-阶段”的算法把大家搞混了。所以我这里申明一下两阶段的意义，我们通常说的两阶段是以Faster RCNN算法为基准的，第一阶段是特征提取和候选框提取，主要是RPN网络，第二阶段是对候选框进行进一步筛选、精修和细分类，主要是ROI Pooling/Align等网络。在我的上一篇博文中提到过两阶段目标检测的关于平移不变性和相等性的矛盾问题，这里我们详细探讨一下。两阶段目标检测中，第一阶段做的事前/背景分类和候选框回归，第二阶段做的是候选框精修回归和细分类。正如之前所讨论的，分类任务希望无论目标的位置和形状怎么变化，什么类别的目标就是什么类别，即需要保证平移和尺度的不变性。而回归任务，我在上一篇博文中提到了，对于物体位置的回归很大程度可能依赖padding信息，当然这不是这次的讨论重点，回归需要保证目标的位置和形状变化反映在特征上，进而回归得到位置，即平移和尺度的相等性。这一问题在行人检索中更加严重，因为行人检索问题中的识别任务要求同类目标的不同身份要区分开，这一点就与目标检测中的分类任务相违背，因为检测中的分类不论什么身份，只要属于同一类别即可。1.2 现有的相关解决方案在正式介绍相关改进策略之前，我们先提提Cascade RCNN算法[3]，其原理如下：要注意的是Iterative bbox方式和Cascade RCNN方式的形式虽然一样，但是不同之处在于前者仅仅是用于测试阶段，可以观察到都是head network都是一样的，而后者各个head network都是训练来的。从形式来看，很明显就是将最后的分类和回归分支级联做了3次。这样做的依据就是：第一幅图中横坐标是回归前候选框与gt的iou，纵坐标是回归后的iou，可以看到不同的候选框质量对于回归效果也有影响。第二幅图中基于不同iou阈值训练得到的网络对于AP也有影响。再考虑到训练集和测试集内样本分布的不同，作者采用分而治之的策略，分别用{0.5,0.6,0.7}三种IOU阈值级联训练。这里提到了各阶段的具体训练方式：分类和回归都是一个模式，不仅用gt的标签，还用到了上一阶段的结果作为标签，来保证结果的稳定性。最后我们来看看各种方案的对比实验结果：可以看到的是iterative bbox(以不同iou阈值做多次nms)和integral loss(以不同iou阈值并联多个回归和分类过程)都能提升一点点AP，但是Cascade RCNN(以不同iou阈值级联多个回归和分类过程)效果提升最大。好了，我们回归正题，Cascade RCNN从样本质量分布mismatch和iou等角度进行了级联的refine操作。那么在IOU-Net[4]则是显式地说明了分类的分数不适合用于NMS的过滤，因为分类置信度高的样本不一定真的好。因此作者增加了一个样本与gt的iou预测阶段，以此作为NMS的排序依据。这里实际上就说明了分类和回归的冲突问题。至于为了提高预测精度的PrROI-pooling,我就不仔细分析其原理了，不是这里的讨论重点：那么真正意义上把分类和回归问题放在明面上的我觉得是Double-Head RCNN[2],来自于18年COCO检测冠军旷视团队。我们可以看到，不同于传统的将回归和分类放在最后阶段，利用两个全连接分支来预测，Double-Head直接从ROI Align之后就将两个人任务分开了，尽可能减少二者共享的特征部分。而Double-Head-Ext方案则是让两个分支都能预测类别和位置。可以看到，四种方案下平衡两个分支损失函数权重后，后两种的效果明显更好。最后我们来看看CVPR2019的Guided Anchoring算法。这个算法解决的是anchor的设计问题，而anchor的设计需要解决形状对齐和特征一致性的问题。其中形状对齐指的是以往anchor的尺寸和长宽比都是预设的固定几个，首先这也是超参数，其次无法适应多样的样本形状，因此该算法以特征图每个点作为中心，先预测anchor的长宽，再用于预测。而特征一致性问题则是一个很巧妙的问题，原因在于，同一层的特征图上每个点的感受野一致，但是预测到的anchor尺寸却不同，那么基于不同大小的anchor来做的分类任务却基于相同的特征感受野，这显然是存在问题的。所以作者基于预测得到的anchor长宽，利用deform-conv为每个anchor分配了新的特征区域，其中deform-conv中的offset直接采用预测得到的anchor长宽。2 One-stage目标检测中的特征不对齐问题2.1 One-stage目标检测中的问题One-stage目标检测算法，以YOLO系列、SSD系列、RetinaNet等为经典，下面是YOLOv3的网络流程：我们可以看到的是单阶段的目标检测算法相当于取消了RPN阶段，所以两阶段目标检测中遇到的问题（分类与回归特征冲突，anchor与特征不对齐），在单阶段目标检测中只会更加严重。不过单阶段目标检测的目标就是提升速度，所以我目前并没有看到对第一个问题的解决方案，而去年对于anchor与特征不对齐的问题有好多解决策略。原因在于两阶段目标检测中ROI Pooling本身有一个利用候选框裁剪特征区域的过程，缓解了这一问题，而单阶段目标检测却没有这一过程。2.2 “1.5-stage”解决策略CVPR2018有一篇RefineDet算法[9]，这个算法是针对SSD算法的改进，融合了单阶段和两阶段的设计思路，但又不是我们之前所说的RPN+ROIPooling这类框架，所以就叫它“1.5stage”检测框架吧。RefineDet有两个模块，其中上面是ARM，用于调整anchors的位置和大小，下面是ODM，用于目标检测。这个跟Guided Anchoring的设计思路很像，不过比较简陋。除此之外，RefineDet还采用了级联预测的模式，利用中间的TCB模块，其通过Deconv和特征Concat反向级联，类似于FPN的模式。同样地，相同的团队在AAAI2019的一篇人脸检测算法SRN[6]也用了RefineDet的框架：可以看到整体框架很像，但是却有所不同，SRN框架包含有STC+STR+RFE三个模块。其中STC模块作用于浅层网络，用于过滤掉大部分的负样本，STR作用于高层特征，用于粗略调整anchor，类似于RefineDet。而RFE则是在接受各个尺度特征的同时，利用非正方形的卷积核对感受野进行增强（考虑到人脸不一定是正的）。在ICCV2019中有一篇比较特别的检测算法Reppoints[7]，其出来的时机正好是anchor-free算法大火的时候，其框架比较特别，可以看作是DCN+Refine操作的集成，有人也称其为DCNv3：这个框架的特别之处在于没有预测框，没有预测中心或者角点，而是预测的目标边缘的九个点。不过我觉得这几个特征点更像是一种解释，而不是出发点。其原理是以特征图上每个点为中心，预测包含该位置的目标的九个边缘点。其方式是通过卷积的方式预测各个点的相对位置(x,y)偏移，以此作为Deform Conv的偏移量对原特征图进行卷积，由此使得特征与目标区域更加重合，从而进行第二阶段的预测。可以发现，Reppoints很像anchor free版的Guided Anchoring，而之前提到的RefineDet和SRN虽然提到了anchor预更新，但是特征并没有校正。WACV2020的一片P&amp;A算法[5]算是对上面的不足做了完善，但是我感觉像是把Guided Anchoring中的Feature Adaption直接搬过来了，为什么这么说呢。因为P&amp;A也是先预测anchor偏移和前景背景分类，然后以此作为deform conv的offsets对特征重提取，再进行目标位置回归和细分类。同时间出来的AlignDet[8]则是提出了ROIConv：上图中(a)指的就是RefineDet类的对齐，(b)就是Reppoints一类的对齐，(c)就是Guided Anchoring类的对齐，(d)就是AlignDet类的对齐。AlignDet把基于anchor偏移量的特征对齐称作ROIConv，还分析了具体的偏移校正过程：可以看到有两次预测过程，作者采用了Cascade的 方式，两次的IOU阈值不同。其实仔细看的话P&amp;A和AlignDet的结构几乎一模一样，看评审怎么看吧，估计也是考虑到这方面因素给挂了。3 《1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation》介绍现在我们来看看商汤在OpenImage2019上的文章，可以当作技术报告来看。我们直接按照论文提到的创新点或者工作来一一说明。Decoupling Head作者出发点是目标检测框架中分类的回归任务对于特征的要求不同，而这一点在我上面提到的Double Head RCNN已经提过了。Decoupling Head则是考虑到我们前文提到的anchor和特征不对齐问题，利用传统的ROI Pooling主干预测anchor的粗略位置，然后用deform conv的方式校正分类分支。再在主干上保留原始的回归和分类任务。总而言之可以将其概括为：Double Head RCNN + AlignDet + Faster RCNNAdj-NMS这部分作者的描述方案很“有意思”，作者考虑到NMS和soft-NMS的不足，先利用0.5的IOU阈值做了一次NMS，将靠得比较近的候选框过滤掉了，然后再用基于高斯核的soft-NMS做二次过滤。我们可以根据这个公式来看看，假设分类置信度阈值为0.5，候选框分类置信度为1，那么Soft-NMS阶段要想留下，IOU必须小于0.59，而第一次的NMS已经将IOU&gt;0.5的候选框过滤掉了，所以这个理论上可行。因此我们可以认为作者几乎不怎么考虑特别密集拥挤的场景了。其效果也有0.174个点的提升。其实如果注意的话，有点像前文介绍Cascade RCNN是所提到了Iterative bbox策略，即做多次NMS。SoftNMS只能通过重新打分捞回原本得分比较低的样本，但是NMS已经将大部分的候选框给过滤掉了，所以我很好奇这是怎么生效的。Model Ensemble很多大型比赛的固定策略“Ensemble”，已经不奇怪了。naive ensemble的策略是借鉴的2018年的OpenImage第二名，给定bounding boxes(P)，以及topk个与之IOU较高的候选框，依据验证集的分数来分配各个模型在集成时的权重，这里还分各个目标类别，然后进行加权：这里作者训练了28个目标检测网络….，利用二叉树的方式进行模型空间搜索。Data Re-sampling确保500个类别的目标中各个类别被选取的概率相等。Decoupling Backbone对于第25~28个模型，采取Decouple Head的策略，其中回归分支的权重较小。Elaborate Augmentation随机选择一个类别，利用旋转放缩裁剪等方式进行数据增强，这样可以使得一幅图中的类别数变少，缓解数据不平衡问题。Expert Model利用专门的网络训练专门的子类别数据集，这里面考虑了正负样本均衡的问题，容易混淆（标注标准不同，表观相似）的样本。AnchorSelecting跟YOLO系列一样，利用k-means方法得到18组anchors(6种长宽比，3种尺寸)。Cascade RCNN设置了0.5,0.5,0.6,0.7四个阶段的级联检测，这我就搞不懂Adj-NMS干嘛用的了。Weakly Supervised Training由于OpenImage数据集中各类别的“长尾分布”很明显，严重不均衡，所以作者增加了一些图像级的标注，结合有监督和WSDDN算法中的弱监督算法联合训练。Relationships Between Categories作者通过分析数据集中部分类别目标之间的联系，比如person和guitar等等，类似于条件概率，来修正分类置信度，比如一个有person在旁边的guitar要比没有person的guitar置信度要高。Data Understanding作者发现OpenImage数据集中对于特定类别的目标标注有歧义，比如火炬和手电筒，剑和匕首等，所以作者将有歧义的类别细分成了上面说的多类。同时作者也发现有些目标，比如葡萄缺乏个体检测框等，作者就利用葡萄串的实例标注，扩展了很多葡萄框。最后的分割部分我就不细讲了，就是基于HRNet和Ensemble的方式进行的实验。4 说在后面的话实际上目标检测任务与多目标跟踪（MOT）也有很多联系，比如MOT数据集中的MOT17Det，又比如新出的基于类检测框架的Tracktor++算法，检测跟踪结合的框架JDE算法等。多目标跟踪领域绝不是一个局限于数据关联的独立领域，应该是个多领域融合的方向。之前基于COCO的预训练模型在MOT17数据集上试了下，在MOT17Det上居然还有0.88AP，然后我基于这个又复现了下Tracktor++，居然也达到了58+MOTA，后面有机会我放github吧。对了，还有个Crowdhuman人体检测的算法分享。唉，公司又推迟入职时间了，先申请看能不能提前入职吧，不然只能在家减肥看论文做实验了…参考资料[1] 1st Place Solutions for OpenImage2019 - Object Detection and Instance Segmentation.[2] Rethinking Classification and Localization for Object Detection[3] Cascade r-cnn_ Delving into high quality object detection[4] Acquisition of Localization Confidence for Accurate Object Detection[5] Propose-and-Attend Single Shot Detector[6] Selective Refinement Network for Face Detection[7] Reppoints_ Point set representation for object detection[8] Revisiting feature alignment for one-stage object detection[9] Single-Shot Refinement Neural Network for Object Detection[10] https://zhuanlan.zhihu.com/p/63273342]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>特征</tag>
        <tag>检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈CNN中的位置和尺度问题]]></title>
    <url>%2F2020%2F03%2F16%2F%E8%B0%88%E8%B0%88CNN%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%92%8C%E5%B0%BA%E5%BA%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言前段时间看到了几篇有意思的文章，也参考了一些相关的讨论，这里想对CNN中的平移和尺度的不变性和相等性，以及CNN对于目标相对和绝对位置、深度的预测原理进行探讨。这些内容对于一些特定任务很重要，比如目标检测、目标分割、深度估计、分类/识别以及单目标跟踪中的置信图预测等。1 CNN是否存在平移和尺度的不变性和相等性1.1 不变性和相等性的定义​ 在介绍卷积神经网络（CNN）之前，我们对于不变性和相等性的理解可能来自于传统图像处理算法中的，平移、旋转、光照和尺度等不变性，比如HOG梯度方向直方图，由于cell的存在，其对于平移、旋转有一定的不变性，另外由于对图像局部对比度归一化的操作，使其对于光照也有着一定的不变性。又比如说SIFT特征提取，其对于以上四点都有着不变性，其中由于尺度金字塔，使得对尺度也有不变性。这里我们对于不变性的理解就是，同一对象发生平移、旋转、光照变化、尺度变换甚至形变等，其属性应该一致。下面我们给出具体的不变性和相等性的定义。​ 其中不变性（invariance）的定义正如上文所说，因此其形式为：$$ F\left( x \right) = F\left[ {transform\left( x \right)} \right] $$​ 而对于相等性（equivalence），顾名思义，就是对输入进行变换之后，输出也发生相应的变换：$$ transform\left[ {F\left( x \right)} \right] = F\left[ {transform\left( x \right)} \right] $$​ 不过如果我们只考虑输出对于输入不变性和相等性的情况，则会难以理解，因为我们更多地是想象着特征层面的映射，比如：​ 那么特征层面对于输出的影响我们可能考虑得比较少，但是却实质存在，比如目标在图像中的平移和尺度等变换，在目标检测任务中，必须要使得网络具有相关的变换相等性，由此捕捉目标的位置和形状变化。而在图像分类、目标识别、行人重识别等任务中，又必须使得网络具有相关变换的不变性。这两点也是目标检测和行人检索领域中一个经典的矛盾问题，目前好像还没有特别好的解决，更多地是分阶段地执行不同的任务，防止特征共用。比如：经典的两阶段目标检测任务中，第一阶段是粗检测和前景背景分类，第二阶段是精修和具体类别分类，有一定的偏重。行人检索算法则大多是先检测后识别的策略。当然除了不变性和相等性的问题，还存在类内差异的问题，比如不同的人对于检测而言都是行人类别，对于识别而言则是不同的人，这对于特征提取也存在挑战。1.2 CNN网络的执行过程​ 我记得我几年前第一次接触到深度学习的时候，对于全连接和CNN的局部连接形式，都有平移、尺度不变性的说法。对于全连接网络，由于下一层的每个节点都会与上一层进行连接：​ 因此无论输入发生了平移、尺度等什么变换，只要其属性没变，全连接网络更能捕捉其中的不变性。而对于卷积神经网络，我们都知道两个特点：局部连接和权值共享。​ 对于局部连接，因为全连接参数太多，容易造成过拟合，并且图像领域更多地关注局部细节信息，所以局部连接方式有效。至于权值共享，也有减少参数的作用，很像图像处理中的滤波器。我们早期对于其不变性的理解更多是遵循一个宏观的感受，即由于卷积核的移位滤波，上一层的特征到下一层的特征相对位置宏观不变，直到最后输出，类似于全连接的效果，从而获得不变性。1.3CNN网络潜在问题与改进​ 正因为我刚说的宏观不变，使得输入在经过多次卷积、池化之后，微观/细节的变化累积放大，从而失去了这种不变性，接下来我会结合两篇论文进行介绍。​ 第一个是为了解决CNN平移不变性对抗性攻击的一篇ICML2019论文《Making Convolutional Networks Shift-Invariant Again》。这篇文章主要讨论了CNN网络中的降采样对于平移不变性的影响：​ 上图是对于一个窗户分别采用从0~7的平移量，其特征图与不平移的差异，可以明显看到，特征图出现了波动。相应地，上半部分是利用pix2pix生成的图像，我们可以看到随着平移量的增大，窗户中的竖直线从两根变成了一根。这一点就表明传统的CNN网络并不具有平移不变性。​ 首先，作者做了这样一个小实验，即采用maxpooling对一维向量[0011001100]进行池化，由此得到向量[01010]：​ 接着，如果将输入向右平移一个单位，则得到向量[111111]：​ 很明显，平移相等性和不变性都丢失了。接着作者做了进一步实验，利用余弦距离来刻画平移相等性，采用VGG网络对Cifar数据集进行试验：​ 其中颜色越深说明差异越大，可以看到每次maxpooling都增加了特征的差异性，不过作者将max和pool操作分开了，为了区分取最大值和降采样的影响：​ 很明显，降采样对于平移相等性的影响更大，而CNN中涉及到降采样的操作有：池化（maxpooling和average pooling）和带步长的卷积（strided convolution）。对此作者提出了一种名为Anti_aliasing方法，中文叫做抗锯齿处理。传统信号处理领域中对于抗锯齿的技术，一般要么增大采样频率，但由于图像处理任务一般都需要降采样，这一点不适合。要么采用图像模糊（bluring）技术，根据Nyquist采样理论，是给定采样频率，通过降低原始信号的频率来使得信号能够被重构出来，如下图所示。对模糊化处理和未处理的原图像进行下采样，得到图中底部的两张图，模糊化处理的原图像下采样的图像还能看出一些轮廓，而未处理的原图像下采样的图像就显得更加混乱。​ 作者就是采用了模糊的方式，提出了三种不同的blur kernel：Rectangle-2：[1, 1]，类似于均值池化和最近邻插值；Triangle-2：[1, 2, 1]，类似于双线性插值；Binomial-5：[1, 4, 6, 4, 1]，这个被用在拉普拉斯金字塔中。​ 每个核都需要归一化，即除以核内所有元素之和，然后加入到降采样过程，即在降采样之前使用blur kernel进行卷积滤波：​ 可以看到其效果很不错：​ ​ 代码和模型见：https://richzhang.github.io/antialiased-cnns/或者https://github.com/adobe/antialiased-cnns​ 第二篇是同年发表在JMLR的一篇论文《Why do deep convolutional networks generalize so poorly to small image transformations?》。作者首先给出了几组示例，分别表示了平移、尺度以及轻微图像差异对网络预测分类置信度的影响：​ 作者认为CNN忽视了采样定理，这一点之前Simoncelli等人已经在论文Shiftable multiscale transforms中验证了二次采样在平移不变性上的失败，他们在文中说：我们不能简单地把系统中的平移不变性寄希望于卷积和二次采样，输入信号的平移不意味着变换系数的简单平移，除非这个平移是每个二次采样因子的倍数。​ 我们现有的网络框架中，越深的网络，降采样次数越多，因此出现的问题更多。紧接着，作者提出了几点论述：如果$r\left( x \right)$是经过卷积操作且满足平移不变性的特征，那么全局池化$\sum\nolimits_x {r\left( x \right)} $操作也满足平移不变性；对于特征提取器$r\left( x \right)$和降采样因子$s$，如果输入的平移都可以在输出上线性插值反映出来：$$ r\left( x \right) = \sum\limits_i {B\left( {x - {x_i}} \right)} r\left( {{x_i}} \right) $$由香农-奈奎斯特定理知，$r\left( x \right)$满足可移位性，要保证采样频率至少为最高信号频率的2倍。​ 接下来，作者对这些问题做了一些改进尝试：抗锯齿，这个就是我们刚刚介绍的方法；数据增强，当前在很多图像任务中，我们基本都会采用随机裁剪、多尺度、颜色抖动等等数据增强手段，的确也让网络学习到了部分不变性；减少降采样，也就是说只依赖卷积对于输入尺度的减小来变化，这一点只对小图像适用，主要是因为计算代价太高。2 CNN对于位置和深度信息的预测2.1CNN如何获取目标的位置信息​ 最早接触神经网络和深度学习相关任务时，我的感觉就是这类算法本质是做的分类任务，比如图像分割是对前景背景的分类和具体类别分类，识别任务就是类间类内的区分任务。其中图像分割任务就利用了CNN中的部分相等性，那么对于目标检测任务中的目标位置回归是怎么获取的呢？我们可以知道的是同样是对目标位置的搜索，在单目标跟踪任务中，存在有置信图：​ 但是置信图本质上是对区域进行搜索得到的，因此可以近似为对多个子区域的识别过程，所以单目标跟踪中的目标定位也可以用分类的理解，但是目标检测则不好用分类来理解了。​ 接下来我们思考一个问题，我们所设计的网络究竟包含了哪些信息？图像特征、网络结构（卷积核大小、padding）。从上一章我们可以知道，网络可以学习到一定的相等性：​ 因此，通过不断地训练，网络在最后的特征输出中是可以通过对应的特征数值和区域大小，结合一定的倍数（降采样大小）估计目标尺度的。但是对于目标的位置，我们人眼对于目标位置的判定是通过坐标系的，即目标距离图像的边缘距离，但是网络是如何了解这一信息的呢？《How much Position Information Do Convolutional Neural Networks Encode?》这篇文章做出了回答。​ 作者首先做了一组实验：​ 对于不同的输入图像，采用多种mask扰动，H、V、G分别代表水平、竖直和高斯等分布，用这种方式生成多种groundtruth，对于这点我们可能对单目标跟踪中以目标中心为均值的高斯分布比较熟悉。结果发现：​ GT是三种分布生成的groundtruth，PosENet是作者提出的网络，没有padding。我们可以看到PosENet并没有得到位置分布信息，而是保持了相等性。而存在padding的VGG和ResNet则都预测出了位置分布。由此可见padding对于位置的作用，也对上一章的内容作了补充，padding对于平移不变性和相等性也有影响。​ 不过这里我们可能不好理解，我做了个小测试，不过不一定是这么做的，仅仅方便我理解：$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&1&2&3\\ 0&4&5&6\\ 0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} {12}&{21}\\ {12}&{21} \end{array}} \right]\\ \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&0&0&0\\ 1&2&3&0\\ 4&5&6&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 6&5\\ {21}&{16} \end{array}} \right] \end{array} $$​ 上面是两个不同位置“目标”的卷积结果，可以看到，从输出结果得不到什么位置反映，如果加入padding：$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&0&1&2&3&0\\ 0&0&4&5&6&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 1&3&6&5\\ 5&{12}&{21}&{16}\\ 5&{12}&{21}&{16}\\ 4&9&{15}&{11} \end{array}} \right]\\ \left[ {\begin{array}{*{20}{c}} 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&1&2&3&0&0\\ 0&4&5&6&0&0\\ 0&0&0&0&0&0 \end{array}} \right] \otimes \left[ {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right]{\rm{ = }}\left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 3&6&5&3\\ {12}&{21}&{16}&9\\ {12}&{21}&{16}&9 \end{array}} \right] \end{array} $$​ 首先我们可以知道的是，加入了zero-padding之后，目标边缘相对中心的输出更小了，其次目标距离图像边缘距离越远，其特征映射中出现0的次数越多。所以我猜网络就是在训练过程中让padding和这个相对的关系对应上了，如果没有padding，目标距离边缘越远，同样出现0的次数也会越多，但问题在于无法跟padding造成的边缘数值小，中心数值大的特殊分布相比。当然，以上仅仅是我个人的理解，为了帮助我加深印象罢了。也有人说加入了padding影响了CNN的平移相等性，从而使得CNN学到了位置信息，但这个不大好解释。​ 不过有关padding的问题，在CVPR2019的一片单目标跟踪算法SiamRPN++中也做了探讨。其出发点在于为何Siamese网络无法扩展为大型网络，其原因就在于padding影响了平移相等性，从而让目标位置的分布发生了偏移。所以作者通过对目标中心的随机扰动，使得网络克服自身的偏移：​ 也有一些研究探索了如何让CNN结合绝对位置信息，比较出名的应该是当前很热门的SOLO单阶段实力分割算法。SOLO的出发点很简单，我们都知道语义分割只需要分割出不同类别的目标，而实力分割对于同一类别的个体还需要区分。但是显而易见，同一类别的目标只要位置和形状不同则可以区分。因此SOLO就是将位置和形状（用尺寸简化）信息结合进来。具体而言，就是将输入系统的图像统一划分为S x S的网格，如果对象的中心落入网格单元，那么这个网格单元就负责预测语义类别以及分割该对象实例。​ 特别地，SOLO算法中采用CoordConv策略（代码：https://github.com/uber-research/coordconv），该算法将每个特征区域的坐标信息结合进来，从而让网络显示地学会记忆特征的绝对位置信息。SOLO通过这个策略提升了3.6AP，并且论证只需要一层提供该信息即可达到稳定的提升。2.2CNN如何预测目标的深度信息​ 深度估计也是一个类似的问题，不同的是，图像中并没有包含深度信息，但是网络是如何获取深度信息的呢。How Do Neural Networks See Depth in Single Images?这篇文章给出了回答，关于这篇文章NaiyanWang老师已经在博客里讨论过，我这里也就再整理下。​ ​ 我们可以看到，物体的绝对深度与相机位姿有着很大关系，那么CNN如何学习到这种需要几何先验的绝对信息的呢？作者做了这样一个实验：​ 上图中作者做了三组实验：同时变化目标位置和尺寸、只变化位置以及只变化尺寸，我们从上面的定性结果好像看不出什么问题，下面是定量的结果：​ 可以发现，尺度对于深度信息的预测没有什么影响，也就是说CNN网络是通过目标纵坐标来估计深度的，所以说网络实际上是在过拟合训练集，从中学习到一些固定场景下的深度和相对位置的对应关系。​ 作者又讨论了相机运动对于深度预测的影响，既然深度与目标纵坐标有关系，那么pitch角的影响应该很大：​ 可以发现，pitch的确影响比较大，相对的roll的影响就比较小了：​ 最后作者还讨论了颜色和纹理对深度估计的影响：​ 可以发现，仅仅是改变目标的颜色，深度估计的效果也会下降，可将CNN网络在训练时有多“偷懒”，不知道如果将上述实验变成数据增强的手段的话会让深度估计网络失效还是变强。参考资料[1] https://zhuanlan.zhihu.com/p/99766566[2] https://zhuanlan.zhihu.com/p/95758284[3] https://zhuanlan.zhihu.com/p/38024868[4] Zhang R. Making Convolutional Networks Shift-Invariant Again[C]//International Conference on Machine Learning. 2019: 7324-7334.[5] Dijk T, Croon G. How Do Neural Networks See Depth in Single Images?[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 2183-2191.[6]Islam M A, Jia S, Bruce N D B. How much Position Information Do Convolutional Neural Networks Encode?[C]//International Conference on Learning Representations. 2019.[7]Wang X, Kong T, Shen C, et al. SOLO: Segmenting Objects by Locations[J]. arXiv preprint arXiv:1912.04488, 2019.[8]Liu R, Lehman J, Molino P, et al. An intriguing failing of convolutional neural networks and the coordconv solution[C]//Advances in Neural Information Processing Systems. 2018: 9605-9616.[9]Novotny D, Albanie S, Larlus D, et al. Semi-convolutional operators for instance segmentation[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 86-102.[10]Li B, Wu W, Wang Q, et al. Siamrpn++: Evolution of siamese visual tracking with very deep networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 4282-4291.[11] Azulay, Aharon, and Yair Weiss. “Why do deep convolutional networks generalize so poorly to small image transformations?.” Journal of Machine Learning Research 20.184 (2019): 1-25.]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度</tag>
        <tag>平移</tag>
        <tag>尺度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的单目深度估计综述]]></title>
    <url>%2F2020%2F03%2F08%2F%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8D%95%E7%9B%AE%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[前言前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。1任务介绍深度估计是计算机视觉领域的一个基础性问题，其可以应用在机器人导航、增强现实、三维重建、自动驾驶等领域。而目前大部分深度估计都是基于二维RGB图像到RBG-D图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的Shape from X方法，还有结合SFM(Structure from motion)和SLAM(Simultaneous Localization And Mapping)等方式预测相机位姿的算法。其中虽然有很多设备可以直接获取深度，但是设备造价昂贵。也可以利用双目进行深度估计，但是由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度也较高，尤其是对于低纹理场景的匹配效果不好。而单目深度估计则相对成本更低，更容易普及。那么对于单目深度估计，顾名思义，就是利用一张或者唯一视角下的RGB图像，估计图像中每个像素相对拍摄源的距离。对于人眼来说，由于存在大量的先验知识，所以可以从一只眼睛所获取的图像信息中提取出大量深度信息。那么单目深度估计不仅需要从二维图像中学会客观的深度信息，而且需要提取一些经验信息，后者则对于数据集中相机和场景会比较敏感。通过阅读文献，可以将基于深度学习的单目深度估计算法大致分为以下几类：监督算法顾名思义，直接以2维图像作为输入，以深度图为输出进行训练：上面给的例子是KITTI数据集中的一组例子，不过深度图可能看的不是很明显，我重新将深度图涂色之后：无监督算法由于深度数据的获取难度较高，所以目前有大量算法都是基于无监督模型的。即仅仅使用两个摄像机采集的双目图像数据进行联合训练。其中双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或是将双目图像中各个像素点的对应问题看作是立体匹配问题进行训练。左视图-右视图示例：视差，以我们人眼为例，两只眼睛看到的图像分别位于不同的坐标系。将手指从较远地方慢慢移动到眼前，会发现，手指在左眼的坐标系中越来越靠右，而在右眼坐标系中越来越靠左，这种差异性就是视差。与此同时，可以说明，视差与深度成反比。除此之外，由于摄像机参数也比较容易获取，所以也可以以相机位姿作为标签进行训练。Structure from motion/基于视频的深度估计这一部分中既包含了单帧视频的单目深度估计，也包含了多帧间视频帧的像素的立体匹配，从而近似获取多视角图像，对相机位姿进行估计。2 数据集介绍2.1 KITTIKITTI是一个多任务属性的数据集，其中原始数据采集平台装配有2个灰度摄像机，2个彩色摄像机，一个Velodyne 64线3D激光雷达，4个光学镜头，以及1个GPS导航系统。其中包含有200+G的原始数据，而有关户外场景的有175G数据。对于这些数据，所标注的任务包含：立体图像匹配、光流、场景流、深度估计（单目或者基于3D点云/激光数据的深度估计）、视觉测距、目标检测（2D/3D物体检测、俯视图物体检测）、目标跟踪、道路/车道线检测、目标分割等。链接：http://www.cvlibs.net/datasets/kitti/eval_object.php2.2vKITTI从名字可以看出这个数据集跟KITTI有关联，其对应KITTI的原始数据和各类任务，创建了全新的虚拟图像，当然，并不是所有原始数据都能对应得上。这里的“虚拟”指的是：左右摄像头15°和30°偏转画面、清晨、晴天、多云、雾天、下雨等基于原图的渲染图像。共计14G原始RGB图像，对应的目标检测、目标跟踪、目标分割标注都存在。这一数据集的意义在于可以缓解深度信息对于光线的敏感问题。链接：http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds2.3CityscapesCityscapes的数据取自德国的50多个城市的户外场景，其中数据包含有左右视角图像、视差深度图、相机校准、车辆测距、行人标定、目标分割等，同时也包含有类似于vKITTI的虚拟渲染场景图像。其中简单的左视角图像、相机标定、目标分割等数据需要利用学生账号注册获取，其他数据需要联系管理员获取。链接：https://www.cityscapes-dataset.com/downloads/2.4NYU Depth V2NYU Depth V2数据集中包含有428G室内场景数据，同时包含有目标分割标注、深度标注。链接：https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html2.5 ScanNetScanNet中包含有约1500个视频序列的RGB-D数据，主要用于三维重建。链接：http://www.scan-net.org/#code-and-data2.6Make3DMake3d数据集包含约1000张室外场景图片，50张室内场景，7000张合成物体。其中包含有激光-2D图像，立体图像、深度数据等。3 数据处理3.1数据组成以KITTI数据集为例，它没有给出深度相关的标注信息。其数据组成包括多个场景下的原始图像数据（gray、color），实例分割、目标跟踪、2d/3d目标检测等任务信息。为了方便我后续使用，我将数据结构解析如下，由于知乎不支持表格，所以我就直接截图了：这一部分内容中，对于一个点云数据P:[X,Y,Z,1]^T，其中Z就是深度信息，将其转化为相机左视图的像素点坐标Q:[u,v,1]^T：ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times P对于GPS/imu中的点G:[X,Y,Z]^T，将其转化为相机左视图的像素点坐标Q：ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times {\left( {\left. R \right|T} \right)_{imu\_to\_velo}} \times G其中最需要注意的是第一个公式，用于深度的信息的提取，以及P_rect_xx，其前三列数据为修正后的相机内参。3.2 数据处理有关相机/像素/世界坐标系的知识我就不介绍了，对于相对位姿，如果将每个视频场景的第一帧的位姿视为初始位姿，那么每一帧的相对位姿计算如下：$$ \begin{array}{l} imu2cam = \left[ {\begin{array}{*{20}{c}} {R\_rect\_00}&0\\ 0&1 \end{array}} \right] \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{imu2velo}}\\ scale = \cos \left( {latitude} \right)\\ t = \left[ {\begin{array}{*{20}{c}} {{t_x}}\\ {{t_y}}\\ {{t_z}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {scale \times longtitude \times \pi /180}\\ {scale \times er \times \log \left[ {\tan \left( {\frac{{90 + latitude}}{{360}}\pi } \right)} \right]}\\ {altitude} \end{array}} \right]\\ roll,yaw,pitch \Rightarrow R = {R_z}{R_y}{R_x}\\ \Rightarrow T = \left[ {\begin{array}{*{20}{c}} R&t\\ 0&1 \end{array}} \right]\\ \Rightarrow {T_{t \to s}} = imu2ca{m_t} \times T_s^{ - 1} \times {T_t} \times imu2cam_t^{ - 1} \end{array} $$由于深度信息的转换需要用到相机内参，所以对于图像的缩放需要先处理，假如图像大小的放缩尺度为[zoom_x,zoom__y]，那么相机内参的变化如下：$$ \begin{array}{l} \left\{ \begin{array}{l} P\_rect\_00[0,:] * = zoom\_x\\ P\_rect\_00[1,:] * = zoom\_ \end{array} \right.y\\ \Rightarrow K = P\_rect\_00[:,:3] \end{array} $$根据世界坐标系的转换：$$ P\_velo2im = P\_rect\_00 \times R\_rect\_00 \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} $$由于要求点云数据的反射强度为1，所以需要先将点云数据的反射强度置为1：$$ \begin{array}{l} Z\left[ {\begin{array}{*{20}{c}} x\\ y\\ 1 \end{array}} \right] = {\left( {P\_velo2im \times \left[ {\begin{array}{*{20}{c}} {{X_v}}\\ {{Y_v}}\\ {{Z_v}}\\ 1 \end{array}} \right]} \right)^T} = \left[ {\begin{array}{*{20}{c}} {X'}\\ {Y'}\\ Z \end{array}} \right]\\ \Rightarrow x = X'/Z,y = Y'/ZZ = Z \end{array} $$最后我们只需要保留满足图像边界约束的点的深度信息，如果映射得到的点坐标相同，则只保留深度更小的。那么对于网络训练过程中的数据增强，则可以进行多种变换，下面列出几种基础的：随机水平翻转，所以需要改变相机内参的水平平移量cx=w-cx；随机尺度变换并剪切至固定大小：$$ \left[ {\begin{array}{*{20}{c}} {{f_x}}&{{c_x}}\\ {{f_y}}&{{c_y}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{f_x} \times scal{e_x}}&{{c_x} \times scal{e_x} + offse{t_x}}\\ {{f_y} \times scal{e_y}}&{{c_y} \times scal{e_y} + offse{t_y}} \end{array}} \right] $$3.3评价指标KITTI数据集在考虑深度估计信息误差时，所以判定的时候只取0.40810811H ~ 0.99189189H，0.03594771W ~ 0.9640522229W部分图像区域，当然也经常会只取50m或者80m范围内的深度信息。为了让预测深度和真实深度的数量级范围一致，一般会用二者深度的中位数作为尺度，对预测深度信息进行尺度放缩。4 相关工作4.1基于单目视觉的深度估计传统编解码结构深度估计任务是从二维图像到二维深度图像的预测，因此整个过程是一个自编码过程，包含编码和解码，通俗点就是下采样和上采样。这类结构主要有FCN框架和U-net框架，二者的下采样过程都是利用了卷积和池化，而上采样利用了逆卷积/转置卷积(Deconvolution)和upsample。深度回归网络早期的单目深度估计网络框架基本上都是直接利用了上面所提到的两个基础框架进行了预测，为了让这类框架更好的应用于深度估计问题，一般都从以下几个方面着手：更好的backbone模型、多尺度、更深的网络。以3DV 2016中《Deeper Depth Prediction with Fully Convolutional Residual Networks》一文为例，其提出了FCRN网络框架：其网络框架主体是基于Resnet-50，在上采样过程中采用了独特的方式，将逆卷积用up-pooing+conv的方式替代了，将Resnet中的project模块进行了一定的改进。其中上采样过程中将特征图利用0元素进行填充，然后利用一类特殊的卷积核进行特征提取，这一过程可以先卷积，然后错位相连得到，原理如下：其中可以发现卷积核并非是正方形，而是矩形，不过过程其实是一样的。而projection部分，即resnet中原图先经过1×1卷积再与特征图相连的部分，变为：具体细节我就不在多讲了，其效果如下：其代码链接为：https://github.com/iro-cp/FCRN-DepthPrediction，基于Tensorflow和matconvnet。深度分类网络将深度估计问题变为回归问题的缺点在于，太依赖于数据集的场景，并且由于图像中深度往往是分层的，类似于等高线之类的，所以也有学者将深度估计变为一个分类问题，而类别数就是将最远实际距离划分为多份而制作的。以此为代表的是CVPR2018中《Deep Ordinal Regression Network for Monocular Depth Estimation》所提出的DORN框架：该框架2018年在多个数据集上取得了第一的名次，不过现在有个别算法超越了。可以看到，原图在经过密集特征提取之后，增加了一个场景理解模块，这一模块包含5个部分。其中full-image encoder部分与传统的自编码器不同：可以看到其先是利用池化进行下采样，将其拉伸为向量之后，利用全连接层进行编解码，然后还原为原图大小。而ASPP模块是利用了膨胀卷积（dilated convolution）进行特征提取，膨胀倍数分别为6,12,18。五个部分concat得到最终的特征图。再进入有序回归模块，实质上是多分类器。其将深度范围划分为多个区间：最后输出W×H×2K的结果，K代表深度区间，2K是因为每两个相邻的通道值2n表示深度小于n的概率，2n+1表示深度大于n的概率，二者利用softmax归一化。其效果如下：可以看到，DORN对于深度的细节把握得非常好，其速度约为2fps，代码基于caffe平台，链接为：https://github.com/hufu6371/DORN无论是回归还是分类，都是以深度信息作为标签的监督算法，因此其受限于训练集场景，仅限于刷榜。4.2结合双目视觉的单目深度估计既然监督算法受限于场景，那么近两年则出现了很多无监督算法，其中就包含有利用双目数据进行训练的算法，下面我用几个例子进行说明。首先以CVPR2017中《Unsupervised Monocular Depth Estimation with Left-Right Consistency》一文所提出的Monodepth算法为例。这篇论文算是这类算法的一个开端吧，效果并没有非常优异，但是引出这样一条思路。其网络框架如下：该算法将深度估计问题变成了左右视图立体匹配问题，都是先从原始图预测另外一个视图的视差，然后结合输出另外一个视图。整体框架依赖DispNet，而DispNet又是在FlowNet基础上进行的改变，主要改变是在多尺度衔接出增加卷积层，以保证图像尽可能平滑。经过自编码器之后，分别利用逆卷积、预测的右视图相对左视图的视差+upsample/双线性插值、预测的左视图相对右视图的视差+upsample/双线性插值、原图。有了这些之后，损失函数部分则同时包含有：外观匹配损失，即预测的视图和实际视图的区别：$$ C_{ap}^l = \frac{1}{N}\sum\limits_{i,j} {\alpha \frac{{1 - SSIM\left( {I_{ij}^l,\tilde I_{ij}^l} \right)}}{2}} + \left( {1 - \alpha } \right)\left\| {I_{ij}^l - \tilde I_{ij}^l} \right\| $$其中SSIM指的是结构相似性。视差平滑性约束：$$ C_{ds}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {{\partial _x}d_{ij}^l} \right|} {e^{ - \left\| {{\partial _x}I_{ij}^l} \right\|}} + \left| {{\partial _y}d_{ij}^l} \right|{e^{ - \left\| {{\partial _y}I_{ij}^l} \right\|}} $$左右视差一致性损失：$$ C_{lr}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {d_{ij}^l - d_{ij + d_{ij}^l}^r} \right|} $$由于其主要在KITTI_outdoor和Cityscapes上训练的，所以对于室外场景效果会略好，又因为其算法框架比较简单，所以深度信息中的细节比较模糊，尤其是存在遮挡或者物体相连的情况时。测试效果如下：通过其原理和论文中的测试效果来看，其对于室外场景下的深度估计效果还行，不过对于边缘部分的把握不是很好。再加上大多是街景数据，所以对于室内场景的视角具有很大的不适应性。另外，由于立体匹配对于大面积的纯色或者颜色相近的图像块效果很差，所以Monodepth不适用于纹理不清晰的场景，容易将大片颜色类似的图像块视为一个整体。代码是基于tensorflow进行开发的：https://github.com/mrharicot/monodepth，也有pytorch0.4.1的复现版本：https://github.com/ClubAI/MonoDepth-PyTorch。与此同时，在CVPR2018中，由商汤团队在《Single View Stereo Matching》一文中提出了类似的SVS算法，其相对于Monodepth，在细节和场景适应性方面有了很大的提升。SVS网络框架如下：从图中不难看出，得到预测的右视图之后，两个视角的图像进行类似于DispNet的立体匹配，从而获得左视图的视差。而关键在于怎么从左视图预测右视图。可以发现，SVS在利用卷积层对左视图进行特征提取之后，分别将每一通道的特征图和原图进行元素乘法，然后再相加。这一部分实际上是借鉴了Deep3D的框架。Deep3D模型是用来将2D图像转化为3D图像对的，其框架为：假设每个通道分别代表着在左视图中每个像素点相对右视图中的视差偏移量的概率分布。综上，SVS实质上就是Deep3D+Dispnet的合体版，其效果图如下：同时可以看看基于KITTI数据集训练的SVS模型在其他数据集上的测试效果：代码是基于caffe开发的：https://github.com/lawy623/SVS4.3基于视频的相机位姿估计和视觉测距基于视频的单目深度估计大多都是面向相机位姿估计和视觉测距的，其核心就是利用相邻视频帧所产生的运动，近似多视角图像，并对相机位姿进行估计，从而可以估计出相机的移动路线，进一步完成SLAM工作。那么在CVPR2017的一篇《Unsupervised Learning of Depth and Ego-Motion from Video》中则是提出了SFM算法，这篇文章中对于深度估计的求解较为简单，所以效果不是很好，但是提出了基于视频帧序列的相机位姿估计算法。其论文中使用了相邻3帧的信息，不过代码却是用的相邻5帧的信息，整体框架比较简单：可以看到，单目深度估计部分仅仅是针对一帧的，直接采用了Dispnet的网络框架，不过我发现实际上是U-net，而相机位姿估计则是将相邻帧的相对相机位姿变化看作一个含有6个元素的向量（可以理解为x,y,z方向的平移量和旋转量）进行预测。有意思的是，SFM并没有使用深度信息作为标签，而是将深度信息作为一个过程变量，将前后帧图像联系起来，从而做到无监督学习，不过相机位姿的训练还是有监督的：利用预测的相机位姿和深度信息，可估计出目标视图相对原视图的像素点位置，由于预测的像素点位置可能不是整数，为了保证其为整数，将采用双线性插值，其中K是相机参数矩阵：$$ \left\{ \begin{array}{l} {p_s} \sim K{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over T} }_{t \to s}}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over D} }_t}\left( {{p_t}} \right){K^{ - 1}}{p_t}\\ {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( {{p_t}} \right) = {I_s}\left( {{p_s}} \right) = \sum\nolimits_{i \in \left\{ {t,b} \right\},j \in \left\{ {l,r} \right\}} {{w^{ij}}{I_s}\left( {p_s^{ij}} \right)} \\ \sum\nolimits_{i,j} {{w^{ij}}} = 1 \end{array} \right. $$可以看到这里的插值方式是对估计像素点位置处的相邻4个位置的像素进行加权平均，然后作为目标像素点位置处的像素值，新合成的视图和目标视图进行一致性约束。不过上述这种做法受限于静态场景，且无遮挡情况，为了缓解这种问题，作者又加入了一个可解释性网络：该网络的编码部分输出的是相机位姿，解码部分输出的是多个尺度的解释性眼膜，其意义是合成视图中的每个像素点能够被成功建模的概率。而这一部分是没有标签的，所以作者通过设计损失函数将其进行了约束：$$ \begin{array}{l} {L_{vs}} = \sum\limits_{\left\langle {{I_1},...,{I_N}} \right\rangle \in S} {\sum\limits_p {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} }_s}\left( p \right)\left| {{I_t}\left( p \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( p \right)} \right|} } \\ L = \sum\limits_l {L_{vs}^l} + {\lambda _s}L_{smooth}^l + {\lambda _e}\sum\limits_s {{L_{reg}}\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} _s^l} \right)} \end{array} $$l指的是尺度，s指的是图片，其中的平滑性约束跟上一节所讲的Monodepth一样，由于解释性掩膜无标签，如果不加约束的话会自动为0，所以利用交叉熵损失函数对其进行了约束，默认为全1矩阵。其效果如下：可以看到，深度估计的效果并不是很好，不过整体的设计思路很新颖，也可以看看其对于解释性掩膜的预测效果：可以发现，对于发生变化的部分，即前景部分，其不可解释性很高，其实这个也能用来估计光流。代码是基于tensorflow的：https://github.com/tinghuiz/SfMLearner，不过有pytorch的复现版本：https://github.com/ClementPinard/SfmLearner-Pytorch果不其然，在CVPR2018中商汤又提出了GeoNet，该网络在SFM的基础上增加了光流监督信息：可以看到，前半部分的深度估计和相机位姿估计都跟SFM一样，只是在后面增加了光流的输出，先利用前半部分得到刚性结构的光流，后半部分增加一个非刚性光流预测环节，二者之和就是最终的光流。效果：可以看到，GeoNet的深度估计效果并没有特别突出，代码是基于Tensorflow的：https://github.com/yzcjtr/GeoNet同样的还有CVPR2018的《Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction》一文中提到的Depth-VO-Feat:直接从这个网络架构可以看到包含了两个部分的图像重构，一个是左视图和右视图的重构，一个是前后两帧间的重构，重构的意义在于找到对应像素点的联系，并非直接利用左右视图进行误差计算，可以看到图中对于右视图的边缘填充。由于该框架假设场景是Lambertian的，即无论从哪个角度观察，其光照强度是一致的，那么这对于图像的重构就很敏感，因此，作者又添加了特征的重构，框架一致。对于训练细节，除了图像和特征的L1重构误差之外，也加入了边缘平滑性约束，骨干网络是Resnet50的变种。对于深度估计，其预测的是深度信息的倒数。效果如下：可以看到，深度估计的效果还是中规中矩，不过其可以用来做视频中相机的移动轨迹预测，这一点在多目标跟踪（MOT）中对于手持相机的场景有所帮助。代码是基于caffe的：https://github.com/Huangying-Zhan/Depth-VO-Feat相应的，近几年关于无监督单目深度估计的研究越来越多，我抽空了看了下，比如有Google出品的vid2depth和struct2depth算法，二者的代码链接如下：vid2depth:https://github.com/tensorflow/models/tree/master/research/vid2depthstruct2depth: https://github.com/tensorflow/models/tree/master/research/struct2depth。其他的也挺多的，后面章节我会再补充一点，不过肯定不全。4.4基于图像风格迁移的单目深度估计实质上，深度图像也是一种图像风格，如果我们要将生成学习引入深度估计的话，就需要注意两个地方，一个是原始图像到深度图像的风格转变，这一点可以获取类似于分割的map，另一点就是对像素点的深度进行回归。这里的方式与第一节讲的深度回归模型不一样，因为第一步的风格转变，已经对于场景和相机位姿有了很好的适应性。ECCV2018中《T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks》所提出的T2Net尝试性地将图像风格迁移引入单目深度估计领域，虽然效果只是2016年的水平，不过也算是一次很好的尝试了。下面介绍下T2Net的思路，首先给出其网络框架：框架很明显，对于室外场景，其训练集用到了KITTI和VKITTI中的sunset场景，对于室内场景，则使用了NYU Depth v2和SUNCG,没仔细看怎么下载，相关工具在https://github.com/shurans/SUNCGtoolbox。从图中可以看到，作者做了两个模块，一个是图像风格迁移模块，一个是单目深度估计模块。其中图像风格迁移模块中包含有合成图像到真实图像的迁移，真实图像到真实图像的迁移，二者共用一个GAN。其中的Loss包含有：由合成图像风格迁移生成的图像与原始图像的GAN Loss，即利用判别器进行判定的误差；由真实图像风格迁移生成的图像预原始图像的重构误差，这一部分计算L1 Loss；由合成图像风格迁移生成的图像与原始图像的编码特征的GAN Loss。然后仅对合成图像分支进行深度估计，同样地，也加入了深度图的平滑性约束。从不匹配的图像对可以看出，其基础框架为CycleGAN。可以看到风格迁移的效果和深度估计的效果如下：从结果中我们发现有一个版本的实现效果超过了完整框架，通过查阅发现，是只利用真实数据进行深度估计的效果，也就是说效果比加入图像迁移的效果更好，打自己脸。。。实际上他是在跟只用合成图像进行深度估计训练的效果作比较，确实好了些。代码链接：https://github.com/lyndonzheng/Synthetic2Realistic除此之外，在CVPR2018也有一篇类似的算法《Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer》，其效果则是达到了state-of-art,我们暂且称其为MDEDA,网络框架如下：熟悉CycleGAN框架的话，应该很容易看懂其中的关系，其中存在三种图像序列，一种是原始图像，一种是合成的图像，一种是深度图像，不同的是三种图像内容是一致的，而非CycleGAN那样不匹配的。其中原始图像和合成图像之间进行图像风格的循环迁移和重构，合成图像与深度图像进行单向的风格迁移。效果如下：左侧的是直接对原图进行深度估计的效果，中间是其他图像迁移算法的效果，右侧是采用本文算法后的合成以及深度估计效果，速度大概为44fps。合成图像对于深度估计的效果提升也反映了一个问题，即图像光暗条件对于深度估计有很大影响，所以对于一些出现了阴影，如影子等的场景，深度估计会出现偏差，如：代码只提供了测试模型：https://github.com/atapour/monocularDepth-Inference4.5多任务深度估计在ICRA2019中《Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations》中基于图像分割算法RefineNet设计了一个多任务框架。其中RefineNets是CVPR2017中提出的算法，其全局框架是基于Resnet的U-net网络框架，可以输出多尺度的分割图：可以看到的是，RefineNet在每一个尺度的上采样部分都增加了一个局部提升的网络，用于多尺度输出的融合：所以其主要创新在于采用skip-connection和 Resnet Block的方式不断融合各种分辨率的特征，用于增加更多的细粒度特征，从而方便生成更高分辨率的预测：那么在BMVC2018中则是提出了一种Light-weighted RefineNet算法，顾名思义，就是RefineNet的轻量级网络，其对于512×512大小的图像，速度从RefineNet的20FPS提升到了55FPS（1080Ti），效果略微下降。代码基于Pytorch: https://github.com/DrSleep/light-weight-refinenet那么回到正题，我们提到的这个同时进行深度估计和目标分割的网络框架，对于1200×350大小的输入，其速度为60FPS。网络框架如下：以上的结构通过之前我介绍的深度估计框架以及Light-Weighted RefineNet框架很容易能看懂，之所以比原本的Light-Weighted RefineNet还要快，是因为将其中的部分1×1卷积替换成了MobileNetV2中采用的depthwise卷积方式。对于分割和深度估计任务的结合，从网络框架和损失函数的设计来看可以发现，其除了特征是共享的之外，预测任务是独立的。效果如下：代码仅提供了测试用例：https://github.com/drsleep/multi-task-refinenetECCV2018中《DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency》一文提出了单目深度估计和光流预测的联合任务框架。不同于单独训练两个任务的方式，作者将二者的一致性进行了考虑，从而做到二者的相互促进，可以看到对比效果：其主要思路是利用无监督的方式从视频中预测深度信息和相机位姿变化，这一部分对于刚性流场景比较适用，即静态背景。通过几何一致性的约束监督，可以将3D的场景流映射到2D光流上，由此与光流预测模型的结果进行一致性约束。具体框架如下：乍一看可以发现网络框架的前半部分很眼熟，图中展示的是分别对前后帧做单目深度估计，然后利用前后帧做相机位姿变化预测和光流预测，结合SFM网络中像素点转移的计算公式，可以利用深度信息和相机位姿变化关系求得在t+1时刻对应像素点位置，由此可以计算刚性流场景下的光流。​ 对于刚性流场景下的合成光流信息和直接预测到的光流信息，二者都反映了相邻两帧的像素点的对应关系，因此作者对此引入了光照约束（利用对比映射和插值，计算每个像素点的像素值差异）和深度的平滑性约束。​ 再来看Forward-Backward模块，由于我们在上面提到了光照一致性约束，但实际上对于重叠区域并不适用，因此加入了前后向一致性的约束。即图中的Valid Mask部分，利用刚性流信息可以检测出一些无效的像素区域，如运动物体、画面边缘等，因为这些都不符合刚性这一条件，那么再在有效区域使用光照一致性假设：感觉这个跟SFM中的Explain Mask一样，然后前后的一致性约束，则是分光流和深度估计两部分，其中深度的一致性跟光照一致性的计算方式一样，而光流的一致性则是真的计算了前向和反向的光流一致性。最后对于深度和光流的共同有效区域，保证二者预测的光流尽可能一致。为了保证更好的训练效果，作者先在SYNTHIA数据集上预训练光流预测，采用的是UnFlownet-C网络，在KITTI和Cityscapes上预训练深度估计和相机位姿预测，采用的是SFM框架，然后进行联合训练。代码基于Tensorflow: https://github.com/vt-vl-lab/DF-Net我前段时间还发现一个多任务的集成框架CVPR2019的CCN算法《Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation》，效果目前好像还是SOTA，其融合了单目深度估计、相机位姿估计、光流估计和运动分割多个任务，代码：https://github.com/anuragranj/cc本小节的内容都是基于无监督的单目深度估计算法。5总结对于单目深度估计模型，目前主要分为基于回归/分类的监督模型，基于双目训练/视频序列的无监督模型，以及基于生成学习的图像风格迁移模型。大概从2017年起，即CVPR2018开始，单目深度估计的效果就已经达到了双目深度估计的效果，主要是监督模型。但是由于现有的数据集主要为KITTI、Cityscapes、NYU DepthV2等，其场景和相机都是固定的，从而导致监督学习下的模型无法适用于其他场景，尤其是多目标跟踪这类细节丰富的场景，可以从论文中看到，基本上每个数据集都会有一个单独的预训练模型。对于GAN，其对于图像风格的迁移本身是一个很好的泛化点，既可以用于将场景变为晴天、雾天等情况，也可以用于图像分割场景。但是深度估计问题中，像素点存在相对大小，因此必定涉及到回归，因此其必定是监督学习模型，所以泛化性能也不好，以CVPR2018的那篇GAN模型为例可以对比：左边是KITTI的测试效果，右边是MOT的测试效果，从上到下依次是原图、合成图，以及深度图。可以看到，其泛化性能特别差。而对于无监督模型，从理论上来讲，其泛化性能更好。那么对于无监督模型，我们分两部分进行讨论，第一部分是利用双目视差进行训练的无监督模型，这里的无监督模型中包含有左右视图预测的监督信息，所以存在一定程度的局限性。以Monodepth为例：对于无监督的算法，可能场景适应性会更好，但依旧不适用于对行人深度的估计。6参考文献[1] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.[2] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.[3] Laina I, Rupprecht C, Belagiannis V, et al. Deeper depth prediction with fully convolutional residual networks[C]//2016 Fourth international conference on 3D vision (3DV). IEEE, 2016: 239-248.[4] Fu H, Gong M, Wang C, et al. Deep ordinal regression network for monocular depth estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2002-2011.[5] Godard C, Mac Aodha O, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 270-279.[6] Dosovitskiy A, Fischer P, Ilg E, et al. Flownet: Learning optical flow with convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2758-2766.[7] Ilg E, Mayer N, Saikia T, et al. Flownet 2.0: Evolution of optical flow estimation with deep networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2462-2470.[8] Mayer N, Ilg E, Hausser P, et al. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4040-4048.[9] Xie J, Girshick R, Farhadi A. Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 842-857.[10] Luo Y, Ren J, Lin M, et al. Single View Stereo Matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.[11] Zhou T, Brown M, Snavely N, et al. Unsupervised learning of depth and ego-motion from video[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 1851-1858.[12] Yin Z, Shi J. Geonet: Unsupervised learning of dense depth, optical flow and camera pose[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1983-1992.[13] Zhan H, Garg R, Saroj Weerasekera C, et al. Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 340-349.[14] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680.[15] Radford A , Metz L , Chintala S . Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[J]. Computer Science, 2015.[16] Arjovsky M, Chintala S, Bottou L. Wasserstein gan[J]. arXiv preprint arXiv:1701.07875, 2017.[17] Gulrajani I, Ahmed F, Arjovsky M, et al. Improved training of wasserstein gans[C]//Advances in Neural Information Processing Systems. 2017: 5767-5777.[18] Mao X, Li Q, Xie H, et al. Least squares generative adversarial networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2794-2802.[19] Mirza M, Osindero S. Conditional generative adversarial nets[J]. arXiv preprint arXiv:1411.1784, 2014.[20] Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.[21] Wang T C, Liu M Y, Zhu J Y, et al. High-resolution image synthesis and semantic manipulation with conditional gans[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8798-8807.[22] Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232.[23] Wang T C , Liu M Y , Zhu J Y , et al. Video-to-Video Synthesis[J]. arXiv preprint arXiv:1808.06601,2018.[24] Zheng C, Cham T J, Cai J. T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 767-783.[25] Atapour-Abarghouei A, Breckon T P. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2800-2810.[26] Nekrasov V , Dharmasiri T , Spek A , et al. Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations[J]. arXiv preprint arXiv:1809.04766,2018.[27] Nekrasov V , Shen C , Reid I . Light-Weight RefineNet for Real-Time Semantic Segmentation[J]. arXiv preprint arXiv:1810.03272, 2018.[28] Lin G , Milan A , Shen C , et al. RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.,2017:1925-1934[29] Zou Y , Luo Z , Huang J B . DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018:36-53.[30] Ranjan A, Jampani V, Balles L, et al. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 12240-12249.]]></content>
      <categories>
        <category>机器学习</category>
        <category>3D视觉</category>
      </categories>
      <tags>
        <tag>深度估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多目标跟踪中的数据关联代码实践(下)]]></title>
    <url>%2F2020%2F03%2F06%2F%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5(%E4%B8%8B)%2F</url>
    <content type="text"><![CDATA[前言接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到github。4 最小代价流4.1 算法形式在了解最小代价流之前，我们需要先铺垫一下几个常见图模型，以帮助我们理解，比如最短路、最大流、最小费用最大流，最小割（闭嘴，我暂时没看懂）。下图是一个很常见的图网络：我们可以看到，图上有很多节点和边，这两个元素是组成图模型的核心。其次，每条边上都会有对应的数值，比如最短路问题中的相邻两节点的距离，最大流中的边容量，最小费用最大流问题中的边容量和费用。那么我们来看看几个问题的具体定义：最短路问题最短路问题一般特指单源单汇最短路问题，即给定起点和终点，从各种路径中选择最短的路径。$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{W_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{j = 1}^N {{x_{ij}} - \sum\limits_{k = 1}^N {{x_{ki}}} } {\rm{ = }}\left\{ \begin{array}{l} 1,i = 1\\ - 1,i = N\\ 0,i \ne 1,N \end{array} \right.\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$上面公式中如果结合图模型来思考会简单很多，即中间节点无论会不会通过，其流入边和流出边一定有且只有0或1个，不可能经过这个点而不经过与之相邻的边。不过对于起点和终点则允许有一条流出边或一条流入边。最大流问题最大流问题就是选择从起点到终点的最大流量分配，与最短路的最大区别在于最短路问题中每个节点只能选择一条流出边和一条流入边，而最大流问题则只要满足边容量限制，则可任意选择流入流出边数量。$$ \begin{array}{l} max = v\left( f \right) = \sum\limits_j {{f_{sj}}} - \sum\limits_i {{f_{is}}} \\ s.t.\left\{ \begin{array}{l} \sum\limits_j {{f_{ij}}} = \sum\limits_k {{f_{ki}}} \\ \sum\limits_j {{f_{sj}}} = \sum\limits_k {{f_{kt}}} = v\left( f \right)\\ 0 \le {f_{ij}} \le {w_{ij}} \end{array} \right. \end{array} $$上面公式的意思是，即中间节点无论会不会通过，其流入边流量之和=流出边流量之和，从起点流出的总流量=流入终点的总流量，每条边的流量有上限。最小费用流问题最小费用流的约束条件和最大流的一样，只不过为了更好描述目标函数我改写成了类似于最短路问题的形式。其目标是选择费用最短的流，当然，它跟最大流问题不同，这里需要设置起始点的流出流量，而且，如果在最大流限制下求解最小费用流，那么就是最小费用最大流问题了。$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{j = 1}^N {{x_{ij}} - \sum\limits_{k = 1}^N {{x_{ki}}} } {\rm{ = }}\left\{ \begin{array}{l} 负数,i = 1\\ 正数,i = N\\ 0,i \ne 1,N \end{array} \right.\\ 0 \le {x_{ij}} \le {w_{ij}} \end{array} \right. \end{array} $$联系到多目标跟踪任务，其数据关联任务从短期来看就是一个二分图匹配问题，从长期来看就是一个图网络模型。（1）如果我们要用最短路模型来描述数据关联问题，其节点就是跟踪对象id，边代表跟踪轨迹和检测之间的代价。那单源单汇最短路模型就远远不足以描述，因为跟踪轨迹和检测数量是大于1的，所以从形式上来讲是多源多汇最短路，但是最短路没有限制中间节点的可重复性，所以这个问题应该用路由问题中的K –最短不相交路线（K shortest disjoint paths）来描述；（2）如果我们要用最大流模型来描述该问题，那么不同于最短路，边容量代表跟踪轨迹和检测之间的连接可能性，所以只可能是0和1。最终要求的就是最大流量，由于边容量的限制，所以不可能重复，也就是最多可能轨迹。而且这么看来，匈牙利算法很像是最大流模型的特例；（3）如果我们要用最小费用流模型来描述该问题，那么就跟第一部分中的最短路问题一样了，只不过多源多汇问题变成了给定初始流量的情形，距离变成了费用。最大的区别在于需要合理设定初始流量（代表了最终有多少条轨迹），还要设定边容量，不然容易所有流都流向同一条边；（4）结合（2）（3）来看，最大流模型只需要设定跟踪轨迹和检测的连接可能性，但是缺乏了相对性。而最小费用流则只需要设定代价值，但是需要设定初始流量。这里的初始流量代表了轨迹数量，所以先用最大流模型求出最大流，即可作为初始的轨迹数量，然后再求最小费用流即可，也就是最小费用最大流。不过两个任务都有着相同的任务，那就是寻找目标轨迹，所以这样来说时间效率会较低。一般来说我们会直接使用最大流模型/最小割模型，或者直接使用最小费用流+搜索算法。总的来说，最大流模型的优点是参数量少，但是确定跟匈牙利算法一样，我们无法对于0.9和0.5相似度的边进行相对选择，因为都是1。最小费用流模型的优点是保留了相似度，但是初始流量这一超参数不好设定。K-最短不相交路模型跟最小费用流一样，都需要设定轨迹数量。所以我们会选择用搜索算法使用最小费用流，通过搜索阈值使用最大流模型.4.2基于最大化后验概率模型的网络流建图对于最小费用流而言，最难的地方在于设定初始流量和边容量，使得跟踪轨迹不交叉，而且跟踪轨迹尽可能多而合理。最重要的是，我们不知道在网络模型中哪个节点是轨迹的起点或者终点，这些都需要我们去建模。再加上我们的目标是使得代价最小，极可能最终出现每条轨迹只有一个节点的情形。下面我们要设定几个代价值，由于每个点都有属于轨迹起点和终点的可能性，所以网络会非常大，为了更好地借鉴已有的最小费用流模型，我们可以转化为单一起点和终点的网络图：我们可以看到，简单的最小代价流结构存在几个问题：一开始我们就要选定哪些节点有可能成为起点，哪些节点有可能成为终点，这无疑增加了参数量；类似于最大流模型，我们通过设定边容量为1可以保证每条边最多被选择一次。但是，我们无法确保最多只有一个节点可以连接到目标节点，这就不能保证跟踪轨迹的不重叠。针对以上问题，我们可以引入过渡节点的概念，同时也就引入了过渡边，每个节点连接一条过渡边，这样通过设定过渡边容量，可以限制每个节点的流出流量，相应地就可以限制最多只有一个节点可以连接到目标节点。而且，我们让每个节点都连接起点，每个节点的过渡节点连接终点，这样就保证上面两个问题都解决了。具体网络结构如下：我们可以看到每个节点u都连接了起点，每个节点v都连接了终点，每个节点u都连接了过渡节点v。正如我们之前说的，每条边容量都是1，可以有效防止轨迹重叠。另外我在图中注明了每条边费用的取值范围，我们定义每条包含起点和终点的边的费用都是比较大的正数，节点与过渡节点的边的费用是负数，这样可以避免过早的终止轨迹，过渡节点与节点之间的边的费用就是跟踪轨迹和检测的代价值，取正数，不然每条轨迹都会在最后一帧终止。所以这里的参数有：初始流量的大小（轨迹数量）、节点属于轨迹起点/终点的概率、节点到过渡节点的补偿（选择这个节点的补偿）、过渡节点到节点的概率（匹配代价）。​ 下面我们联系多目标跟踪模型的形式来为这些参数赋予特殊的含义，首先给出后验概率形式，T表示已有轨迹，Z表示观测值：$$ \begin{array}{l} {T^ * } = \mathop {argmax}\limits_T P\left( {\left. T \right|Z} \right)\\ \;\;\;\; = \mathop {argmax}\limits_T \frac{{P\left( {\left. Z \right|T} \right)P\left( T \right)}}{{P\left( Z \right)}}\\ \;\;\;\; = \mathop {argmax}\limits_T \frac{{\prod\limits_i {P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)} }}{{P\left( Z \right)}} \end{array} $$如果我们不考虑目标之间的联系，即假设目标相互独立，将联系归于代价值之中。那么上式就可以转化为：$$ {T^ * } = \mathop {argmax}\limits_T \frac{{\prod\limits_i {P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)} }}{{\prod\limits_i {P\left( {{z_i}} \right)} }} = \mathop {argmax}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} $$这里我们就需要了解三个概率：$$ \left\{ \begin{array}{l} P\left( {{z_i}} \right) = {\theta _i}\\ P\left( {\left. {{z_i}} \right|T} \right) = P\left( {\left. {{z_i}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_k}}}}}} \right\}} \right)\\ P\left( {{T_k}} \right) = P\left( {\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_k}}}}}} \right\}} \right)\\ \;\;\;\;\;\;\;\;\;\; = {P_s}\left( {{x_{{k_0}}}} \right)P\left( {\left. {{x_{{k_1}}}} \right|{x_{{k_0}}}} \right)P\left( {\left. {{x_{{k_2}}}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}}} \right\}} \right)...P\left( {\left. {{x_{{k_{{l_k}}}}}} \right|\left\{ {{x_{{k_0}}},{x_{{k_1}}},...,{x_{{k_{{l_{k - 1}}}}}}} \right\}} \right) \end{array} \right. $$另外，我们还需要补充节点的概念来完善数据关联模型，因为上面的几个概念中我们还没有加入轨迹的终点的概率，所以明确一下联合概率数据关联模型：上图中虚线部分代表非当前时刻的节点，这样我们就将虚警和杂波利用起点和终点消除了，由于我们可以跨帧连接，所以虚拟目标就可以近似忽略。接下来我们开始分析概率模型，我们可以利用对数似然概率来描述：$$ \begin{array}{l} {T^ * } = \mathop {argmax}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} \\ \;\;\;\; = \mathop {argmin - ln}\limits_T \prod\limits_i {\frac{{P\left( {\left. {{z_i}} \right|T} \right)P\left( T \right)}}{{P\left( {{z_i}} \right)}}} \\ \;\;\;\; = \mathop {argmin}\limits_T \sum\limits_i {\left( { - ln\left[ {P\left( {\left. {{z_i}} \right|T} \right)} \right] - ln\left[ {P\left( T \right)} \right] + ln\left[ {P\left( {{z_i}} \right)} \right]} \right)} \end{array} $$其中$P\left( {\left. {{z_i}} \right|T} \right)$ 就是当前跟踪轨迹和检测之间的相似度，$P\left( T \right)$就是当前跟踪轨迹存在的概率，$P\left( \right)$表示该观测存在的概率，因为观测有可能是杂波或者虚警，我们可以用于过渡边的代价描述。最后就只剩下${P_s}\left( {{x_{{k_0}}}} \right)$和$P\left( {\left. t \right|T} \right)$两个概率值，这个我们可以当做是一个参数进行试验。就这样，我们最小代价流模型中的每个节点和每条边都赋予了有意义的概念。4.3 在线和离线跟踪分析这里我们所说的在线和离线模型的意思是一帧一帧使用min cost flow或者多帧一起优化。对于在线的优化，我们就不需要考虑有多个节点连接同一个节点的特殊情况了，也就是说我们可以直接忽略过渡边，这样就跟KM算法一模一样了，所以我们可以认为匈牙利算法是最大流模型的特殊情况，KM算法是最小费用流的特殊情况。接下来我们分别对在线和离线的最小代价流模型进行对比实验，其中在线最小代价流模型我们还是采用Kalman+马氏距离的方式构建代价矩阵。而离线的方式下我们则直接使用IOU和HSV直方图作为构建代价矩阵的指标。而对于观测量的概率，即决定过渡边权的指标，我们采用检测的置信度(ln(a*confidence+b))作为指标，而对于起点和终点的判定，我们将其作为超参数，连同代价阈值和特征衰减变量作为超参数。其中特征衰减变量是对轨迹短暂消失的惩罚：$$ similarity = similarity \times miss\_rat{e^{time\_gap - 1}} $$另外，无论是在线跟踪还是离线跟踪，MinCostFlow这个任务本身都需要设定初始流量，也就是跟踪轨迹数量，这个值我们都知道是最少是1，最多是总id数。那么我们就需要用搜索算法来解决，为了保证求解效率，我们简单假设这个问题是一维凸优化问题，采用二分搜索或者斐波那契搜索来进行。其中二分搜索很简单，对于斐波那契搜索，我们知道斐波那契数列{0,1,1,2,3…}，即f(n)=f(n-1)+f(n-2)。对于这个通项公式，我们可以看到对于长度为f(n)的搜索空间，可以将其分为f(n-1)和f(n-2)两个部分，这样就实现了搜索空间的缩减。下面给出具体的算法：可以看到我上面用了哈希表来存储搜索过程中的结果，避免重复运算，保证O(1)的查询效率。另外，对于求解结果，一般返回的是匹配点对，我们如何将其变成轨迹呢？这就是一个经典的“朋友圈”问题，可以采用并查集来求解，只需要O(n)的时间复杂度和O(n)的空间复杂度。不过我们这个问题简单一点，不存在一个点对应多个点的情况，所以可以简单利用数组或哈希表建立多叉树求解。4.4 代码实验为了方便，我们直接用IOU和HSV颜色直方图作为特征进行试验。由于代码太长，我这里这放一部分，其中的Fibonacci搜索过程代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108def fibonacci(self, n): """Use Fibonacci Search to speed up Searching there can exist u~v flows(id), so we need to find the min cost flows Parameters: ----------- n: int Returns: ----------- fn: int the n th fibonacci number """ assert n &gt; -1, "n must be non-negative number" if n in self.fib: return self.fib[n] else: return self.fib.setdefault(n, self.fibonacci(n - 1) + self.fibonacci(n - 2))def fibonacci_search(self): """Run Fibonacci Searching to find the min cost flow Returns ------- trajectories: List[List] List of trajectories min_cost: float cost of assignments """ k = 0 r = max(0, self.max_flow - self.min_flow) s = self.min_flow cost = &#123;&#125; trajectories = [] # find the nearest pos of fibonacci while r &gt; self.fibonacci(k): k = k + 1 while k &gt; 1: u = min(self.max_flow, s + self.fibonacci(k - 1)) v = min(self.max_flow, s + self.fibonacci(k - 2)) if u not in cost: self.graph.SetNodeSupply(0, u) self.graph.SetNodeSupply(1, -u) if self.graph.Solve() == self.graph.OPTIMAL: cost[u] = self.graph.OptimalCost() else: cost[u] = np.inf if v not in cost: self.graph.SetNodeSupply(0, v) self.graph.SetNodeSupply(1, -v) if self.graph.Solve() == self.graph.OPTIMAL: cost[v] = self.graph.OptimalCost() else: cost[v] = np.inf if cost[v] == cost[u]: s = v k = k - 1 elif cost[v] &lt; cost[u]: k = k - 1 else: s = u k = k - 2 self.graph.SetNodeSupply(0, s) self.graph.SetNodeSupply(1, -s) if self.graph.Solve() == self.graph.OPTIMAL: min_cost = self.graph.OptimalCost() / multi_factor hashlist = &#123;0: []&#125; # create disjoint set for arc in range(self.graph.NumArcs()): if self.graph.Flow(arc) &gt; 0: if self.graph.Tail(arc) == 0: hashlist[0].append(self.graph.Head(arc)) else: hashlist[self.graph.Tail(arc)] = self.graph.Head(arc) for entry in hashlist[0]: tracklet = [( self.node[entry]['frame_idx'], self.node[entry]['box_idx'], self.node[entry]['box'] )] point = hashlist[entry] while point != 1: if self.node[point]['type'] == 'object': tracklet.append(( self.node[point]['frame_idx'], self.node[point]['box_idx'], self.node[point]['box'] )) if point in hashlist: point = hashlist[point] else: break trajectories.append(tracklet) else: min_cost = inf_cost return trajectories, min_cost跟踪部分代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144def process(self, boxes, scores, image = None, features = None, **kwargs): """Process one frame of detections. Parameters ---------- boxes : ndarray An Nx4 dimensional array of bounding boxes in format (top-left-x, top-left-y, width, height). scores : ndarray An array of N associated detector confidence scores. image : Optional[ndarray] Optionally, a BGR color image; features : Optional[ndarray] Optionally, an NxL dimensional array of N feature vectors corresponding to the given boxes. If None given, bgr_image must not be None and the tracker must be given a feature model for feature extraction on construction. **kwargs : other parameters that model needed Returns ------- trajectories: List[List[Tuple[int, int, ndarray]]] Returns [] if the tracker operates in offline mode. Otherwise, returns the set of object trajectories at the current time step. entire_trajectories: List[List[Tuple[int, int, ndarray]]] entire time steps trajectories """ # save the first node id in current frame first_node_id = deepcopy(self.node_idx) # initialize graph in every time step when online if self.mode == "online" and self.current_frame_idx &gt; 1: self.graph = pywrapgraph.SimpleMinCostFlow() self.trajectories = [] if self.powersave: self.node = &#123;key: self.node[key] for key in self.node \ if key not in range(2, self.last_frame_id)&#125; for i in range(self.last_frame_id, self.node_idx): self.graph.AddArcWithCapacityAndUnitCost(0, int(i), 1, \ int(multi_factor * self.entry_exit_cost)) # Compute features if necessary. parameters = &#123;'image': image, 'boxes': boxes, 'scores': scores, 'miss_rate': self.miss_rate, 'batch_size': self.batch_size&#125; parameters.update(kwargs) if features is None: assert self.feature_model is not None, "No feature model given" features = self.feature_model(**parameters) # Add nodes to graph for detections observed at this time step. observation_costs = (self.observation_model(**parameters) if len(scores) &gt; 0 else np.zeros((0,))) node_ids = [] for i, cost in enumerate(observation_costs): self.node.update(&#123;self.node_idx: &#123; "type": 'object', "box": boxes[i], "feature": features[i], "frame_idx": self.current_frame_idx, "box_idx": i, 'cost': cost &#125; &#125; ) # save object node id to this time step node_ids.append(self.node_idx) if self.mode == 'online': if self.current_frame_idx == 0: self.graph.AddArcWithCapacityAndUnitCost(0, int(self.node_idx), 1, \ int(multi_factor*self.entry_exit_cost)) else: self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx), 1, 1, \ int(multi_factor * self.entry_exit_cost)) self.node_idx += 1 else: self.node.update(&#123;self.node_idx + 1: &#123; "type": 'transition', &#125; &#125;) self.graph.AddArcWithCapacityAndUnitCost(0, int(self.node_idx), 1, \ int(multi_factor * self.entry_exit_cost)) self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx), int(self.node_idx + 1), \ 1, int(multi_factor * cost)) self.graph.AddArcWithCapacityAndUnitCost(int(self.node_idx + 1), 1, 1, \ int(multi_factor * self.entry_exit_cost)) self.node_idx += 2 # Link detections to candidate predecessors. predecessor_time_slices = ( self.nodes_in_timestep[-(1 + self.max_num_misses):]) for k, predecessor_node_ids in enumerate(predecessor_time_slices): if len(predecessor_node_ids) == 0 or len(node_ids) == 0: continue predecessors = [self.node[x] for x in predecessor_node_ids] predecessor_boxes = np.asarray( [node["box"] for node in predecessors]) if isinstance(features,np.ndarray): predecessor_features = np.asarray( [node["feature"] for node in predecessors]) else: predecessor_features = torch.cat( [node["feature"].unsqueeze(0) for node in predecessors]) time_gap = len(predecessor_time_slices) - k transition_costs = self.transition_model( miss_rate = self.miss_rate, time_gap = time_gap, predecessor_boxes = predecessor_boxes, predecessor_features = predecessor_features, boxes = boxes, features = features, **kwargs) for i, costs in enumerate(transition_costs): for j, cost in enumerate(costs): if cost &gt; self.cost_threshold: continue if self.mode == 'online': last_id = int(predecessor_node_ids[i]) else: last_id = int(predecessor_node_ids[i] + 1) self.graph.AddArcWithCapacityAndUnitCost(last_id, int(node_ids[j]), 1, int(multi_factor * cost)) self.nodes_in_timestep.append(node_ids) # Compute trajectories if in online mode if self.mode == 'online': if self.current_frame_idx &gt; 0: min_cost, n_flow = self.binary_search(high = min(len(predecessor_time_slices[0]), len(node_ids))) if n_flow &gt; 0: self.trajectories = self.get_trajectory() else: self.trajectories = self.node2trajectory(first_node_id, self.node_idx) self.entire_trajectories = self.merge_trajectories(self.trajectories, self.entire_trajectories) else: self.trajectories = self.node2trajectory(2, self.node_idx) self.entire_trajectories = deepcopy(self.trajectories) self.current_frame_idx += 1 self.last_frame_id = first_node_id return self.trajectories, self.entire_trajectories完整版的请前往我的github，下面是我以MOT17-10视频为例跑的结果：在线MOTA=0.676，离线的MOTA=0.594，离线的特征关联方法很简单，而在线的用的是Kalman+马氏距离。以上都是我自己根据自己理解写的，可能理解有误，也有可能代码实现有问题。5 基于深度学习的端到端数据关联近几年由于深度学习框架的兴起，端到端的训练和推理框架展现出一定的数据利用优势，而传统的数据关联算法基本都不满足可导可微的特性，因此出现了很多近似的端到端数据关联框架。这里由于篇幅有限，如果专栏和github反响还可以，后续我会考虑单独开一个基于深度学习的数据关联算法专题，现在我只简要介绍几类出现的框架。我将近期出现的端到端数据关联框架大致可分为：多特征输入，输出关联矩阵这类框架只完成了数据关联的任务，即完成对多个目标的匹配，如PAMI2019中的DAN网络结构：这种框架就是典型的输入历史帧多条跟踪轨迹的特征和当前帧多个特征序列，输出多对多的关联矩阵，这种方式是通过形式的拟合来近似数据关联。又比如ICCV2019的FAMNet：这个框架将SOT和数据关联相集成。综上，这些方法虽然从形式上近似了数据关联算法，但是都要解决两个问题，一个是所有跟踪轨迹和观测的匹配交互，一个是如何过滤虚警和误检。可微数据关联模块这类框架就是讲传统不可微的数据关联模块改造成可微的模块，比如DeepMOT:这种方式基于匈牙利算法求解过程中的row-wise和colunm-wise操作，利用Bi-RNN完成全局的关联记忆，最后将关联矩阵通过连续的0~1的数据代替0-1匹配关系，从而实现可微。基于RNN的数据关联预测这种方式的特点在于，利用过去时间的跟踪记忆，基于不同行人的空间分布进行位置关系预测，比如ICCV2017的AMIR算法：不过这类算法严格来说不能划分为数据关联类算法，这里我提出来肯定是有争议的~基于图卷积的数据关联近几年图卷积网络在视觉领域开始热门起来，也有个别团队采用了这种方式，即利用图卷积网络的消息传递机制，模拟离线数据关联的网络图，这种方式的优点在于可以在线学习：参考资料[1] SUN S, AKHTAR N, SONG H, et al. Deep affinity network for multiple object tracking[J]. IEEE transactions on pattern analysis and machine intelligence, 2019.[2] CHU P, LING H. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2019. 6172-6181.[3] XU Y, BAN Y, ALAMEDA-PINEDA X, et al. DeepMOT: A Differentiable Framework for Training Multiple Object Trackers[J]. arXiv preprint arXiv:1906.06618, 2019.[4] BRASó G, LEAL-TAIXé L. Learning a Neural Solver for Multiple Object Tracking[J]. arXiv preprint arXiv:1912.07515, 2019.[5] SADEGHIAN A, ALAHI A, SAVARESE S. Tracking the untrackable: Learning to track multiple cues with long-term dependencies[C]. in: Proceedings of the IEEE International Conference on Computer Vision. 2017. 300-311.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>数据关联</tag>
        <tag>多目标跟踪</tag>
        <tag>最小代价流 - End2End</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多目标跟踪中的数据关联代码实践(上)]]></title>
    <url>%2F2020%2F03%2F04%2F%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5(%E4%B8%8A)%2F</url>
    <content type="text"><![CDATA[前言接下来我们将对多目标跟踪任务中的数据关联算法进行研究，我将结合一些文献和自己的理解，利用一些工具对相关数据关联算法进行实验，包括基于IOU的贪婪匹配、基于匈牙利和KM算法线性偶图匹配、基于图论的离线数据关联(主要介绍最小代价流)以及最新的一些基于深度学习的端到端数据关联网络。代码我都会随博客一起发到github。1 背景简介之前我们也说到，目前主流的MOT框架是DBT框架，这种框架的特点就是离不开数据关联算法，不论是对不同帧之间跟踪轨迹的关联还是跟踪轨迹和观测量的关联， 有数据关联才能更好的保证目标ID的连续性。数据关联算法以偶图匹配类为主，尤其是在离线跟踪中存在有大量基于图论的算法，比如：最小代价流、最大流、最小割、超图等等。而近两年也出现了一批端到端的数据关联算法，对于这一点呢，有几个很大的问题，一个是数据关联是一个n:m的问题，而网络对于输出的尺寸一般要求是固定的，如果构建关联矩阵是一个问题。另外，数据关联从理论上来讲是一个不可导的过程，其包含有一些排序的过程，如何处理这一点也很重要。2 基于IOU的贪婪匹配2.1 IOU Tracker &amp; V-IOU Tracker不得不说IOU Tracker和他的改进版V-IOU Tracker算是多目标跟踪算法中的一股清流，方法特别简单粗暴，对于检测质量很好的场景效果比较好。首先我们交代一下IOU的度量方式：$$ IOU\left( {a,b} \right) = \frac{{Area\left( a \right) \cap Area\left( b \right)}}{{Area\left( a \right) \cup Area\left( b \right)}} $$IOU Tracker的跟踪方式没有跟踪，只有数据关联，关联指标就是IOU，关联算法就是一种基于IOU的贪婪匹配算法：这里我们留到下一节将，我会利用不同的贪婪方式进行IOU匹配。那么V-IOU的改进就是，由于IOU Tracker仅仅是对观测量进行了关联，当目标丢失或者检测不到的时候，便无法重建轨迹，因此V-IOU加入了KCF单目标跟踪器来弥补这一漏洞，也很粗暴。。2.2 IOU Matching基于贪婪算法的数据关联的核心思想就是，不考虑整体最优，只考虑个体最优。这里我设计了两种贪婪方式，第一种Local IOU Matching,即依次为每条跟踪轨迹分配观测量，只要IOU满足条件即可，流程如下：IOU Tracker就是采用的这种局部贪心方式，这种贪心策略的特点是每次为当前跟踪轨迹选择与之IOU最大的观测量。那么我们再提出一种全局的贪婪策略，即每次选择所有关联信息中IOU最大的匹配对，流程如下：这两种贪婪方式的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def GreedyAssignment(cost, threshold = None, method = 'global'): """Using iou matching to make linear assignment Parameters ---------- cost : ndarray A NxM matrix for costs between each track_ids with dection_ids threshold: float if cost &gt; threshold, then will not be considered method: str eg: global, local Returns ------- row_idx: List of matched tracks (&lt;=N,) assigned tracklets' id col_idx: List of matched dets (&lt;=M,) assigned dets' id unmatched_rows: List of unmatched tracks unassigned tracklets' id unmatched_cols: List of unmatched dets unassigned dets' id """ cost_c = np.atleast_2d(cost) sz = cost_c.shape if threshold is None: threshold = 1.0 row_idx = [] col_idx = [] if method == 'global': vector_in = list(range(sz[0])) vector_out = list(range(sz[1])) while min(len(vector_in), len(vector_out)) &gt; 0: v = cost_c[np.ix_(vector_in, vector_out)] min_cost = np.min(v) if min_cost &lt;= threshold: place = np.where(v == min_cost) row_idx.append(vector_in[place[0][0]]) col_idx.append(vector_out[place[1][0]]) del vector_in[place[0][0]] del vector_out[place[1][0]] else: break else: vector_in = [] vector_out = list(range(sz[1])) index = 0 while min(sz[0] - len(vector_in), len(vector_out)) &gt; 0: if index &gt;= sz[0]: break place = np.argmin(cost_c[np.ix_([index], vector_out)]) if cost_c[index, vector_out[place]] &lt;= threshold: row_idx.append(index) col_idx.append(vector_out[place]) del vector_out[place] else: vector_in.append(index) index += 1 vector_in += list(range(index, sz[0])) return np.array(row_idx), np.array(col_idx), np.array(vector_in), np.array(vector_out)下面我们先看看两种方式的跟踪效果：可以看到跟踪效果并不是很差，那么我们观察二者定量的结果则为：Local: MOTA=0.77, IDF1=0.83Global: MOTA=0.79, IDF1=0.88相比之下全局的贪心策略比局部的要好3 线性分配3.1 匈牙利算法和KM算法简介线性分配问题也叫指派问题，通常的线性分配任务是给定N个workers和N个tasks，结合相应的N×N的代价矩阵，就能得到匹配组合。其模型如下：$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} = 1\\ \sum\limits_{j = 1}^N {{x_{ij}}} = 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$上述模型有个问题，即要求workers和tasks的数量对等，然而在MOT问题中待匹配的跟踪轨迹和检测数量大概率不相同，而且我们经常还会设定阈值来限制匹配。匈牙利算法是专门用来求解指派问题的算法，并且通常用于求解二分图最大匹配的。也就是说我们需要先利用规则判断边是否连接，然后匹配，因此不会出现非边节点存在匹配，只有可能出现剩余未匹配：$$ \begin{array}{l} max = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^M {{L_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} \le 1\\ \sum\limits_{j = 1}^M {{x_{ij}}} \le 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$匈牙利算法的问题在于一旦确定边之后就没有相对优劣了，所以我们这里介绍带权二分图匹配KM，顾名思义，就是求最小代价，只不过是不对等匹配：$$ \begin{array}{l} min = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^M {{C_{ij}}{x_{ij}}} } \\ s.t.\left\{ \begin{array}{l} \sum\limits_{i = 1}^N {{x_{ij}}} {\rm{ = }}1\\ \sum\limits_{j = 1}^M {{x_{ij}}} \le 1\\ {x_{ij}} = 0\;or\;1 \end{array} \right. \end{array} $$这里我们可以看到有一个维度的和要求是1，原因就是必须要保证有一个维度满分配，不然就会直接不建立连接了。3.2 实验对比这里我们借用scipy工具箱实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354inf_cost = 1e+5def LinearAssignment(cost, threshold = None, method = 'KM'): """Using Hungarian or KM algorithm to make linear assignment Parameters ---------- cost : ndarray A NxM matrix for costs between each track_ids with dection_ids threshold: float if cost &gt; threshold, then will not be considered method : str 'KM': weighted assignment 'Hungarian': 01 assignment Returns ------- row_idx: List of matched tracks (&lt;=N,) assigned tracklets' id col_idx: List of matched dets (&lt;=M,) assigned dets' id unmatched_rows: List of unmatched tracks unassigned tracklets' id unmatched_cols: List of unmatched dets unassigned dets' id min_cost: float cost of assignments """ cost_c = deepcopy(np.atleast_2d(cost)) sz = cost_c.shape if threshold is not None: cost_c = np.where(cost_c &gt; threshold, inf_cost, cost_c) if method == 'Hungarian': t = threshold if threshold is not None else inf_cost cost_c = np.where(cost_c &lt; t, 0, cost_c) # linear assignment row_ind, col_ind = linear_sum_assignment(cost_c) if threshold is not None: t = inf_cost - 1 if threshold == inf_cost else threshold mask = cost_c[row_ind, col_ind] &lt;= t row_idx = row_ind[mask] col_idx = col_ind[mask] else: row_idx, col_idx = row_ind, col_ind unmatched_rows = np.array(list(set(range(sz[0])) - set(row_idx))) unmatched_cols = np.array(list(set(range(sz[1])) - set(col_idx))) min_cost = cost[row_idx, col_idx].sum() return row_idx, col_idx, np.sort(unmatched_rows), np.sort(unmatched_cols), min_cost同样地，为了更好地对比，我们以IOU为代价指标，分别以匈牙利算法和KM算法为数据关联算法进行实验，有意思的是对于MOT-04-SDP视频而言，二者并没有太大区别，整体效果跟Global IOU Assignment一致。以上代码我都放到了github。]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>数据关联</tag>
        <tag>多目标跟踪</tag>
        <tag>iou track - 匈牙利算法 - KM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kalman滤波在MOT中的应用(三)——实践篇]]></title>
    <url>%2F2020%2F03%2F01%2FKalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%B8%89)%E2%80%94%E2%80%94%E5%AE%9E%E8%B7%B5%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以代码实践进行讲解。下文中所有的代码都会开源在https://github.com/nightmaredimple/libmot。1Kalman Filter本章将结合Kalman理论部分进行讲述，Kalman滤波器主要分为预测和更新两个阶段，在这之前能，我们需要预先设定状态变量和观测变量维度、协方差矩阵、运动形式和转换矩阵：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def __init__(self, dim_x, dim_z, dim_u = 0, x = None, P = None, Q = None, B = None, F = None, H = None, R = None): """Kalman Filter Refer to http:/github.com/rlabbe/filterpy Method ----------------------------------------- Predict | Update ----------------------------------------- | K = PH^T(HPH^T + R)^-1 x = Fx + Bu | y = z - Hx P = FPF^T + Q | x = x + Ky | P = (1 - KH)P ----------------------------------------- note: In update unit, here is a more numerically stable way: P = (I-KH)P(I-KH)' + KRK' Parameters ---------- dim_x: int dims of state variables, eg:(x,y,vx,vy)-&gt;4 dim_z: int dims of observation variables, eg:(x,y)-&gt;2 dim_u: int dims of control variables,eg: a-&gt;1 p = p + vt + 0.5at^2 v = v + at =&gt;[p;v] = [1,t;0,1][p;v] + [0.5t^2;t]a """ assert dim_x &gt;= 1, 'dim_x must be 1 or greater' assert dim_z &gt;= 1, 'dim_z must be 1 or greater' assert dim_u &gt;= 0, 'dim_u must be 0 or greater' self.dim_x = dim_x self.dim_z = dim_z self.dim_u = dim_u # initialization # predict self.x = np.zeros((dim_x, 1)) if x is None else x # state self.P = np.eye(dim_x) if P is None else P # uncertainty covariance self.Q = np.eye(dim_x) if Q is None else Q # process uncertainty for prediction self.B = None if B is None else B # control transition matrix self.F = np.eye(dim_x) if F is None else F # state transition matrix # update self.H = np.zeros((dim_z, dim_x)) if H is None else H # Measurement function z=Hx self.R = np.eye(dim_z) if R is None else R # observation uncertainty self._alpha_sq = 1. # fading memory control self.z = np.array([[None] * self.dim_z]).T # observation self.K = np.zeros((dim_x, dim_z)) # kalman gain self.y = np.zeros((dim_z, 1)) # estimation error self.S = np.zeros((dim_z, dim_z)) # system uncertainty, S = HPH^T + R self.SI = np.zeros((dim_z, dim_z)) # inverse system uncertainty, SI = S^-1 self.inv = np.linalg.inv self._mahalanobis = None # Mahalanobis distance of measurement self.latest_state = 'init' # last process name上述即对Kalman滤波器中各个参数的初始化，一般各个协方差矩阵都会初始化为单位矩阵，因此具体的矩阵初始化还需要针对特定场景设计，会在下一章介绍。然后进入预测环节，这里我们为了保证通用性，引入了遗忘系数$\alpha$，其作用在于调节对过往信息的依赖程度，$\alpha$越大对历史信息的依赖越小。$$ \begin{array}{l}x = Fx + Bu\\P = \alpha FxF^T + Q\end{array} $$相应的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344def predict(self, u = None, B = None, F = None, Q = None): """ Predict next state (prior) using the Kalman filter state propagation equations: x = Fx + Bu P = fading_memory*FPF^T + Q Parameters ---------- u : ndarray Optional control vector. If not `None`, it is multiplied by B to create the control input into the system. B : ndarray of (dim_x, dim_z), or None Optional control transition matrix; a value of None will cause the filter to use `self.B`. F : ndarray of (dim_x, dim_x), or None Optional state transition matrix; a value of None will cause the filter to use `self.F`. Q : ndarray of (dim_x, dim_x), scalar, or None Optional process noise matrix; a value of None will cause the filter to use `self.Q`. """ if B is None: B = self.B if F is None: F = self.F if Q is None: Q = self.Q elif np.isscalar(Q): Q = np.eye(self.dim_x) * Q # x = Fx + Bu if B is not None and u is not None: self.x = F @ self.x + B @ u else: self.x = F @ self.x # P = fading_memory*FPF' + Q self.P = self._alpha_sq * (F @ self.P @ F.T) + Q self.latest_state = 'predict'而对于更新阶段，有：$$ \left\{ \begin{array}{l} K = PH^T/\left( {HPH^T + R} \right)\\ x = Hx + K\left( {z - Hx} \right)\\ P = (1 - KH)P \end{array} \right. $$不过在实际工程应用中通常会做一些微调：$$ P = \left( {1 - KH} \right)P{\left( {1 - KH} \right)^T} + KR{K^T} $$12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def update(self, z, R = None, H = None): """ Update Process, add a new measurement (z) to the Kalman filter. K = PH^T(HPH^T + R)^-1 y = z - Hx x = x + Ky P = (1 - KH)P or P = (I-KH)P(I-KH)' + KRK' If z is None, nothing is computed. Parameters ---------- z : (dim_z, 1): array_like measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector. R : ndarray, scalar, or None Optionally provide R to override the measurement noise for this one call, otherwise self.R will be used. H : ndarray, or None Optionally provide H to override the measurement function for this one call, otherwise self.H will be used. """ if z is None: self.z = np.array([[None] * self.dim_z]).T self.y = np.zeros((self.dim_z, 1)) return z = reshape_z(z, self.dim_z, self.x.ndim) if R is None: R = self.R elif np.isscalar(R): R = np.eye(self.dim_z) * R if H is None: H = self.H if self.latest_state == 'predict': # common subexpression for speed PHT = self.P @ H.T # S = HPH' + R # project system uncertainty into measurement space self.S = H @ PHT + R self.SI = self.inv(self.S) # K = PH'inv(S) # map system uncertainty into kalman gain self.K = PHT @ self.SI # P = (I-KH)P(I-KH)' + KRK' # This is more numerically stable and works for non-optimal K vs # the equation P = (I-KH)P usually seen in the literature. I_KH = np.eye(self.dim_x) - self.K @ H self.P = I_KH @ self.P @ I_KH.T + self.K @ R @ self.K.T # y = z - Hx # error (residual) between measurement and prediction self.y = z - H @ self.x self._mahalanobis = math.sqrt(float(self.y.T @ self.SI @ self.y)) # x = x + Ky # predict new x with residual scaled by the kalman gain self.x = self.x + self.K @ self.y self.latest_state = 'update'其中我们可以注意到马氏距离的计算方式：1self._mahalanobis = math.sqrt(float(self.y.T @self.SI @ self.y))另外，我们保存上个阶段的状态，其主要作用在于，第一防止多次更新带来的重复运算，第二防止前一次更新对下一次更新参数造成影响。2KalmanTracker那么对于Kalman滤波器的跟踪器设计，我们这里直接借鉴DeepSort的参数设计方案%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E7%AF%87/](https://huangpiao.tech/2020/02/29/Kalman滤波在MOT中的应用(二)——应用篇/),因此基于KalmanFilter的设计，需要在初始化阶段之后修改对应的参数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def __init__(self, box, fading_memory = 1.0, dt = 1.0, std_weight_position = 0.05, std_weight_velocity = 0.00625): """Tracking bounding boxes in assumption of uniform linear motion The 8-dimensional state space x, y, a, h, vx, vy, va, vh contains the bounding box center position (x, y), aspect ratio a, height h, and their respective velocities. Object motion follows a constant velocity model. The bounding box location (x, y, a, h) is taken as direct observation of the state space (linear observation model). Parameters -------------- box: array like 1x4 matrix of boxes (x,y,w,h) fading_memory: float larger means fading more dt: float time step for each update std_weight_position: float std for position std_weight_velocity:flaot std for velovity """ box = np.atleast_2d(box) #1x4 # initialization self.x_dim = 8 self.z_dim = 4 self.dt = dt state = self.box2state(box) state = np.r_[state, np.zeros_like(state)] #8x1 self.F = np.eye(self.x_dim, self.x_dim) for i in range(self.z_dim): self.F[i, self.z_dim + i] = self.dt self._std_weight_position = std_weight_position self._std_weight_velocity = std_weight_velocity std = [ 2 * self._std_weight_position * state[3][0], 2 * self._std_weight_position * state[3][0], 1e-2, 2 * self._std_weight_position * state[3][0], 10 * self._std_weight_velocity * state[3][0], 10 * self._std_weight_velocity * state[3][0], 1e-5, 10 * self._std_weight_velocity * state[3][0]] covariance = np.diag(np.square(std)) self.H = np.eye(self.z_dim, self.x_dim) self.kf = KalmanFilter(dim_x = self.x_dim, dim_z = self.z_dim , x = state, P = covariance, F = self.F, H = self.H) self.kf.alpha = fading_memory self._x = self.kf.x self._mahalanobis = self.kf.mahalanobis其中，DeepSort对于Q和R的设计中，为了保证各自对目标尺度更加敏感，采用了自适应的方式，即同样的公式应用在每一次跟踪：1234567891011121314151617181920212223242526272829303132333435363738394041424344def predict(self): """Predict next state (prior) using the Kalman filter state propagation equations: x = Fx + Bu P = fading_memory*FPF^T + Q """ std_pos = [ self._std_weight_position * self.kf.x[3][0], self._std_weight_position * self.kf.x[3][0], 1e-2, self._std_weight_position * self.kf.x[3][0]] std_vel = [ self._std_weight_velocity * self.kf.x[3][0], self._std_weight_velocity * self.kf.x[3][0], 1e-5, self._std_weight_velocity * self.kf.x[3][0]] motion_cov = np.diag(np.square(np.r_[std_pos, std_vel])) self.kf.predict(Q = motion_cov)def update(self, measurement): """ Update Process, add a new measurement (z) to the Kalman filter. K = PH^T(HPH^T + R)^-1 y = z - Hx x = x + Ky P = (1 - KH)P or P = (I-KH)P(I-KH)' + KRK' Parameters -------------- measurement: array like 1x4 matrix of boxes (x,y,w,h) """ box = np.atleast_2d(measurement) # 1x4 z = self.box2state(box) # 4x1 std = [ self._std_weight_position * self.kf.x[3][0], self._std_weight_position * self.kf.x[3][0], 1e-1, self._std_weight_position * self.kf.x[3][0]] innovation_cov = np.diag(np.square(std)) self.kf.update(z = z, R = innovation_cov)另外，为了方便多个观测量对Kalman滤波器的多次更新，我加入了一个批处理模块，主要作用有：防止重复更新造成的变量内存改变、消除重复更新参数部分.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def batch_filter(self, zs): """ Batch processes a sequences of measurements. Parameters ---------- zs : list-like list of measurements at each time step `self.dt`. Missing measurements must be represented by `None`. Returns ------- means : np.array((n,dim_x,1)) array of the state for each time step after the update. Each entry is an np.array. In other words `means[k,:]` is the state at step `k`. covariance : np.array((n,dim_x,dim_x)) array of the covariances for each time step after the update. In other words `covariance[k,:,:]` is the covariance at step `k`. mahalanobis: np.array((n,1)) array of the mahalanobises for each time step during the update """ zs = np.atleast_2d(zs) n = zs.shape[0] # mean estimates from Kalman Filter x_copy = deepcopy(self.kf.x) means = np.zeros((n, self.x_dim, 1)) # state covariances from Kalman Filter covariances = np.zeros((n, self.x_dim, self.x_dim)) mahalanobis = np.zeros(n) std = [ self._std_weight_position * self.kf.x[3][0], self._std_weight_position * self.kf.x[3][0], 1e-1, self._std_weight_position * self.kf.x[3][0]] innovation_cov = np.diag(np.square(std)) if n &gt; 0: for i, z in enumerate(zs): self.kf.x = deepcopy(x_copy) measurement = self.box2state(z) # 4x1 self.kf.update(z = measurement, R = innovation_cov) means[i, :] = deepcopy(self.kf.x) if i == 0: covariances = np.tile(self.kf.P[np.newaxis, :, :],(n,1,1)) mahalanobis[i] = deepcopy(self.kf._mahalanobis) return (means, covariances, mahalanobis)3Example前两章将Kalman滤波器和跟踪器的代码层面都设计好了，接下来我们以MOT17-10数据集为例进行跟踪实验。这里不采用DeepSort的方式，我自己简单搭建了一套流程：Step1 我们先初始化参数：123456789track_len = 655 # total tracking lengththresh_det = 0.75 # threshold for detection，this is for SDP detectorthresh_track = chi2inv95[4] # threshold for data association, specifically the mahalanobis distancefading_memory = 1.14 # fading memory for predictiondt = 0.15 # time step for predictionstd_weight_position = 0.04 # std of position predictionstd_weight_velocity = 0.05 # std of velocity predictionpatience = 2 # patience for waiting reconnectionmin_len = 4 # mininum length of active trajectoryStep2 读取detection和groundtruth文件，筛选出满足行人类别和检测置信度阈值的目标：123456789# prefetchgt = np.genfromtxt(dir_path + 'gt\\gt.txt', delimiter = ',')gt = gt[(gt[:, 0] &lt; track_len)&amp;(gt[:, 6] == 1) , :]mask = (gt[:, 7] == 1) | (gt[:, 7] == 2) | (gt[:, 7] == 7)gt = gt[mask].astype(np.int32)dets = np.genfromtxt(dir_path + 'det\\det.txt', delimiter = ',')dets = dets[(dets[:, 0] &lt; track_len)&amp;(dets[:, 6] &gt; thresh_det) , :]dets = dets.astype(np.int32)Step3 对每个目标新建一个Kalman滤波器，逐一进行预测、更新、数据关联。其中如果数据关联失败的话，对于匹配失败的跟踪轨迹，在一定时间内，我们依旧允许其预测。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# begin tracktotal_id = len(dets[dets[:, 0] == 1])for i, det in enumerate(dets[dets[:, 0] == 1]): tracks.append(&#123;'id' : i + 1, 'pause': 0, 'kf': LinearMotion(det[2:6], fading_memory = fading_memory, \ dt = dt, std_weight_position = std_weight_position, \ std_weight_velocity = std_weight_velocity) &#125;) record.append([1, i+1, det[2], det[3], det[4], det[5], 1])for i in range(2, track_len): det = dets[dets[:, 0] == i] cost = (thresh_track + 1)*np.ones((len(tracks), len(det))) save = [None] * len(tracks) track_copy = deepcopy(tracks) track_boxes = np.zeros((len(tracks), 4)) # predict for j, track in enumerate(tracks): track['kf'].predict() track_boxes[j] = track['kf'].x # iou_blocking if len(tracks) &gt; 0 and len(det) &gt; 0: keep = iou_blocking(track_boxes, det[:, 2:6], 2*track_boxes[:, 2:]) xs = np.zeros((det.shape[0], tracks[0]['kf'].x_dim, 1)) Ps = np.zeros((det.shape[0], tracks[0]['kf'].x_dim, tracks[0]['kf'].x_dim)) ds = np.zeros(det.shape[0]) # update for j, track in enumerate(tracks): xs[keep[j], :, :], Ps[keep[j], :, :], ds[keep[j]] = track['kf'].batch_filter(det[keep[j], 2:6]) save[j] = &#123;'xs': deepcopy(xs), 'Ps': deepcopy(Ps)&#125; cost[j, keep[j]] = ds[keep[j]] # data association row_idx, col_idx, unmatched_rows, unmatched_cols, _ = LinearAssignment(cost, threshold=thresh_track, method = 'KM') else: row_idx = [] col_idx = [] unmatched_rows = np.arange(len(tracks)) unmatched_cols = np.arange(len(det)) for r, c in zip(row_idx, col_idx): tracks[r]['kf'].kf.x = save[r]['xs'][c, :, :] tracks[r]['kf'].kf.P = save[r]['Ps'][c, :, :] tracks[r]['pause'] = 0 for r in np.flip(unmatched_rows, 0): if tracks[r]['pause'] &gt;= patience: del tracks[r] else: tracks[r]= deepcopy(track_copy[r]) tracks[r]['kf'].predict() tracks[r]['pause'] += 1 for c in unmatched_cols: tracks.append(&#123;'id': total_id + 1, 'pause': 0, 'kf': LinearMotion(det[c, 2:6], fading_memory=fading_memory, \ dt=dt, std_weight_position=std_weight_position, \ std_weight_velocity=std_weight_velocity) &#125;) total_id += 1 for track in tracks: if track['pause'] == 0: record.append([i, track['id'], track['kf'].x[0], track['kf'].x[1], track['kf'].x[2], track['kf'].x[3], 1]) else: record.append([i, track['id'], track['kf'].x[0], track['kf'].x[1], track['kf'].x[2], track['kf'].x[3], 0])record = np.array(record)值得注意的是其中的iou_blocking部分，这个模块使我们基于iou mask改进升级的，原本的iou是用来删除iou&lt;0.3的关联边，现在我们可以放宽要求，将不在目标邻域的观测删除：12345678910111213141516171819202122232425262728293031def iou_blocking(tracks, dets, region_shape): """Blocking regions for each tracks Parameters ----------- tracks: 2-dim ndarray Nx4 matrix of (x,y,w,h) dets: 2-dim ndarray Mx4 matrix of (x,y,w,h) region_shape: Tuple(w,h) or array-like (Nx2) region shape for each track Returns --------- blocks: ndarray of boolean(NxM) block sets for each track, """ tracks = np.atleast_2d(tracks) dets = np.atleast_2d(dets) if not isinstance(region_shape, tuple): region_shape = np.atleast_2d(region_shape) else: region_shape = np.array([[region_shape[0], region_shape[1]]]) region_shape = np.tile(region_shape, (tracks.shape[0], 1)) centers = tracks[:, :2] + tracks[:, 2:]/2. overlap = iou(np.c_[centers - region_shape/2., region_shape], dets) keep = overlap &gt; 0 return keepStep4 我们将轨迹中有效长度较短的轨迹视为无效轨迹：123456789101112# post processuremax_id = record[:, 1].flatten().max()new_record = Nonefor i in range(1, max_id + 1): temp = record[record[:, 1] == i] index = int(temp[:, -1].nonzero()[0][-1]) temp = temp[:(index+1), :-1] if len(temp) &gt; min_len or temp[-1, 0] == track_len or temp[0, 0] &gt; track_len - min_len - 1: if new_record is not None: new_record = np.r_[new_record, temp] else: new_record = temp上述过程呢，我们可以得到以下结果：DetectionMOTA↑MOTP↑IDF1↑ID Sw.↓SDP0.6750.2030.518201可视化效果如下：可以看到，在检测质量较好时，跟踪效果也还不错，以上的代码我都放在了https://github.com/nightmaredimple/libmot，参考资源[1]WOJKE N, BEWLEY A, PAULUS D. Simple online and realtime tracking with a deep association metric[C]. in: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. 3645-3649.[2]https://github.com/rlabbe/filterpy/tree/master/filterpy/[3]https://github.com/nwojke/deep_sort]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>Kalman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kalman滤波在MOT中的应用(二)——应用篇]]></title>
    <url>%2F2020%2F02%2F29%2FKalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%BA%8C)%E2%80%94%E2%80%94%E5%BA%94%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言Kalman滤波器是多目标跟踪任务中一个经典的运动模型，本次主要以经典应用为主。其中应用算法主要介绍Sort和Deepsort算法。Sort系列算法的原理不复杂，但是为近些年多目标跟踪的发展提供了很多的实验性baseline帮助，也帮助很多新人入门了。首先我们先谈谈Sort算法，这个算法实际上就是一个很直接的Kalman算法应用。正如我们上一讲所介绍的，Kalman滤波算法需要设计以下几个点：状态量x和观测量z的确定、各个协方差参数(P,Q,R)的确定、运动形式F和转换矩阵H的预设。1SortSort对于状态量x的设定是一个七维向量：$$ x = {\left[ {u,v,s,r,\dot u,\dot v,\dot s} \right]^T} $$分别表示目标中心位置的x,y坐标，相对原始目标框大小的尺度s（可以看做面积尺度）和当前目标框的纵横比，最后三个则是横向，纵向，尺度的变化速率，其中速度部分初始化为0；对于Detection based Tracking框架，由于提供了检测结果，因此相较于Kalman滤波器预测的状态，检测结果就充当了观测量的角色；各个协方差参数的设定，从Sort提供的代码来看，应该是经验参数：self.kf.R[2:,2:] *= 10.self.kf.P[4:,4:] *= 1000.self.kf.P *= 10.self.kf.Q[-1,-1] *= 0.01self.kf.Q[4:,4:] *= 0.01可以看到，对于Kalman的各个协方差，初始状态为：$$ \begin{array}{l} P = diag\left( {{{\left[ {\begin{array}{*{20}{c}} {10}&{10}&{10}&{10}&{1e4}&{1e4}&{1e4} \end{array}} \right]}^T}} \right)\\ Q = diag\left( {{{\left[ {\begin{array}{*{20}{c}} 1&1&1&1&{0.01}&{0.01}&{1e{\rm{ - }}4} \end{array}} \right]}^T}} \right)\\ R = diag\left( {{{\left[ {\begin{array}{*{20}{c}} 1&1&{10}&{10} \end{array}} \right]}^T}} \right) \end{array} $$可以看到，作者主要是基于速度状态不确定性大于位置形状状态不确定性的依据，以及观测行人框的检测位置比形状置信度高的先验；运动形式和转换矩阵的确定，无论是Sort还是Deepsort都是基于匀速运动的假设，即：$$ \begin{array}{l} \left[ {\begin{array}{*{20}{c}} u\\ v\\ s \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} u\\ v\\ s \end{array}} \right] + \left[ {\begin{array}{*{20}{c}} {\dot u}\\ {\dot v}\\ {\dot s} \end{array}} \right] \Rightarrow F = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&1&0&0\\ 0&1&0&0&0&1&0\\ 0&0&1&0&0&0&1\\ 0&0&0&1&0&0&0\\ 0&0&0&0&1&0&0\\ 0&0&0&0&0&1&0\\ 0&0&0&0&0&0&1 \end{array}} \right]\\ z = Hx \Rightarrow \left[ {\begin{array}{*{20}{c}} u\\ v\\ s\\ r \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&0&0&0\\ 0&1&0&0&0&0&0\\ 0&0&1&0&0&0&0\\ 0&0&0&1&0&0&0 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} u\\ v\\ s\\ r\\ {\dot u}\\ {\dot v}\\ {\dot s} \end{array}} \right] \end{array} $$然而对于Kalman滤波器的更新环节而言，观测量如何选择也是一个很重要的部分，即对于n条跟踪轨迹而言，如果存在m个观测，如果分配的问题。通常我们采用匈牙利算法/KM算法先对Klaman滤波器预测阶段得到的运动估计和观测进行关联，代价矩阵为二者之间的IOU。匹配成功的则进行Kalman滤波器的更新，匹配失败的跟踪轨迹则视为丢失，匹配失败的观测量则视为新增轨迹；综上就可以实现各个目标的运动估计和更新。其中对于丢失的轨迹，考虑到跟踪对象或者检测质量的影响，目标经常容易短暂消失，一般情况下我们是允许他参与到后续帧的匹配关联以继续跟踪的。然而Sort跟踪器主要依据的是运动信息，也比较简单，所以实际实现过程中只允许丢失一帧。2Deepsort同样地，DeepSort对于Sort算法进行了改进，改进方向有：状态量的调整、协方差参数的调整、代价矩阵的确定方式。DeepSort对于状态量x的设定是一个八维向量：$$ x = {\left[ {u,v,\gamma ,h,\dot u,\dot v,\dot \gamma ,\dot h} \right]^T} $$分别表示目标中心位置的x,y坐标，当前目标框的纵横比和高，以及上述四个状态的速度变量。对于Detection based Tracking框架，由于提供了检测结果，因此相较于Kalman滤波器预测的状态，检测结果就充当了观测量的角色；各个协方差参数的设定，从DeepSort提供的代码来看，可以看到，对于Kalman的各个协方差，初始状态为：$$ \left\{ \begin{array}{l} P = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {2{\sigma _p}h}&{2{\sigma _p}h}&{1e - 2}&{2{\sigma _p}h}&{10{\sigma _v}h}&{10{\sigma _v}h}&{1e - 5}&{10{\sigma _v}h} \end{array}} \right]}^T}} \right)^2}\\ Q = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {{\sigma _p}h}&{{\sigma _p}h}&{1e - 2}&{{\sigma _p}h}&{{\sigma _v}h}&{{\sigma _v}h}&{1e - 5}&{{\sigma _v}h} \end{array}} \right]}^T}} \right)^2}\\ R = diag{\left( {{{\left[ {\begin{array}{*{20}{c}} {{\sigma _p}h}&{{\sigma _p}h}&{1e - 1}&{{\sigma _p}h} \end{array}} \right]}^T}} \right)^2} \end{array} \right. $$可以看到，作者引入了位置和速度的标准差这两个额外的参数，整体的设计来看，依旧是速度相对于位置形状不确定性要高，长宽比尽可能变化幅度小。运动形式和转换矩阵的确定，无论是Sort还是Deepsort都是基于匀速运动的假设，不过Deepsort新增了运动的步长即：$$ \begin{array}{l} F = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&{dt}&0&0&0\\ 0&1&0&0&0&{dt}&0&0\\ 0&0&1&0&0&0&{dt}&0\\ 0&0&0&1&0&0&0&{dt}\\ 0&0&0&0&1&0&0&0\\ 0&0&0&0&0&1&0&0\\ 0&0&0&0&0&0&1&0\\ 0&0&0&0&0&0&0&1 \end{array}} \right]\\ z = Hx \Rightarrow \left[ {\begin{array}{*{20}{c}} u\\ v\\ \gamma \\ h \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&0&0&0&0&0&0&0\\ 0&1&0&0&0&0&0&0\\ 0&0&1&0&0&0&0&0\\ 0&0&0&1&0&0&0&0 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} u\\ v\\ \gamma \\ h\\ {\dot u}\\ {\dot v}\\ {\dot \gamma }\\ {\dot h} \end{array}} \right] \end{array} $$对于预测估计和观测量的关联，DeepSort引入了更多的设定，其中IOU的判定作为初筛标准，将不符合要求的关联删除，实际采用的是马氏距离和余弦距离的加权值。其中马氏距离，利用的是Kalman滤波器更新阶段的系统协方差：$$ {d^{\left( 1 \right)}}\left( {i,j} \right) = {\left( {{d_j} - {y_i}} \right)^T}S_i^{ - 1}\left( {{d_j} - {y_i}} \right) $$不过马氏距离的取值范围并没有上限，这不利于确定阈值，因此作者巧妙地利用马氏距离和卡方分布的联系，不同维度状态下马氏距离满足95%置信度的阈值如下：| 维度 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 || :—: | :—-: | :—-: | :—-: | :——: | :——: | :——: | :——: | :——: | :——: || 阈值 | 3.842 | 5.992 | 7.815 | 9.4877 | 11.070 | 12.592 | 14.067 | 15.507 | 16.919 |这里实际比对的时候并不会考虑速度状态， 因此只有四维即9.4877。对于表观特征，作者训练了一个Re-ID网络用于特征提取，然后计算余弦距离，最后对两个距离加权。其中作者对于特征向量的处理采用了比较简陋的特征融合方式，即保存轨迹的历史特征，最多100个，每次计算余弦距离之前先将历史特征平均。最后由于轨迹丢失会暂时保留，因此针对不同时效性的轨迹，采用了级联匹配的方式：只不过这个看起来可能比较不清楚，CSDN博客有个整理得比较好的：整理而言呢，Sort系列算法原理比较简单，尤其是Sort算法，而Deepsort算法则是开创了Kalman滤波器+Re-ID这种模式的MOT算法先河。两种算法的速度也非常快，不过对于其论文里面所说的SOTA效果，感觉还是有一定的作弊，因为他们采用的是自己训练的Faster RCNN检测器，以及POI检测器。参考资源[1]BEWLEY A, GE Z, OTT L, et al. Simple online and realtime tracking[C]. in: 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016. 3464-3468.[2]WOJKE N, BEWLEY A, PAULUS D. Simple online and realtime tracking with a deep association metric[C]. in: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. 3645-3649.[3]https://github.com/abewley/sort/blob/master/sort.py[4]https://github.com/nwojke/deep_sort[5]https://blog.csdn.net/zjc910997316/article/details/83721573]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>Kalman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kalman滤波在MOT中的应用(一)——理论篇]]></title>
    <url>%2F2020%2F02%2F29%2FKalman%E6%BB%A4%E6%B3%A2%E5%9C%A8MOT%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8(%E4%B8%80)%E2%80%94%E2%80%94%E7%90%86%E8%AE%BA%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言Kalman滤波器是多目标跟踪任务中一个经典的运动模型，接下来会从理论、发展和代码实践三个方面对其进行展开，本次主要以理论基础为主。 在这篇之前还有一个关于多目标背景任务的介绍，不过被知乎删了，可以去我的博客看1背景介绍卡尔曼滤波（Kalman）无论是在单目标还是多目标领域都属于很基础的一种算法，这个是属于自动控制理论中的一种方法，所以对于工科比较容易理解。为了方便理解，我下面对其进行详细的讲解。首先，卡尔曼滤波器处理的是随机信号，例如，假设我们研究一个房间的温度，依据我们的经验判断，房间温度是恒定的（假设为23℃），但是由于我们的经验不完全可信，所以我们可以引入高斯白噪声来衡量一定的偏差程度（假设上一时刻的偏差是3，但是自己对于预测的不确定度为4，那么此时预测的偏差为5，这里采用的勾股定理）。这时候我们在房间放一个温度计，但是温度计本身也不一定准确，测量值（25℃）与实际值的偏差也视为一种高斯白噪声（假设为4），那么现在同时存在一个根据上一个时刻温度得来的估计量和这个时刻的测量值。假设这两个值为23℃和25℃，这其实就对应着跟踪问题中的运动估计和实际跟踪结果，此时我们究竟相信谁呢？可利用他们的均方误差来计算：$$ \begin{array}{l} {H^2} = \frac{{{5^2}}}{{{5^2} + {4^2}}} = 0.78 \Rightarrow T = 23 + 0.78 * \left( {25 - 23} \right) = 24.56\\ Err = \sqrt {\left( {1 - H} \right) * {5^2}} = 2.35 \end{array} $$由此可将均方误差不断地传递下去，从而估计估算出最优的温度值。为了方便解释卡尔曼滤波方程，下面以一辆小车的运动为例，假设我们已知上一时刻小车的状态，现在要估计当前时刻的状态：$$ \begin{array}{l}\left\{ \begin{array}{l}{p_t} = {p_{t - 1}} + {v_{t - 1}} \times \Delta t + \frac{1}{2}{a_t} \times \Delta {t^2}\\{v_t} = {v_{t - 1}} + {a_t} \times \Delta t\end{array} \right.\\ \Rightarrow \left[ {\begin{array}{*{20}{c}}{{p_t}}\\{{v_t}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}1&{\Delta t}\\0&1\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{p_{t - 1}}}\\{{v_{t - 1}}}\end{array}} \right] + \left[ {\begin{array}{*{20}{c}}{\frac{{\Delta {t^2}}}{2}}\\{\Delta t}\end{array}} \right]{a_t}\\ \Rightarrow \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} _t^ - = {F_t}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_{t - 1}} + {B_t}{a_t}\end{array} $$可以看到，对于一个线性系统，我们能根据上一时刻状态得到一个简单的估计量，其中F代表了状态转移矩阵，B则代表了控制矩阵，反映了加速度如何作用于当前状态。假设每一时刻的各个维度的不确定性都通过协方差矩阵来描述，另外预测模型本身也不一定准确，所以系统状态的不确定性如下：$$ \Sigma _t^ - = F\Sigma _{t - 1}^ - {F^T} + Q $$有了预测值，现在我们通过在路上布设装置来测定小汽车的位置，观测值的误差记为V，然后将真实状态x通过一定变换，可以得到真实状态x和观测状态y的关系：$$ {y_t} = H{x_t} + V $$显然，这里的H是[1 0]，因为观测到的是位置信息p，同样的我们需要用一个协方差矩阵R来取代上式中的V，以衡量观测不确定性。现在，我们已知此时刻的预测值，观测值，以及几个不确定性矩阵，可以得到此时刻最终的估计：$$ \begin{array}{l} {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t} = {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t}^ - + {K_t}\left( {{y_t} - H{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_t}^ - } \right)\\ {K_t} = \Sigma _t^ - {H^T}{\left( {H\Sigma _t^ - {H^T} + R} \right)^{ - 1}} \end{array} $$其中 ${y_t} - H{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} _t}^ - $表示实际观测值和预估观测值之间的残差，$K_t$为卡尔曼系数，又叫滤波增益矩阵。可以看到其中同时包含预测状态的协方差矩阵和观测误差矩阵，如果我们相信预测模型多一点，那么对应的协方差矩阵会更小，则K会小一点，反之如果我们相信观测模型多一点，那么K会更大。最后我们需要更新最有估计值的噪声分布：$$ {\Sigma _t} = \left( {1 - {K_t}H} \right)\Sigma _t^ - $$2高斯分布融合先从高斯分布说起，Kalman滤波算法的假设分布即为高斯分布，而一维的高斯分布概率密度函数及其分布示意图如下：$$ f\left( {x,\mu ,\sigma } \right) = \frac{1}{{\sigma \sqrt {2\pi } }}\exp \left[ { - \frac{{{{\left( {x - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right] $$这是一个很简单的高斯分布图，标准差决定分布曲线宽窄，均值决定其中心位置。那么如果我们对任意两个高斯分布进行运算，则会得如下的效果图：其中两个高斯分布的运算都加入了归一化，可以发现，他们的分布融合也是高斯分布，下面我们对这两个新的高斯分布进行求解证明，主要讲乘法融合，加法以此类推。对于任意两个高斯分布，将二者相乘之后可得：$$ f\left( {{x_1}} \right)f\left( {{x_2}} \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right] $$对于这个概率分布函数，我们利用高斯分布的两个特性进行求解，其一是均值处分布函数取极大值，其二是均值处分布曲线的曲率为其二阶导数，并且与s2成反比。$$ \begin{array}{l} \because f\left( x \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right]\\ \therefore f'\left( x \right) = \frac{1}{{{\sigma _1}{\sigma _2}\sqrt {2\pi } }}\exp \left[ { - \left( {\frac{{{{\left( {x - {\mu _1}} \right)}^2}}}{{2{\sigma _1}^2}} + \frac{{{{\left( {x - {\mu _2}} \right)}^2}}}{{2{\sigma _2}^2}}} \right)} \right] * \left[ { - \left( {\frac{{x - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{x - {\mu _2}}}{{{\sigma _2}^2}}} \right)} \right]\\ \because f'\left( \mu \right) = 0\\ \therefore \frac{{\mu - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{\mu - {\mu _2}}}{{{\sigma _2}^2}} = \mu \frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}} - \frac{{{\mu _2}{\sigma _1}^2 + {\mu _1}{\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}} = 0\\ \therefore \mu = \frac{{{\mu _2}{\sigma _1}^2 + {\mu _1}{\sigma _2}^2}}{{{\sigma _1}^2 + {\sigma _2}^2}}\\ \because f''\left( x \right) = f'\left( x \right) * \left[ { - \left( {\frac{{x - {\mu _1}}}{{{\sigma _1}^2}} + \frac{{x - {\mu _2}}}{{{\sigma _2}^2}}} \right)} \right] - f\left( x \right)\frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}}\\ \therefore f''\left( \mu \right) = - f\left( \mu \right)\frac{{{\sigma _1}^2 + {\sigma _2}^2}}{{{\sigma _1}^2{\sigma _2}^2}}\\ \because f''\left( {{\mu _1}} \right) = - f\left( {{\mu _1}} \right)\frac{1}{{\sigma _1^2}},f''\left( {{\mu _2}} \right) = - f\left( {{\mu _2}} \right)\frac{1}{{\sigma _2^2}}\\ \therefore {\sigma ^2} = \frac{{{\sigma _1}^2{\sigma _2}^2}}{{{\sigma _1}^2 + {\sigma _2}^2}} \end{array} $$因此我们可以得到如下结论：$$ \begin{array}{l} f\left( {{x_1}} \right)f\left( {{x_2}} \right) \sim N\left( {\frac{{\sigma _1^2{\mu _2} + \sigma _2^2{\mu _1}}}{{\sigma _1^2 + \sigma _2^2}},\frac{{\sigma _1^2\sigma _2^2}}{{\sigma _1^2 + \sigma _2^2}}} \right)\\ f\left( {{x_1}} \right) + f\left( {{x_2}} \right) \sim N\left( {{\mu _1} + {\mu _2},\sigma _1^2 + \sigma _2^2} \right) \end{array} $$当然我们遇到的问题大多是多阶的所以要引入多维高斯分布：$$ \begin{array}{l} f\left( {x,\mu ,\Sigma } \right) = \frac{1}{{\sqrt {{{\left( {2\pi } \right)}^n}\left| \Sigma \right|} }}\exp \left[ { - \frac{1}{2}{{\left( {x - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( {x - \mu } \right)} \right]\\ \Rightarrow {f_1}{f_2} \sim N\left( {{\Sigma _2}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\mu _1} + {\Sigma _1}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\mu _2},{\Sigma _1}{{\left( {{\Sigma _1} + {\Sigma _2}} \right)}^{ - 1}}{\Sigma _2}} \right) \end{array} $$3线性卡尔曼滤波3.1理论推导首先假设状态变量为x，观测量为z，那么结合贝叶斯后验概率模型：$$ posterior = \frac{{likelihood \times prior}}{{normalization}} \Leftrightarrow P\left( {\left. x \right|z} \right) = \frac{{P\left( {\left. z \right|x} \right)P\left( x \right)}}{{P\left( z \right)}} $$多目标跟踪从形式上讲可以理解为最大化后验概率，现在结合第二节的内容，假设状态变量x服从高斯分布，反映的是运动模型的不稳定性。基于状态变量x的估计先验，观测量z也服从高斯分布，反映的是量测误差，比如传感器误差。那么我们就可以利用高斯分布的融合来刻画Kalman滤波器的更新部分。这里我们先给出一阶Kalman滤波器的公式，其中预测环节就是基于线性运动特性对状态变量的预测，即：$$ \begin{array}{l} x = Fx + Bu\\ P = FxF^T + Q \end{array} $$其中$x$为状态变量的均值，$P$为预测方差，那么$Fx$对应的高斯分布方差即为 $FxF^T$ ，而$Q$则是线性运动模型本身的误差，由此得到预测环节。即预测结果服从高斯分布$N(x,P)$。对于更新环节，同样地，假设量测误差分布满足$N(z,R)$，那么：$$ \begin{array}{l} \mu = \frac{{{\mu _z}{\sigma _x}^2 + {\mu _x}{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}} = \frac{{{\sigma _x}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}{\mu _z} + \frac{{{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}{\mu _x}\\ K = \frac{{{\sigma _x}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}}\\ \Rightarrow \mu = K{\mu _z} + \left( {1 - K} \right){\mu _x} = {\mu _x} + K\left( {{\mu _z} - {\mu _x}} \right)\\ \therefore {\sigma ^2} = \frac{{{\sigma _x}^2{\sigma _z}^2}}{{{\sigma _x}^2 + {\sigma _z}^2}} = K{\sigma _z}^2 = \left( {1 - K} \right){\sigma _x}^2 \end{array} $$代入变量得：$$ \left\{ \begin{array}{l} K = PH^T/\left( {HPH^T + R} \right)\\ x = Hx + K\left( {z - Hx} \right)\\ P = (1 - KH)P \end{array} \right. $$上式即为Kalman滤波器是更新环节，其中H是从状态变量到观测量/输出变量的转换矩阵。3.2实验分析我们可以看到的是，Kalman滤波器有很多参数，除去运动模型形式假设的F和B参数，存在有多个协方差矩阵P、Q、R。下面我们逐一分析各个参数的影响。协方差矩阵变化规律在此之前可以看看不经过更新校正的状态变量均值和方差的变化，假设有如下的运动方式：$$ \begin{array}{l} \left\{ \begin{array}{l} x = x + \Delta tv\\ v = v \end{array} \right. \Rightarrow \left[ {\begin{array}{*{20}{c}} x\\ v \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]\left[ {\begin{array}{*{20}{c}} x\\ v \end{array}} \right]\\ \Rightarrow P = \left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]P{\left[ {\begin{array}{*{20}{c}} 1&{\Delta t}\\ 0&1 \end{array}} \right]^T} \end{array} $$则有状态变量分布如下：可以看到，在不引入量测的情况下，物体一直保持匀速直线运动，所以其误差的协方差分布一直向水平方向倾斜。（1）不同Kalman模型下面我们分别用一阶和二阶的Kalman滤波器去跟踪一个直线运动的物体，其中一阶Kalman滤波完全依赖量测的矫正，二阶Kalman滤波加入了速度因素，可见二阶模型跟踪效果更好，不过其实在这里，如果加入控制变量u，也能恰好达到匀速直线运动的效果。（2） R和Q的影响对于匀速直线运动，我们保持量测误差R不变，对比运动估计误差Q发现，Q越小，模型越相信运动规律，而模型正好也是匀速直线运动，因此跟踪效果更好。而当R变大时，模型会更加不相信量测结果，从而使得状态变量的协方差越来越大，但是由于预测环节模型的准确性，跟踪依然比较准确，可以从图中看出，当初始状态偏差很大时，模型不相信量测，导致跟踪轨迹很难与目标轨迹一致，而当R变小却可以重新跟踪到。（3）P的影响对于上面两幅图，表面上看上去P=1时，跟踪轨迹跟贴近于真实轨迹，但是如果将协方差矩阵P中的参数绘制出来即为：我们可以发现，后者关于位置的方差变化趋势比较复杂，虽然二者均能跟踪到，但是当初始状态估计不好时，P过小会使得跟踪周期变长，而P较大时跟踪效果没有明显降低，因此通常P取值较大。参考资源https://github.com/rlabbe/filterpy]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>Kalman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多目标跟踪任务介绍与评价规则]]></title>
    <url>%2F2020%2F02%2F28%2F%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AF%84%E4%BB%B7%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[前言由于最近忙着整理毕设以及小论文发表，所以有些内容我暂时不能分享出来，先从多目标跟踪领域的基础知识开始分享，后续也会陆续结合代码讲解。如果有机会，还可以分享我在MOTChallenge上刷到sota的经验。这次先结合一篇综述和我的理解进行一个背景介绍。1. 多目标跟踪简介多目标跟踪可认为是多变量估计问题，即给定一个图像序列，$s_t^i$表示第 t 帧第 i 个目标的状态，${S_t} = \left( {s_t^1,s_t^2,...,s_t^{{M_t}}} \right)$表示第 t 帧所有目标的状态序列，$s_{{i_s}:{i_c}}^i = \left( {s_{{i_s}}^i,...,s_{{i_e}}^i} \right)$ 表示第 i 个目标的状态序列，其中${i_s}$和${i_e}$分别表示目标 i 出现的第一帧和最后一帧，${S_{1:t}} = \left( {{S_1},{S_2},...,{S_t}} \right)$表示所有目标从第一帧到第t帧的状态序列。这里的状态可以理解为目标对应图像中哪个位置，或者是否存在于此图像，这个是算法所求得的所有信息。在多目标跟踪领域最常见的就是tracking-by-detection算法，通过匹配得到对应的观测目标${O_t} = \left( {o_t^1,o_t^2,...,o_t^{{M_t}}} \right)$ ，${O_{1:t}} = \left( {{O_1},{O_2},...,{O_t}} \right)$ 表示所有目标从第一帧到第t帧的观测目标序列，这里的目标都是我们观测到的位置。那么多目标跟踪的目的就是找到所有目标的最佳状态序列，而在所有观测目标的状态序列上的条件分布上，可以通过最大化后验概率（MAP）得到。这里可以区分一下最大似然概率（MLE）和最大后验概率（MAP）：最大似然概率要了解最大似然概率，首先要区分似然（likelihood）和概率（probability），这里提出函数$P\left( {x|\theta } \right)$。其中$x$表示某一数据，$\theta $表示模型参数。那么当$x$为变量，$\theta $为常量时，该函数为概率函数，表示在该模型下，出现不同$x$的概率；反之则为似然函数，表示对于不同的模型参数，出现该$x$的概率，所以概率函数更偏向于结果，似然函数更偏向于结构化。了解了似然函数，顾名思义，最大化似然函数就是怎样设置模型参数能让一个预期的结果以最大概率出现。下面给出一个简单例子：抛掷一枚硬币多次，假设硬币正面朝上的概率为$\theta $，那么出现事件${x_0}$=“反正正正正反正正正反”的似然函数就是：$$ f\left( {{x_0},\theta } \right) = \left( {1 - \theta } \right) \times {\theta ^4} \times \left( {1 - \theta } \right) \times {\theta ^3} \times \left( {1 - \theta } \right) = {\theta ^7} \times {\left( {1 - \theta } \right)^3} $$可以发现，当 $\theta $=0.7时该似然函数最大，也就是说当 $\theta $=0.7时，事件${x_0}$ 的出现概率最大，为0.22%。最大后验概率最大似然函数是最大化似然函数，那么最大化后验概率则是在此基础上考虑了模型参数 $\theta $，其认为该参数也有概率分布，即先验概率。所以最大化后验概率是在最大化下面的函数：P\left( {{x_0}|\theta } \right)P\left( \theta \right) $$ 而由于${x_0}$ 已知，所以该函数可以通过贝叶斯公式转化为下面的后验概率：P\left( {\theta |{x_0}} \right) = \frac{{P\left( {{x_0}|\theta } \right)P\left( \theta \right)}}{{P\left( {{x_0}} \right)}} $$那么现在问题就比较抽象了，还是举个栗子说明吧。假设我们先验的认为 $\theta $=0.5的概率较高，那么不妨先假设$\theta $的先验概率分布为均值为0.5，方差为0.1的高斯分布。即硬币正面朝上概率的先验分布如下：后验概率对应的分布为：这里的分布与后验概率分布趋势一致，只是纵坐标绝对值不同而已，可以看到，当 $\theta $ =0.558时，后验概率最大，由此可见与最大似然概率的不同。回到多目标跟踪问题上，最大化后验概率在这里的体现就是在观测目标已知的前提下，选择最合理的状态序列分布，使得该观测目标序列出现的概率最大：$$ {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over S} _{1\;:\;t}} = \mathop {argmax}\limits_{{S_{1\;:\;t}}} P\left( {{S_{1\;:\;t}}|{O_{1\;:\;t}}} \right) $$2分类多目标跟踪问题由于比较复杂，所以基于不同假设前提的挑战赛有很多，由此作为多目标跟踪的分类依据。2.1初始化方法不同于单目标跟踪，多目标跟踪问题中并不是所有目标都会在第一帧出现，也并不是所有目标都会出现在每一帧。那么如何告知算法出现了新目标变成了一个问题，即不同的初始化方法。常见的初始化方法分为两大类，一个是Detection-Based-Tracking(DBT),一个是Detection-Free-Tracking(DFT)。DBT可以看到，DBT的方式就是典型的tracking-by-detection模式，即先检测目标，然后将目标关联进入跟踪轨迹中。那么就存在两个问题，第一，该跟踪方式非常依赖目标检测器的性能，第二，目标检测的实质是分类和回归，即该跟踪方式只能针对特定的目标类型，如：行人、车辆、动物。DFTDFT是单目标跟踪领域的常用初始化方法，即每当新目标出现时，人为告诉算法新目标的位置，这样做的好处是target free，坏处就是过程比较麻烦，存在过多的交互，所以DBT相对来说更受欢迎。2.2处理模式同样地，MOT也存在着不同的处理模式，Online和Offline两大类，其主要区别在于是否用到了后面帧的信息。Online TrackingOnline Tracking是对视频帧逐帧进行处理，当前帧的跟踪仅利用过去的信息。Offline Tracking不同于Online Tracking，Offline Tracking会利用前后视频帧的信息对当前帧进行目标跟踪，这种方式只适用于视频，如果应用于摄像头，则会有滞后效应，通常采用时间窗方式进行处理，以节省内存和加速。3外观模型3.1视觉表达全局颜色特征全局颜色特征除了灰度（即RGB2GRAY）之外，比较有名的就是单目标跟踪领域中的CN2特征了，该颜色特征将颜色空间划分为了黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄共11种，然后将其投影至10维子空间的标准正交基上。具体过程可见我之前关于KCF的一片博客。局部特征局部特征比较典型的就是光流法，其中比较典型的有KLT（Kanade-Lucas-Tomasi Tracking）算法，这个算法在CVPR94中经由来Jianbo Shi和Carlo Tomasi两人在《Good Features to Track》一文中提出shi-tomasi角点而改进，光流法的好处就是方便提取运动轨迹信息。其中在多目标跟踪领域利用光流特征比较出名的是NOMT算法：不过近几年深度学习领域发展迅速，基于深度光流框架（FlowNet系列等)的也有，比如最近刚出的《Multiple Object Tracking by Flowing and Fusing》，就是很粗暴地结合光流 和检测框架。区域特征和局部特征不同的是，区域特征是针对选定的区域进行特征提取的，以区域间的关系不同可分为三类：Zero-Order:这一层面一般指的是颜色特征，常用的有颜色直方图，亦或是类似于全图特征；First-Order：这一层面的特征则是用到了梯度信息，最常见的就是HOG特征，还有不常见的水平集特征；Up-to-Second-Order：这一层面用到了二阶梯度，一般采用区域协方差矩阵。深度特征现在深度学习领域中关于backbone框架的各种变种层出不穷，比如ResNet，VGGNet等。特征的选择有很多，我们可以根据速度、精度、鲁棒性三个方向进行不同的选择，当然也能针对特定的场景进行选择。比如颜色直方图经常使用，然而其忽略了目标区域的空间分布。局部特征是高效的，但是对遮挡和平面外旋转敏感，基于梯度的特征例如HOG可以描述目标的形状并且对光照有适应性，但它不能很好地处理遮挡和变形。区域协方差矩阵相对来说比较鲁棒，因为它们使用了较多的信息，但同时带来了较高的计算复杂度。目标跟踪领域的Martin大神在其之前的论文UPDT中对比分析了深度特征和浅层特征：深度特征：主要是CNN的高层激活，典型VGGNet的layer 5。优点是包含高层语义，对旋转和变形等外观变化具有不变性，何时何地都能找到目标，即鲁棒性很强；缺点是空间分辨率低，对平移和尺度都有不变性，无法精确定位目标，会造成目标漂移和跟踪失败，即准确性很差。浅层特征：主要是手工特征如RGB raw-pixel, HOG, CN，和CNN的低层激活，典型VGGNet的Layer 1。优点是主要包含纹理和颜色信息，空间分辨率高，适合高精度定位目标，即准确性很强；缺点是不变性很差，目标稍微形变就不认识了，尤其是旋转，一转就傻，即鲁棒性很差。不过近几年由于行人重识别Re-ID领域的迅速发展，深度特征逐渐从基于图像分类任务的backbone框架转变到了基于Re-ID的识别类任务，这样更容易得到端到端的特征训练框架。尤其是在DBT框架中检测质量参差不齐，而且遮挡严重，训练框架需要对前景目标具有一定的捕捉能力，一个精心设计的端到端训练框架尤为重要。3.2统计测量统计测量即相似性度量，这里我们分为单线索和多线索两种方法：单线索使用单线索进行外观建模时要么是将距离转换为相似性，要么直接计算相似性，比如说归一化互相关，这一点有点像相关滤波，即对两个相同大小的区域进行相关运算，亦或是利用巴氏距离计算两直方图的距离，并将其转化为相似性，或者加入高斯分布进行转化。多线索多线索，顾名思义就是结合不同的线索进行特征融合，有以下这些方法：Boosting：从特征池中选择一部分特征进行基于boosting的算法，例如对于颜色直方图、HOG、协方差矩阵，分别采用Adaboost、RealBoost和 HybirdBoost来区分不同目标各自的轨迹；Concatenation：直接进行特征拼接Summation：对不同的特征得到的相似度加权求和；Product：如果说不同特征间独立，则可以将相似度相乘；Cascading：使用不同的方法级联计算，例如考虑不同粗细粒度。4运动与假设模型为了简化现实生活中的多目标跟踪，我们可以引入一些假设来人为简化求解过程，分别有运动模型、交互模型、社会力模型、排斥模型和遮挡模型。4.1运动模型运动模型捕捉目标的动态行为，它估计目标在未来帧中的潜在位置，从而减少搜索空间。在大多数情况下，假设目标在现实中是平缓运动的，那么在图像空间也是如此。对于行人的运动，大致可分为线性和非线性两种运动：线性运动：线性运动模型是目前最主流的模型，即假设目标的运动属性平稳（速度，加速度，位置）；非线性运动：虽然线性运动模型比较常用，但由于存在它解决不了的问题，非线性运动模型随之诞生。它可以使tracklets间运动相似度计算得更加准确。当然，对于存在相机运动的场景也需要考虑相机运动，不过这就涉及到了一些相机几何知识了，我以后再开专题讲解。4.2交互模型交互模型也称为相互运动模型，它捕捉目标对其他目标的影响。在拥挤场景中，目标会从其他的目标和物体中感受到“力”。例如，当一个行人在街上行走时，他会调整他的速度、方向和目的地，以避免与其他人碰撞。另一个例子是当一群人穿过街道时，他们每个人都跟着别人，同时引导其他人。社会力模型也被称为群体模型。在这些模型中，每个目标都被认为依赖于其他目标和环境因素，这种信息可以缓解拥挤场景中跟踪性能的下降。在社会力模型中，目标会根据其他物体和环境的观察来确定它们自己的速度、加速度和目的地。更具体地说，在社会力模型中，目标行为可以由两方面建模而成：基于个体力和群体力。人群运动模型。通常这类模型适用于目标密度非常高的超密集场景，这时目标都比较小，那些外观、个人运动模式线索就会受到极大干扰，所以人群运动模式就相对比较适合，类似于元胞自动机或者有限元分析。该类模式又分结构化模式和非结构化模式，结构化模式主要得到集体的空间结构而非结构化模式主要得到不同个体运动的模式。通常来说，运动模式由不同方法学习得到，甚至考虑场景结构，然后运动模式可作为先验知识辅助目标跟踪。4.3遮挡模型一种比较流行的方法是将全局目标（类似一个跟踪框,bounding box）分割成几个部分，然后对每个部分计算相似度，具体来说就是当发生遮挡时，被遮挡的那些部分的相似度权重降低，而提高没被遮挡部分的相似性权重。至于如何进行分割，有将目标均匀地切分成一个个格子的，也有以某种形态例如人来切分目标的，还有由DPM检测器得到的部分。利用重构误差判断某个部分是否被遮挡，外观模型只根据可见部分进行更新，对于两个轨迹的相似度可利用各部分相似度加权求得。而缓冲模型则是在发生遮挡前记录目标状态并且将发生遮挡时的观测目标存入缓冲区中，当遮挡结束后，目标状态基于缓冲区的观测目标和之前记录的状态恢复出来。当发生遮挡时，保持最多15帧的trajectory，然后推断发生遮挡时潜在的轨迹。当目标重新出现时，重新进行跟踪并且ID也维持不变。当跟踪状态因为遮挡而产生歧义时观测模式就会启动，只要有足够的观测目标，就会产生假设来解释观测目标。以上就是”buffer-and-recover”策略。5概率预测型模型概率预测模型大多都是基于两部迭代的方式，假设目标的状态转移服从一阶马尔科夫模型：$$ \begin{array}{l} Predict:P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}}) = \int {P(\left. {{S_t}} \right|{S_{t - 1}})P(\left. {{S_{t - 1}}} \right|{O_{1\;:\;t - 1}})d{S_{t - 1}}} \\ Update:P(\left. {{S_t}} \right|{O_{1\;:\;t}}){\rm{ = }}\frac{{P(\left. {{O_t}} \right|{S_t})P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}})}}{{\int {P(\left. {{O_t}} \right|{S_t})P(\left. {{S_t}} \right|{O_{1\;:\;t - 1}})d{S_t}} }} \end{array} $$其中预测阶段是动态模型，更新阶段为观测模型。常见的概率预测型算法大致可分为以下几类：传统概率模型。以Kalman滤波器和粒子滤波为主，其中Kalman滤波器依旧在MOT领域活跃，比如我们熟知的Sort系列；联合概率数据关联。 联合概率数据互联JPDA是数据关联算法之一，它的基本思想是对应于观测数据落入跟踪门相交区域的情况，这些观测数据可能来源于多个目标。JPDA的目的在于计算观测数据与每一个目标之间的关联概率，且认为所有的有效回波都可能源于每个特定目标，只是它们源于不同目标的概率不同。JPDA算法的优点在于它不需要任何关于目标和杂波的先验信息，是在杂波环境中对多目标进行跟踪的较好方法之一。然而当目标和量测数目增多时，JPDA算法的计算量将出现组合爆炸现象，从而造成计算复杂。算法分成联合事件生成和关联概率计算两部分。多假设跟踪。多假设跟踪MHT是数据关联另一种算法。它的基本思想是：与JPDA不同的是，MHT算法保留真实目标的所有假设，并让其继续传递，从后续的观测数据中来消除当前扫描周期的不确定性。在理想条件下，MHT是处理数据关联的最优算法，它能检测出目标的终结和新目标的生成。但是当杂波密度增大时，计算复杂度成指数增长，在实际应用中，要想实现目标与测量的配对也是比较困难的。在ICCV2015和CVPR2017都有相关的工作。随机有限集。国内在这一块的研究都处于起步阶段，其优势在于无需考虑数据关联，不用先假设各种组合关系，不用担心出现“组合爆炸”情况，直接估测目标的个数和状态，这样就可以不需要目标检测的结果了，当然有了更好。即RFS直接将目标整体看做一个目标，因此多目标跟踪就变成了一个单目标跟踪问题，计算复杂度也是随着目标数量线性增长的，不过其理论比较挑战我的理论基础，有一本专门介绍概率型单/多目标跟踪的书籍《advances in statistical multisource-multitarget information fusion》，1000来页，1页一块钱，不过我最近发现了中文版《多源多目标统计信息融合》，才100多¥，数学hold住的可以看看。目前这块都需要基于一定的分布建模，典型的有PHD滤波和伯努利滤波。6数据关联大多数DBT框架都避免不了数据关联过程，数据关联分为以下几类：偶图匹配。即在相邻两帧之间对跟踪轨迹和观测进行数据关联，常用的有IOU Matching(贪婪)、匈牙利算法/KM算法，也有少数人用MinCostFlow；图论。这类算法大多用于离线跟踪的建模，比如MinCostFlow、最大流/最小割、超图等；马尔可夫随机场和条件随机场等。7深度学习模型近两年，深度学习算法开始在MOT领域发展，一般分为这么几类：以Re-ID为主的表观特征提取网络，如《Aggregate Tracklet Appearance Features for Multi-Object Tracking》；基于单目标跟踪领域中成熟的Siam类框架构建的多目标跟踪框架，如《Multi-object tracking with multiple cues and switcher-aware classification》；联合目标检测框架和单目标跟踪框架的多任务框架，如《Detect to track and track to detect》；端到端的数据关联类算法，如《DeepMOT: A Differentiable Framework for Training Multiple Object Trackers》；联合运动、表观和数据观联的集成框架，如《FAMNet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking》；基于LSTM类算法实现的运动估计、表观特征选择和融合等等算法。8MOTChallenge评价体系由于MOTChallenge是最主流的MOT数据集，所以我这里就以它为例进行介绍。其包含有MOT15~17三个数据集，其中MOT15提供了3D的坐标信息，包含5500帧训练集和5783帧测试集，提供了基于ACF检测器的观测结果。而MOT16和MOT17则包含5316帧训练集和5919帧测试集，其中MOT16仅提供了基于DPM检测器的观测，而MOT17则提供了SDP、FasterRcnn、DPM三种检测结果。MOT提供的目标检测结果标注格式为： frame_id 、 target_id、 bb_left 、bb_top、 bb_width、bb_height 、confidence 、x 、y 、z 。即视频帧序号、目标编号（由于暂时未定，所以均为-1）、目标框左上角坐标和宽高、检测置信度（不一定是0~1）、三维坐标（2D数据集中默认为-1）.那么我们所需要提供的跟踪结果格式也是同上面一致的，不过需要我们填写对应的target_id和对应的目标框信息，而confidence,x,y,z均任意，保持默认即可。相应地，官方所采用的跟踪groudtruth格式则为： frame_id、 target_id 、bb_left、bb_top、bb_width bb_height 、is_active、label_id、visibility_ratio 。其中is_active代表此目标是否考虑，label_id表示该目标所属类别，visibility_ratio表示目标的可视程度，目标类别分类如下：类别标签类别标签Pedestrian1Static Person7Person on vehicle2Distractor8Car3Occluder9Bicycle4Occluder on the ground10Motorbike5Occluder full11Non motorized vehicle6Reflection12最后，数据集还提供了各个视频的视频信息seqinfo.ini，主要包括视频名称、视频集路径、帧率fps、视频长度、图像宽高、图像格式等。根据MOT官方工具箱中的评价工具，可分析如下的评价规则：Step1 数据清洗对于跟踪结果进行简单的格式转换，这个主要是方便计算，意义不大，其中根据官方提供的跟踪groundtruth，只保留is_active = 1的目标（根据观察，只考虑了类别为1，即处于运动状态的无遮挡的行人）。另外将groudtruth中完全没有跟踪结果的目标清除，并保持groudtruth中的视频帧序号与视频帧数一一对应。为了统一跟踪结果和groudtruth的目标ID，首先建立目标的映射表，即将跟踪结果中离散的目标ID按照从1开始的数字ID替代。Step2 数据匹配将跟踪结果和groundtruth中同属一帧的目标取出来，并计算两两之间的IOU，并将其转换为cost矩阵（可理解为距离矩阵，假定Thresh=0.5）。利用cost矩阵，通过匈牙利算法（Hungarian）建立匹配矩阵，从而将跟踪结果中的目标和groundtruth中的目标一一对应起来。Step3 数据分析对视频每一帧进行分析，利用每一帧中的跟踪目标和groudtruth目标之间的匹配关系，可作出以下几个设定：对于当前帧检测到但未匹配的目标轨迹记作falsepositive；对于当前帧groudtruth中未匹配的目标轨迹记作missed；对于groudtruth中的某一目标，如果与之匹配的跟踪目标ID前后不一致，则记作IDswitch；对于已匹配的轨迹记作covered，总轨迹为gt。其中，对于匹配和未匹配到的目标都有各自的评价依据，评价指标很多，这里就不细讲了，网上都有。利用cost矩阵，通过匈牙利算法（Hungarian）建立匹配矩阵，从而将跟踪结果中的目标和groundtruth中的目标一一对应起来。参考资源[1] LUO W, XING J, MILAN A, et al. Multiple object tracking: A literature review[J]. arXiv preprint arXiv:1409.7618, 2014.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>目标跟踪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多目标跟踪MOT相关论文和代码资源列表]]></title>
    <url>%2F2020%2F02%2F23%2F%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AAMOT%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E5%92%8C%E4%BB%A3%E7%A0%81%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言这段时间在整理毕设，所以这里结合了SpyderXu分享的内容把多目标跟踪相关的文献资源共享一下，由于文章很多，所以我这里只整理3年以内的，对于年限久远的，这里只取提供了代码的和比较经典的。并且尽可能注释了相关算法在MOT数据集上的名称。各自算法的性能比较可以看论文以及MOT官网。在线跟踪（Online)NameSourcePublicationNotesAdopting Tubes to Track Multi-Object in a One-Step Training Model[pdf] [code]CVPR2020TubeTKJoint Detection and Multi-Object Tracking with Graph Neural Networks[pdf]arxiv(2020)JDMOT_GNNGraph Networks for Multiple Object Tracking[pdf] [code]WACV2020GNMOTDeep association: End-to-end graph-based learning for multiple object tracking with conv-graph neural network[pdf]ICMR2019DANSQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking[pdf]arxiv(2020)SQEAutoregressive Trajectory Inpainting and Scoring for Tracking[pdf]CVPR2020ArTISTMultiple Object Tracking with Siamese Track-RCNN[pdf]arxiv(2020)Siamese Track-RCNNOnline Single Stage Joint Detection and Tracking[pdf]CVPR2020RetinaTrackA Simple Baseline for Multi-Object Tracking[pdf][code]arXiv(2019)FairMOTTracking Objects as Points[pdf] [code]arXiv(2019)CenterTrackRefinements in Motion and Appearance for Online Multi-Object Tracking[pdf] [code]arXiv(2019)MIFTMultiple Object Tracking by Flowing and Fusing[pdf]arXiv(2019)FFTA Unified Object Motion and Affinity Model for Online Multi-Object Tracking[pdf][code]CVPR2020UMADeepMOT:A Differentiable Framework for Training Multiple Object Trackers[pdf] [code]CVPR2020DeepMOTOnline multiple pedestrian tracking using deep temporal appearance matching association[pdf] [code]arXiv(2019)DD_TAMA19Spatial-temporal relation networks for multi-object tracking[pdf]ICCV2019STRNTowards Real-Time Multi-Object Tracking[pdf] [code]arXiv(2019)JDE(private)Multi-object tracking with multiple cues and switcher-aware classification[pdf]arXiv(2019)LSSTFAMNet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking[pdf]ICCV2019FAMNetOnline multi-object tracking with instance-aware tracker and dynamic model refreshment[pdf]WACV2019KCFTracking without bells and whistles[pdf] [code]ICCV2019TracktorMOTS: Multi-Object Tracking and Segmentation[pdf] [code]CVPR2019Track R-CNNEliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking[pdf] [code]CVPR2019SAS_MOT17Deep affinity network for multiple object tracking[pdf] [code]PAMI(2019)DANRecurrent autoregressive networks for online multi-object tracking[pdf]WACV2018RANReal-time multiple people tracking with deeply learned candidate selection and person re-identification[pdf] [code]ICME2018MOTDTOnline multi-object tracking with dual matching attention networks[pdf] [code]ECCV2018DMANExtending IOU Based Multi-Object Tracking by Visual Information[pdf] [code]AVSS2018V-IOUOnline Multi-target Tracking using Recurrent Neural Networks[pdf] [code]AAAI2017MOT-RNNDetect to Track and Track to Detect[pdf] [code]ICCV2017D&amp;T(private)Online multi-object tracking using CNN-based single object tracker with spatial-temporal attention mechanism[pdf]ICCV2017STAMTracking the untrackable: Learning to track multiple cues with long-term dependencies[pdf]ICCV2017AMIRSimple online and realtime tracking with a deep association metric[pdf] [code]ICIP2017DeepSortHigh-speed tracking-by-detection without using image information[pdf] [code]AVSS2017IOU TrackerSimple online and realtime tracking[pdf] [code]ICIP2016SortTemporal dynamic appearance modeling for online multi-person tracking[pdf]CVIU(2016)TDAMOnline multi-object tracking via structural constraint event aggregation[pdf]CVPR2016SCEAOnline Multi-Object Tracking Via Robust Collaborative Model and Sample Selection[pdf] [code]CVIU2016RCMSSLearning to Track: Online Multi-Object Tracking by Decision Making[pdf] [code]ICCV2015MDPLearning to Divide and Conquer for Online Multi-Target Tracking[pdf] [code]ICCV2015LDCTRobust online multi-object tracking based on tracklet confidence and online discriminative appearance learning[pdf] [code]CVPR2014CMOTThe Way They Move: Tracking Targets with Similar Appearance[pdf] [code]ICCV2013SMOTOnline Multi-Person Tracking by Tracker Hierarchy[pdf] [code]AVSS2012OMPTTH离线跟踪（Batch)NameSourcePublicationNotesLifted Disjoint Paths with Application in Multiple Object Tracking[pdf] [code]ICML2020Lif_TLearning non-uniform hypergraph for multi-object tracking[pdf]AAAI2019NTLearning a Neural Solver for Multiple Object Tracking[pdf] [code]CVPR2020MPNTrackerDeep learning of graph matching[pdf]CVPR2018深度图匹配muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking[pdf] [code]NIPS(2019)muSSPExploit the connectivity: Multi-object tracking with trackletnet[pdf] [code]ACM mm 2019TNT(eTC)Multiple people tracking using body and joint detections[pdf]CVPRW2019JBNOTAggregate Tracklet Appearance Features for Multi-Object Tracking[pdf]SPL(2019)NOTACustomized multi-person tracker[pdf]ACCV2018HCCMulti-object tracking with neural gating using bilinear lstm[pdf]ECCV2018MHT_bLSTMTrajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking[pdf]ICME2018GCREMultiple People Tracking with Lifted Multicut and Person Re-identification[pdf]CVPR2017LMPDeep network flow for multi-object tracking[pdf]CVPR2017-Non-markovian globally consistent multi-object tracking[pdf] [code]ICCV2017-Multi-Object Tracking with Quadruplet Convolutional Neural Networks[pdf]CVPR2017Quad-CNNEnhancing detection model for multiple hypothesis tracking[pdf]CVPRW2017EDMTPOI: Multiple Object Tracking with High Performance Detection and Appearance Feature[pdf]ECCV2016KNDTMultiple hypothesis tracking revisited[pdf] [code]ICCV2015MHT-DAMNear-Online Multi-target Tracking with Aggregated Local Flow Descriptor[pdf]ICCV2015NOMTOn Pairwise Costs for Network Flow Multi-Object Tracking[pdf] [code]CVPR2015-Multiple Target Tracking Based on Undirected Hierarchical Relation Hypergraph[pdf] [code]CVPR2014H2TContinuous Energy Minimization for Multi-Target Tracking[pdf] [code]CVPR2014CEMGMCP-Tracker: Global Multi-object Tracking Using Generalized Minimum Clique Graphs[pdf] [code]ECCV2012GMCPMultiple Object Tracking using K-Shortest Paths Optimization[pdf] [code]PAMI2011KSPGlobal data association for multi-object tracking using network flows[pdf] [code]CVPR2008-跨摄像头跟踪（MTMC）NameSourcePublicationNotesLocality Aware Appearance Metric for Multi-Target Multi-Camera Tracking[pdf] codeCVPR2019 WorkshopLAAMCityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification[pdf]CVPR2019CityFlowFeatures for multi-target multi-camera tracking and re-identification[pdf] [code]CVPR2018DeepCC(MTMC)Rolling Shutter and Radial Distortion Are Features for High Frame Rate Multi-Camera Tracking[pdf]CVPR2018-Towards a Principled Integration of Multi-Camera Re-Identification andTracking through Optimal Bayes Filters[pdf] [code]CVPR2017towards-reid-tracking3D&amp;多模态跟踪NameSourcePublicationNotesJoint 3D Tracking and Forecasting with Graph Neural Network and Diversity Sampling[pdf] [code]arxivGNNTrkForecastGraph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning[pdf] [code]CVPR2020GNN3DMOTRobust Multi-Modality Multi-Object Tracking[pdf] [code]ICCV2019mmMOTA baseline for 3D Multi-Object Tracking[pdf] [code]arXiv-综述Multiple Object Tracking: A Literature ReviewMachine Learning Methods for Solving Assignment Problems in Multi-Target TrackingDeep Learning in Video Multi-Object Tracking_ A SurveyGlobally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects数据集MOT：包含2D MOT2015、3D MOT2015、MOT16、MOT17和MOT17Det等多个子数据集，提供了ACF、DPM、Faster RCNN、SDP等多个检测器输入。包含不同的相机视角、相机运动、场景和时间变化以及密集场景。KITTI：提供了汽车和行人的标注，场景较稀疏。TUD Stadtmitte：包含3D人体姿态识别、多视角行人检测和朝向检测、以及行人跟踪的标注，相机视角很低，数据集不大。ETHZ：由手机拍摄的多人跟踪数据集，包含三个场景。EPFL：多摄像头采集的行人检测和跟踪数据集，每隔摄像头离地2米，实验人员就是一个实验室的，分为实验室、校园、平台、通道、篮球场这5个场景，每个场景下都有多个摄像头，每个摄像头拍摄2分钟左右。KIT AIS：空中拍摄的，只有行人的头PETS：比较早期的视频，有各式各样的行人运动。DukeMTMC：多摄像头多行人跟踪。MOTS：多目标跟踪与分割。评价体系ClearMOTIDF1Code: python、matlab]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>目标跟踪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的最优化方法]]></title>
    <url>%2F2019%2F02%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言无论是机器学习还是深度学习都需要对目标函数进行优化求解，那么这里我先抛开最底层的数学证明，重点介绍几类算法的收敛条件问题，以及梯度相关算法的原理。毕竟不是教学性质，所以我只讲要点。1.优化理论1.1凸优化设集合$S \subset {R^n}$，若S中的任意两点连线仍属于S，则称S为凸集，即$$ {x_1} + \lambda \left( {{x_2} - {x_1}} \right) \in S $$在此基础上，凸函数的定义就是,设集合$S \subset {R^n}$,f是定义在S上的实函数，若对任意x1,x2属于S，$\lambda \in \left( {0,1} \right)$，都有$$ f\left( {{x_1} + \lambda \left( {{x_2} - {x_1}} \right)} \right) \le f\left( {{x_1} + \lambda \left[ {f\left( {{x_2}} \right) - f\left( {{x_1}} \right)} \right]} \right) $$可以理解为，函数上任意两点的连线一定在两点之间弧的上方。基于这个定义实际上就可以推理出，凸函数的局部极小值点就是全局极小点。那么对于凸规划，实质就是求凸函数在凸集上的极小点，这里就不给出正式的形式说明了。顺便提一下算法难度评级：NP问题：可以在多项式时间内被验证的问题。或者说，可以在非确定性多项式时间内被解决的问题。P问题：可以在多项式时间内被解决的问题。NP-Hard问题：不确定是否可以在多项式时间内被验证。NP-Complete问题：如果一个问题已经被证明是一个NP-Hard问题，并且可以证明该问题是一个NP问题，那么该问题是NPC问题。1.2最优性条件对于凸优化来说，最优性条件一般有KT、KKT、F-J等。首先我们介绍下泰勒展开，由此推导出函数的极值条件以及函数在某点出的下降方向：$$ \begin{array}{l} \because f\left( {\bar x + \lambda \vec d} \right) = f\left( {\bar x} \right) + \lambda \nabla f{\left( {\bar x} \right)^T}\vec d + o\left( {\left\| {\lambda \vec d} \right\|} \right),\lambda \in \left( {0,\delta } \right)\\ \therefore \lambda \to 0,\nabla f{\left( {\bar x} \right)^T}\vec d \le 0 \Rightarrow f\left( {\bar x + \lambda \vec d} \right) \le f\left( {\bar x} \right) \end{array} $$上面给出的正是极值条件的必要条件，从中可以看出，如果x0是极小点，那么一定不存在满足上述条件的$\lambda$，进而可以推出对于无约束极值问题来说负梯度方向是下降方向。对于有约束极值问题，有下面的一般问题形式：$$ \begin{array}{l} \min \;\;\;f(x)\\ s.t.\;\;{g_i}\left( x \right) = 0\\ \;\;\;\;\;\;{h_j}\left( x \right) \le 0 \end{array} $$那么有存在有Fritz John条件，还有KKT条件来作为极值点的必要条件，下面只给出KKT条件形式：$$ \left\{ \begin{array}{l} \nabla f + \sum\limits_i^n {{\lambda _i}\nabla {g_i}} + \sum\limits_j^m {{\mu _j}\nabla {h_j}} = 0\\ {g_i} = 0\\ {h_j} \le 0\\ {\mu _j} \ge 0\\ {\mu _j}{h_j} = 0\; \end{array} \right. $$最后一个条件一般被称作互补松弛条件，关于这个理论，利用对偶问题会比较好解释，这里参照知乎网友的评论如下图：最后要记住，当f,h都是凸函数，且可行解满足KKT条件，则其一定为最优解。2.拉格朗日乘子法基于KKT条件，可以引出广义的Lagrange函数：$$ L\left( {x,w,v} \right) = f\left( x \right) + \sum\limits_{i = 1}^m {{w_i}{g_i}\left( x \right)} + \sum\limits_{j = 1}^n {{v_j}{h_j}\left( x \right)} $$其中w,v就是拉格朗日乘子，可以理解为将KKT条件中的微分算子移到一侧，即：$$ \frac{{\partial L}}{{\partial f}},\frac{{\partial L}}{{\partial {g_i}}},\frac{{\partial L}}{{\partial {h_i}}} = 0 $$3.梯度下降法第一章中介绍了，最速下降方向是凸函数f(x)在x0处的负梯度方向，也就是说，最优解的位置一定在x0的负梯度方向，这时候我们只需要利用更新公式即可迭代求解逼近最优解：$$ \begin{array}{l} {x^{k + 1}} = {x^k} + {\lambda _k}{d^k}\\ {d^k} = - \nabla f\left( {{x^k}} \right) \end{array} $$其中d平行于负梯度方向，影响归一化项可以用学习率$\lambda$来代替，这类迭代算法的特性就是要注意两个点：学习率：当学习率过大时，更新震荡会变大，可能不容易接近最优解；当学习率过小时，更新太慢；终止条件：一般终止条件有两种，一种是当x或者f(x)的值变化幅度很小的时候停止，另一种是设定迭代次数。而实际上在问题规模较小时，学习率可以利用简单的搜索算法进行求解，将子问题转化为关于学习率的搜索问题，如：$$ \min \;f\left( {{x^k} + \lambda {d^k}} \right) $$但是面对深度学习这种参数量大，样本多的问题，这种搜索会极大降低算法效率。另外由于一维搜索的存在：$$ \begin{array}{l} \varphi \left( x \right) = f\left( {{x^k} + \lambda {d^k}} \right),{d^k} = - \nabla f\left( {{x^k}} \right)\\ \Rightarrow \varphi '\left( \lambda \right) = \nabla f{\left( {{x^k} + \lambda {d^k}} \right)^T}{d^k} = 0\\ \Rightarrow - \nabla f\left( {{x^{k + 1}}} \right)\nabla f\left( {{x^k}} \right) = 0 \end{array} $$所以相邻两次的迭代方向是垂直的，从而也就导致收敛过程存在锯齿现象。4.牛顿法4.1牛顿法如果说梯度下降法是根据一阶泰勒展开得到的，那么我们不妨看看二阶泰勒展开的形式：$$ \begin{array}{l} f\left( x \right) \approx \varphi \left( x \right) = f\left( {{x^k}} \right) + \nabla f{\left( {{x^k}} \right)^T}\left( {x - {x^k}} \right) + {\left( {x - {x^k}} \right)^T}\frac{{{\nabla ^2}f\left( {{x^k}} \right)}}{2}\left( {x - {x^k}} \right)\\ \Rightarrow \varphi '\left( x \right) = \nabla f\left( {{x^k}} \right) + {\nabla ^2}f\left( {{x^k}} \right)\left( {x - {x^k}} \right) = 0\\ \Rightarrow x = {x^k} - {\nabla ^2}f{\left( {{x^k}} \right)^{ - 1}}\nabla f\left( {{x^k}} \right) \end{array} $$可以看到，牛顿法的迭代更像是曲面的拟合，其对于二次凸函数，可以在有限迭代次数收敛，其中步长中的二阶导数指的是Hessian矩阵：为了保证最速下降方向，所以Hessian矩阵必须正定，但是当函数不是二次型凸函数时，如果当前点距离极值点很远，则有可能导致Hessian矩阵不正定，那么可能迭代过程目标值会增大。另外，当变量非常多的时候，求解Hessian矩阵的复杂度也很高。4.2阻尼牛顿法对于牛顿法的缺点，阻尼牛顿法做了两点改进，一个是一维搜索，这个就不再赘述，主要用来保证目标值在迭代过程中有所下降，另一个就是构造矩阵解决Hessian矩阵不正定的问题：$$ {G_k} = {\nabla ^2}f\left( {{x^k}} \right) + {\varepsilon _k}IQ $$由此解决了牛顿法收敛方向的问题，但是求解速度依然没有解决。4.3拟牛顿法常见的拟牛顿法包括拟牛顿法、BFGS、L-BFGS法等。$$ \begin{array}{l} f\left( x \right) = f\left( {{x^k}} \right) + \nabla f{\left( {{x^k}} \right)^T}\left( {x - {x^k}} \right) + {\left( {x - {x^k}} \right)^T}\frac{{{\nabla ^2}f\left( {{x^k}} \right)}}{2}\left( {x - {x^k}} \right)\\ \Rightarrow \nabla f\left( x \right) - \nabla f\left( {{x^k}} \right) = {\nabla ^2}f\left( {{x^k}} \right)\left( {x - {x^k}} \right) = 0\\ \Rightarrow {x^{k + 1}} = {x^k} - {\nabla ^2}f{\left( {{x^k}} \right)^{ - 1}}\left( {\nabla f\left( {{x^{k + 1}}} \right) - \nabla f\left( {{x^k}} \right)} \right) \end{array} $$可以看到当前步的Hessian矩阵本身跟其他的四个变量相关，因此拟牛顿法要做的就是利用迭代的方式更新Hessian矩阵:$$ \begin{array}{l} {s_k} = {x^{k + 1}} - {x^k},{y_k} = \nabla f\left( {{x^{k + 1}}} \right) - \nabla f\left( {{x^k}} \right),{D_k} = {\nabla ^2}f{\left( {{x^k}} \right)^{ - 1}}\\ BFGS:{D_{k + 1}} = {D_k} + \left( {\frac{1}{{s_k^T{y_k}}} + \frac{{y_k^T{D_k}{y_k}}}{{{{\left( {s_k^T{y_k}} \right)}^2}}}} \right){s_k}s_k^T - \frac{1}{{s_k^T{y_k}}}\left( {{D_y}{y_k}s_k^T + {s_k}y_k^T{D_k}} \right),{D_0} = I \end{array} $$可以看到Hessian矩阵的求解只依赖于上一步的矩阵和s,y两个一维向量。而为了解决Hessian矩阵的存储问题，工业界又提出了L-BFGS算法，即保存近m步的s和y的值，然后在计算当前步的Hessian矩阵时，循环迭代求解，不需要保存Hessian矩阵。5.共轭梯度下降法共轭梯度下降法最初是用来求解二次凸函数的最优值的：$$ \begin{array}{l} \min \;f{\rm{ = }}\frac{1}{2}{x^T}Ax + {b^T}x + c\\ {d_1} = - \nabla f\left( {{x^1}} \right) = - {g_1},{\lambda _1} = \mathop {\min }\limits_\lambda \;f\\ {x^{k + 1}} = {x^k} + {\lambda _k}{d_k} \Rightarrow {g_{k + 1}} = \nabla f\left( {{x^{k + 1}}} \right)\\ {\lambda _k} = - \frac{{g_k^T{d_k}}}{{d_k^TA{d_k}}},{\beta _k} = - \frac{{d_k^TA{g_{k + 1}}}}{{d_k^TA{d_k}}}\\ {d_{k + 1}} = - {g_{k + 1}} + {\beta _k}{d_k} \end{array} $$6.最小二乘法最小二乘法或者岭回归算法一般是用来做曲线拟合问题的，比如：$$ \min \;f = \left\| {Ax - b} \right\|_2^2 = {\left( {Ax - b} \right)^T}\left( {Ax - b} \right) $$很显然这个问题很适合共轭梯度下降法，不过，我们利用拉格朗日乘子法可以很轻易求解：$$ \begin{array}{l} f = {\left( {Ax - b} \right)^T}\left( {Ax - b} \right)\\ \left( 1 \right)df = tr\left( {df} \right)\\ = tr\left( {d{{\left( {Ax - b} \right)}^T}\left( {Ax - b} \right) + {{\left( {Ax - b} \right)}^T}d\left( {Ax - b} \right)} \right)\\ = tr\left( {{{\left( {Adx} \right)}^T}\left( {Ax - b} \right) + {{\left( {Ax - b} \right)}^T}Adx} \right)\\ = tr\left( {{{\left( {Ax - b} \right)}^T}Adx + {{\left( {Ax - b} \right)}^T}Adx} \right)\\ = tr\left( {2{{\left( {Ax - b} \right)}^T}Adx} \right)\\ \therefore \frac{{\partial f}}{{\partial x}} = 2{A^T}\left( {Ax - b} \right) = 0\\ \therefore x = {\left( {{A^T}A} \right)^{ - 1}}{A^T}b\\ \left( 2 \right)df = tr\left( {df} \right)\\ = tr\left( {d{{\left( {Ax - b} \right)}^T}\left( {Ax - b} \right) + {{\left( {Ax - b} \right)}^T}d\left( {Ax - b} \right)} \right)\\ = tr\left( {{{\left( {dAx} \right)}^T}\left( {Ax - b} \right) + {{\left( {Ax - b} \right)}^T}dAx} \right)\\ = tr\left( {{{\left( {Ax - b} \right)}^T}dAx + x{{\left( {Ax - b} \right)}^T}dA} \right)\\ = tr\left( {2x{{\left( {Ax - b} \right)}^T}dA} \right)\\ \therefore \frac{{\partial f}}{{\partial A}} = 2\left( {Ax - b} \right){x^T} = 0\\ \therefore A = b{x^T}{\left( {x{x^T}} \right)^{ - 1}} \end{array} $$参考资料https://www.zhihu.com/question/27471863/answer/123244103]]></content>
      <categories>
        <category>机器学习</category>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据结构]]></title>
    <url>%2F2019%2F02%2F20%2Fpython%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[前言之前整理了C++的数据结构，但是对于python的数据结构的底层还是不大了解，这里对python中的set、list、dict、tuple等基础数据结构的原理和底层结构进行简要说明。1.set跟C++中的一样，python中的set也是集合的结构，，用到了散列表结构，具体结构见下文，用法类似：123456x = set('runoob')y = set('google')x, y #(set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) # 重复的被删除x &amp; y # 交集，set(['o'])x | y # 并集，set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u'])x - y # 差集，set(['r', 'b', 'u', 'n'])2.list虽说叫作list，但是python中的list并不是链表，而是长度可变的动态数组，可以理解为vector：可以看到，list存储的是元素的引用，在分配空间快满的时候，会自动申请两倍空间，其效率比传统的链表低很多。另外，当执行pop等删除操作之后，list会动态释放部分内存。3.tupletuple和list很像，唯一的不同在于tuple是只读属性，但是要注意tuple的实现跟list几乎是一样，所以tuple中每个位置指向的是一个引用，引用不能变，但是引用所指向的数据可以变，比如：123a=(1, 2, [1, 3])a[2][1] = 0 #a=(1,2, [1, 0])a=a+(3,4) # a=(1,2, [1, 0], 3, 4),这里a这个对象整体变了4.dictdict和set一样，都是用了哈希表来构建的，利用开放寻址法来解决哈希冲突：如图，在存dict的时候，首先会根据dict的key进行hash映射到对应的表元，然后再对应的表元中开辟内存，存入数据，当如果存在不同的两个key的hash结果相同的时候，就会使用散列值的另一部分来定位散列表中的另一行。在dict中查找指定的key时，会先计算key的散列值，然后使用散列值的一部分来定位表元，如果没有找到相应的表元，则说明dict中不存在对应的key跑出KeyError异常。如果找到表元之后，会判断表元中的key是否和要查找的key相等，相等就返回对应值，如果不相等则使用其对应的散列值的其他部分来定位散列表中的其他行。（这是因为不同的对象通过的散列值有一定的概率相同，这也是为什么在存放dict时开辟内存时候需要有1/3的空地址出来，这样如果有相同的hash值就会有空的地址来存放随数据增加，还会继续开辟新的内存，以确保空内存时刻在1/3左右）补充：①python中set的值得存储方式也是和dict一样。所以dict的key和set的值都必须是可hash（不可修改的）的对象。②dict的花销大，因为会空出进1/3的内存主来，但是查询速度快。参考资料https://www.cnblogs.com/yc3110/p/10451431.html]]></content>
      <categories>
        <category>python</category>
        <category>基础结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++之数据结构]]></title>
    <url>%2F2019%2F02%2F18%2FC%2B%2B%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[前言C++的基础数据结构基本都存在于STL的头文件中，包括栈、队列、堆、哈希表、数组等等结构，这里我要对这些结构的基本使用做一个总结，具体包含元素的插入、查找、删除、遍历、取出等基本操作。1.简单容器（pair/tuple)pair和tuple属于简单的一维向量容器，二者都属于静态容器，即必须提前申明元素个数和元素类型，其中pair中只能包含两个元素，tuple个数不限。下面就二者的创建、元素访问、元素修改进行对比说明：pairtuple形式, eg:, eg:头文件&lt; utility &gt;&lt; tuple &gt;直接创建std::paira = {1, 2}std::tuplefoo (10,’x’);函数创建std::paira =std::make_pair(1,2)auto bar = std::make_tuple (“test”, 3.1, 14, ‘y’);访问a.first和a.secondstd::get&lt;位置&gt;(bar)修改a.first=xstd::get&lt;位置&gt;(bar)=x交换两容器swap(a,b)或者a.swap(b)swap(a,b)或者a.swap(b)2.多功能容器(array/vector)2.1 arrayarray是对数组的一种封装形式，比如：int a[] = {1,2,3,4} &lt;=&gt; std::arraya = {1,2,3,4}，因此其有以下几个特点：静态数组：默认创建在栈上，元素类型和元素个数固定，要注意的是跟tuple不同，array的元素类型只支持基础数据类型，如：int、double、char…；数组创建与初始化：123std::array&lt;int,5&gt; x = &#123;10, 20, 30, 40, 50&#125;;std::array&lt;int,5&gt; y = &#123;1,2&#125;;//剩余默认填充为0std::array&lt;char,5&gt; z = &#123;&#125;;//默认初始化为''数组属性获取：12a.size()//元素数量a.empty()//数组是否为空，eg:std::array&lt;int,0&gt; a;元素直接访问：12345a[0];//返回第一个元素a.at(0);//同上a.front();//返回第一个元素a.back();//返回最后一个元素a.data();//返回数组头指针迭代器访问：12345678910111213141516std::array&lt;int,5&gt; x = &#123;10, 20, 30, 40, 50&#125;;std::array&lt;int,5&gt;::iterator it;//auto it = x.begin();for ( it = x.begin(); it != x.end(); ++it )//前向迭代 std::cout &lt;&lt; ' ' &lt;&lt; *it;//*iterator返回迭代器指向的元素for ( auto it = x.rbegin(); it != x.rend(); ++it )//反向迭代 std::cout &lt;&lt; ' ' &lt;&lt; *it;//*iterator返回迭代器指向的元素for ( int&amp; p: x)//循环访问 std::cout &lt;&lt; ' ' &lt;&lt; p;//*iterator返回迭代器指向的元素//防止修改元素for ( it = x.cbegin(); it != x.cend(); ++it )//前向迭代 std::cout &lt;&lt; ' ' &lt;&lt; *it;//*iterator返回迭代器指向的元素for ( auto it = x.crbegin(); it != x.crend(); ++it )//反向迭代 std::cout &lt;&lt; ' ' &lt;&lt; *it;//*iterator返回迭代器指向的元素元素修改：元素的修改可直接利用元素的访问进行直接修改，并且array提供了一键填充的函数fill()以及容器交换的功能swap()。与此同时，array还提供了元素级的逻辑比较运算符,比如：&gt;,==,&lt;等等，都是要求所有元素都满足逻辑操作才返回true。2.2 vectorvector是一个多功能的容器，其被创建于堆上，支持所有元素属性，其最大的特点就是动态容器特性，即可以动态调整内存，不过一般每次扩大内存都会比之前大一倍，然后将之前的元素复制到新内存，最后释放原内存。上述所有简单容器的功能vector都具备，用法也类似，所以我这里只介绍其不同的地方。其包含于&lt;vector&gt;头文件之中。容器创建与初始化：1std::vector&lt;int&gt; foo (3,0);//创建一个有3个int元素的容器，并初始化为0容器属性：1234567a.size();//容器大小a.empty();//容器是否为空a.resize(5);//调整容器大小为5，不足的初始化为0，多余的去掉，不影响已分配的内存大小a.resize(5,10);//调整容器大小为5，不足的初始化为10，不影响已分配的内存大小a.capacity();//容器当前已分配的大小(（)单位是元素)a.reserve(100);//强制调整容器已分配内存元素数量至100a.shrink_to_fit();//强制调整a.capacity()=a.size()元素调整：123456789101112a.assign(InputIterator first, InputIterator last);//将另一个容器的first~last位置处的元素赋值给aa.assign(size_type n, const value_type&amp; val);//赋值n个val值a.erase(a.begin()+5);//移除a[6]a.erase(a.begin(),myvector.begin()+3);//移除a[0]~a[3]a.emplace(a.begin()+1, 100);//在a[0]和a[1]之间插入元素值100，返回当前迭代器a.begin+1a.emplace_back(100);//在a末尾添加100a.insert(a.begin()+1,100)//在a[1]之前插入100a.insert(a.begin(),2，100)//在a[0]处插入两个100a.insert (a.begin(), src, dst);//将src地址/迭代器到dst的地址/迭代器指向的元素插入到a.begin()处a.push_back (100);//在末尾添加100a.pop_back();//移除末尾元素a.clear();清空容器3.队列(queue/dequeue)队列，顾名思义就是遵循先进先出(First in First out, FIFO)原则的线性表结构，在c++的stl标准中也添加了队列结构。其中queue位于头文件&lt;queue&gt;中，用法和原理如下图所示：注：初始化不需要定义元素个数，只需要定义元素类型，如：queue&lt;int&gt;q;当然，随之元素不断的添加，queue很可能会发生内存冲突，因此可以利用queue(int,other)来初始化，其中other代表其它数据类型，一旦被定义，则第一个元素int失效，比如other = std::list&lt;int&gt;代表queue会在遵循FIFO的前提下采用链表存储队列。当然，还可以为其他类型，比如下面要提到的deque：deque是双端队列，顾名思义其可以在队列的头部以及尾部插入和删除数据，其跟vector的用法很像，不过不同的是deque的动态内存体现在由多个连续的缓冲区组成，并由一块连续内存进行管理：可以看到，map中包含每个缓冲区的节点，每个节点内含有当前缓冲区的中当前元素的指针，当前缓冲区头指针，当前缓冲区尾指针。deque包含于&lt;deque&gt;头文件中，其绝大多数功能和vector类似，包括元素访问、元素修改等等，比较特殊的就是其添加和去除元素是在队列两端执行的，如：pop_back(),pop_front(),push_back(),push_front()。其初始化方式4.链表(forawrd_list/list)链表是一类典型的非连续内存线性表，其包括单向链表(forward_list)和双向列表(list)，分别位于同名头文件中。既然是线性表，其自然就具有基本功能，如元素的遍历访问、元素的删除、元素的插入、元素的添加、元素的修改等。二者的创建以及初始化方式相同，不同的就是功能上，单向链表不支持末尾方向的迭代、添加和取出。初始化方式一般有：1234567891011121314151617//第一种，构造函数int a[] = &#123;1,2,3,4,5&#125;;list&lt;int&gt; l1(a, a+5); // 将数组a的内容赋值给l1list&lt;int&gt; l2(2,100); // 2个值为100的元素list&lt;int&gt; l3(l2);list&lt;int&gt; l4(l3.begin(),l3.end()); list&lt;int&gt; l5 = &#123;1,2,3,4,5&#125;;//第二种，用push_back或push_front，forward_list不支持push_back()for (int i = 1; i &lt;= 5; ++i) l5.push_back(i); l5.push_front (200); l5.push_front (300);//第三种,用assignlist&lt;int&gt; first;list&lt;int&gt; second;first.assign(7,100); // 给first添加7个值为100的元素second.assign(first.begin(), first.end()); // 复制first给second另外，forward_list的元素删除是利用erase_after()，即将迭代器指向位置之后的一个元素删除，而list可以直接利用erase删除当前指向位置。除此之外，链表还提供了以下几个特殊功能：1234567891011std::forward_list&lt;int&gt; list1 = &#123;10, 20, 30, 40, 30, 20, 10&#125;;std::list&lt;int&gt; list2 = &#123;10, 20, 30, 40, 30, 20, 10&#125;;list1.remove(20);//移除20bool single_digit (const int&amp; value) &#123; return (value&lt;20); &#125;list2.remove_if(single_digit);//移除小于20的元素list1.sort();//从小到大排序list2.sort(std::greater&lt;int&gt;());//从大到小排序list1.merge(other);//合并两个链表，不过最好先排序再合并，止痒合并之后会自动排序，不然顺序看不懂list.unique(); //链表去重，可自定义指标，如：bool same_integral_part (double first, double second)&#123; return ( int(first)==int(second) ); &#125;还有链表的衔接功能，具体可以去看官网。5.哈希表和红黑树(map/unordered_map)map和unordered_map的功能就是实现了类似于字典查询的功能，即&lt;class T1, class T2&gt;的key-value模式，二者都在同名头文件之下，虽然功能一样，但是二者的底层实现完全不同。map采用的是红黑树结构，即一种自平衡的二叉排序树，其查询速度是logn，利用中序遍历方式便利，因此map本身会对元素进行排序。而unordered_map采用的是散列表的结构，散列函数为除留余数法，并利用链地址法防止散列冲突。因此map比unordered_map在时间效率上慢，不过空间占用要少。二者的功能一致，主要包含：创建、查找、增加、删除、遍历，另外要注意的是，map的排序是默认排序，如果key不是常见的数值元素，最好自定义一个比较函数。123456789101112131415161718192021//创建std::map&lt;std::string,int&gt; mymap = &#123; &#123; "alpha", 0 &#125;, &#123; "beta", 0 &#125;, &#123; "gamma", 0 &#125; &#125;;//可以利用pair结构填充 mymap.at("alpha") = 10; mymap.at("beta") = 20; mymap.at("gamma") = 30; mymap["alpha"]="an element"; mymap["beta"]="another element"; mymap["gamma"]=mymap['b'];//增加mymap.insert (std::pair&lt;string,int&gt;("abc",100) );//删除mymap.erase ("abc");it=mymap.find("alpha");mymap.erase (it);//遍历//同vector//查找it = mymap.find('b');//如果it != mymap.end()则存在，否则不存在，mymap.end()指向最后一个元素之后6.集合(set/unordered_set)集合的特性就是元素的独一性，与上一章类似，set和unordered_set分别采用红黑树和哈希表的方式创建，支持的元素不止基本数据类型，还支持容器。使用方式如下：123456789101112131415161718192021222324//创建std::set&lt;int&gt; first; // empty set of intsint myints[]= &#123;10,20,30,40,50&#125;;std::set&lt;int&gt; second (myints,myints+5); // rangestd::set&lt;int&gt; third (second); // a copy of secondstd::set&lt;int&gt; fourth (second.begin(), second.end()); std::set&lt;vector&lt;int&gt;&gt; s;//增加s.insert(&#123;1, 2&#125;);s.insert(&#123;1, 3&#125;);s.insert(&#123;1, 2&#125;);//删除myset.erase (it);//删除迭代器指向的元素myset.erase (40);it = myset.find (60);myset.erase (it, myset.end());//遍历//同上//查找//同map//功能second.count(10);//统计10的个数7.栈(stack)栈跟队列很像，不同的就是栈式遵循先进后出(Last in First out, LIFO)，功能很简单：1234567std::stack&lt;int&gt; mystack;mystack.push(10);//添加mystack.push(20);mystack.top() -= 5;//栈顶元素-5mystack.pop();//取出栈顶元素8.串(string)字符串(string)与字符数组(char[])很像，从速度上来讲string操作更慢，从操作效率来讲，string更简单，其中string在&lt;string&gt;头文件下，char相关字符操作在&lt;cstring&gt;头文件下。要注意的是下面两种char数组的区别：12char a[] = "123";//实际存储为&#123;'1', '2', '3', '\0'&#125;，strlen(a)=3char b[] = &#123;'1', '2', '3'&#125;;//实际存储为&#123;'1', '2', '3'&#125;，strlen(b)=3具体如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111//创建std::string s0 ("Initial string");// constructors used in the same order as described above:std::string s1;std::string s2 (s0);//copy s0std::string s3 (s0, 8, 3);//copy s0[8]~s0[11]std::string s4 ("A character sequence");std::string s5 ("Another character sequence", 12);//只保留12个字符std::string s6a (10, 'x');//10个xstd::string s6b (10, 42); // 42 is the ASCII code for '*'std::string s7 (s0.begin(), s0.begin()+7);//copy s0[0]~s0[6]//属性std::string str ("Test string");std::cout &lt;&lt; "size: " &lt;&lt; str.size() &lt;&lt; "\n";//11std::cout &lt;&lt; "length: " &lt;&lt; str.length() &lt;&lt; "\n";//11std::cout &lt;&lt; "capacity: " &lt;&lt; str.capacity() &lt;&lt; "\n";//15std::cout &lt;&lt; "max_size: " &lt;&lt; str.max_size() &lt;&lt; "\n";//429496729//元素访问str.at(i);str[i];for ( std::string::iterator it=str.begin(); it!=str.end(); ++it) std::cout &lt;&lt; *it;//元素赋值//stringstd::string str;std::string base="The quick brown fox jumps over a lazy dog.";str.assign(base);str.assign(base,10,9);//copy base[10]~base[18],"brown fox"str.assign("pangrams are cool",7);//截断为长度7,"pangram"str.assign("c-string");str.assign(10,'*');// "**********"str.assign&lt;int&gt;(10,0x2D);// "----------"str.assign(base.begin()+16,base.end()-12);// "fox jumps over"//char[]char ch1[] = "give me";char ch2[] = "a cup";strcpy(ch1,ch2);//ch1 = "a cup";//数组合并//stringstring str1 = "give me ";string str2 = "a cup";str1 = str1 + str2;//ch1=give me a cup//charchar ch1[15] = "give me "; // 注意长度，合并后为13char ch2[] = "a cup";strcat(ch1,ch2);//ch1=give me a cup//合并//stringstring str1 = "ab";string str2 = "cdefg";str1.append(str2,2,3); // str1=str1+str2[2~4],abefg//charchar ch1[10] = "ab"; // 注意合并后的长度char ch2[] = "abc";strncat(ch1,ch2,3); // ch1+ch2[0~2],ababc//替换//stringstr1.replace(0,1,str2,4,2);//str1[0]=str2[4]+str2[5]//charstrncpy(ch1,ch2,3);//ch1=ch1+ch2[0~2]//拷贝//stringstring str1 = "abc";char ch2[10] = "defg";str1.copy(ch2,10,1);//将str1[1~10]放在ch2，覆盖//charchar ch1[10] = "abc";char ch2[] = "de";memmove(ch1,ch2,2);//将ch2[0~1]放在ch1，覆盖//插入std::string str="to be question";std::string str2="the ";std::string str3="or not to be";std::string::iterator it;str.insert(6,str2); // to be (the )questionstr.insert(6,str3,3,4); // to be (not )the questionstr.insert(10,"that is cool",8); // to be not (that is )the questionstr.insert(10,"to be "); // to be not (to be )that is the questionstr.insert(15,1,':'); // to be not to be(:) that is the questionit = str.insert(str.begin()+5,','); // to be(,) not to be: that is the questionstr.insert (str.end(),3,'.'); // to be, not to be: that is the question(...)str.insert (it+2,str3.begin(),str3.begin()+3); // (or )//删除std::string str ("This is an example sentence.");str.erase (10,8); //从第10个位置删除8个元素，"This is an sentence."str.erase (str.begin()+9); //删除第10个元素， "This is a sentence."str.erase (str.begin()+5, str.end()-9); // //查找string str("Hello worldw");int m = str.find('w',0); // 从str起始位置开始查找w字符int n = str.find_first_not_of('w',0); // 查找str起始位置开始不是w的字符int k = str.find_first_of('w',0); // 从str起始位置开始查找第一个w字符int l = str.find_last_of('w'); // 查找最后一个w的位置int p = str.find_last_not_of('w'); // 查找最后一个不是w的字符的位置int q = str.rfind('w'); // 反向查找//转换string str1 = "Hello World";const char* ch1;ch1 = str1.c_str();9.bitset有时候为了操作的效率以及大数据处理，我们可能会用到数组元素只有{0,1}，仅仅占用1bit，不同于bool型的数组，其可以用来做2进制操作，位于同名头文件。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273//创建bitset&lt;4&gt; bitset1; //0000bitset&lt;8&gt; bitset2(12); //将12转化为8位2进制，00001100string s = "100101";bitset&lt;10&gt; bitset3(s); //0000100101char s2[] = "10101";bitset&lt;13&gt; bitset4(s2); //0000000010101bitset&lt;2&gt; bitset1(12); //12的二进制为1100（长度为4），但bitset1的size=2，只取后面部分，即00string s = "100101"; bitset&lt;4&gt; bitset2(s); //s的size=6，而bitset的size=4，只取前面部分，即1001char s2[] = "11101";bitset&lt;4&gt; bitset3(s2); //与bitset2同理，只取前面部分，即1110//位运算bitset&lt;4&gt; foo (string("1001"));bitset&lt;4&gt; bar (string("0011"));cout &lt;&lt; (foo^=bar) &lt;&lt; endl; // 1010 (foo对bar按位异或后赋值给foo)cout &lt;&lt; (foo&amp;=bar) &lt;&lt; endl; // 0010 (按位与后赋值给foo)cout &lt;&lt; (foo|=bar) &lt;&lt; endl; // 0011 (按位或后赋值给foo)cout &lt;&lt; (foo&lt;&lt;=2) &lt;&lt; endl; // 1100 (左移２位，低位补０，有自身赋值)cout &lt;&lt; (foo&gt;&gt;=1) &lt;&lt; endl; // 0110 (右移１位，高位补０，有自身赋值)cout &lt;&lt; (~bar) &lt;&lt; endl; // 1100 (按位取反)cout &lt;&lt; (bar&lt;&lt;1) &lt;&lt; endl; // 0110 (左移，不赋值)cout &lt;&lt; (bar&gt;&gt;1) &lt;&lt; endl; // 0001 (右移，不赋值)cout &lt;&lt; (foo==bar) &lt;&lt; endl; // false (0110==0011为false)cout &lt;&lt; (foo!=bar) &lt;&lt; endl; // true (0110!=0011为true)cout &lt;&lt; (foo&amp;bar) &lt;&lt; endl; // 0010 (按位与，不赋值)cout &lt;&lt; (foo|bar) &lt;&lt; endl; // 0111 (按位或，不赋值)cout &lt;&lt; (foo^bar) &lt;&lt; endl; // 0101 (按位异或，不赋值)//元素的访问和修改bitset&lt;4&gt; foo ("1011");//最低位是0，与常规数组索引相反！！！！！！！！cout &lt;&lt; foo[0] &lt;&lt; endl; //1cout &lt;&lt; foo[1] &lt;&lt; endl; //1cout &lt;&lt; foo[2] &lt;&lt; endl; //0//功能bitset&lt;8&gt; foo ("10011011");cout &lt;&lt; foo.count() &lt;&lt; endl; //5 bitset中1的个数cout &lt;&lt; foo.size() &lt;&lt; endl; //8 bitset的大小cout &lt;&lt; foo.test(0) &lt;&lt; endl; //true test函数用来查下标处的元素是0还是1，并返回false或truecout &lt;&lt; foo.test(2) &lt;&lt; endl; //false foo[2]为0，返回falsecout &lt;&lt; foo.any() &lt;&lt; endl; //true any函数检查bitset中是否有1cout &lt;&lt; foo.none() &lt;&lt; endl; //false none函数检查bitset中是否没有1cout &lt;&lt; foo.all() &lt;&lt; endl; //false all函数检查bitset中是全部为1bitset&lt;8&gt; foo ("10011011");cout &lt;&lt; foo.flip(2) &lt;&lt; endl; //10011111 指定位取反cout &lt;&lt; foo.flip() &lt;&lt; endl; //01100000 全部取反cout &lt;&lt; foo.set() &lt;&lt; endl; //11111111 全部置1cout &lt;&lt; foo.set(3,0) &lt;&lt; endl; //11110111 第3位置0cout &lt;&lt; foo.set(3) &lt;&lt; endl; //11111111 第3位置1cout &lt;&lt; foo.reset(4) &lt;&lt; endl; //11101111 第4位置0cout &lt;&lt; foo.reset() &lt;&lt; endl; //00000000 全部置0//转换bitset&lt;8&gt; foo ("10011011");string s = foo.to_string(); //“10011011”unsigned long a = foo.to_ulong(); //155，将bitset转换成unsigned long类型unsigned long long b = foo.to_ullong(); /155，将bitset转换成unsigned long long类型参考资料http://www.cplusplus.com/reference]]></content>
      <categories>
        <category>C++</category>
        <category>基础结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++之数据类型与运算符]]></title>
    <url>%2F2019%2F02%2F17%2FC%2B%2B%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[前言最近在学习数据结构的时候发现C++里面的很多数据类型、运算符的常识在特殊情况下能起到奇效，所以我想对这些基础做一个小小的总结，以方便后续使用。1.基础数据类型对于数据类型，我们需要掌握的是有哪些数据类型，每种数据类型在32位/64电脑上的内存占用如何，其数值范围是多少，而这些都是一一相关的，数值范围自然只跟数据所占字节位数以及unsigned和signed有关：数据类型说明32位字节数64位字节数取值范围16进制量级bool布尔型11true,false--char字符型11-128~1270x7F百unsigned char无符号字符型110~2550xFF百short短整型22-32768~327670x7FFF3万unsigned short无符号短整型220~655350xFFFF6万int整型44-2147483648~21474836470x7FFFFFFF21亿unsigned int无符号整型440~42949672950xFFFFFFFF42亿long长整型48---unsigned long无符号长整型48---long long长整型88---float单精度浮点数44---double双精度浮点数88---*指针48---ssize_t计数类型48---size_t无符号计数类型48---注：1字节byte=8位bit，1KB=1024B=2^13bit，1M=1024KB==2^23bit，1G=1024M=2^33bit其中要特殊注意的地方有：受操作系统位数影响的数据类型只有long、指针、size_t等基础类型，都是从4字节变为8字节；size_t和ssize_t等同于long和unsigned long；对于大数据处理，利用int的数值范围作为bit数组的索引，其中unsigned int的取值范围为0~2^32-1,所以存储一个42亿大小的bit数组需要用1/2G=512M内存；char字符串数组的定义形式有：char a[] = {‘a’, ‘b’, ‘\0’}，或者char a[] = “ab”；由于操作系统中编译器会进行内存对齐，其中32位/x86系统对齐内存为4字节,x64系统对齐内存为8字节，即内存地址是4的倍数，这就意味着，对于32位系统中的例子如下：1234567struct A&#123; A()&#123;&#125; ~A()&#123;&#125; int m1; char m2; static char m3;&#125;这个结构体占8字节内存，这是因为类的大小只与成员变量（非static成员)的虚函数指针有关，因此只需要考虑m1和m2的内存占用，即4+1=5，但是由于内存对齐的作用，其会占用8字节。指针的内存占用与指向的内容无关，即double *并不是说指针类型为double；float和double的取值范围计算方式很独特，比如float的4字节=32位，包括1符号位+8指数位+23尾数位，所以其取值范围为：$$ value = significand \times {2^{exponent}} $$由于指数位为8，所以指数范围为-127~128，那么其取值范围为-2^128~2^128，而尾数位23决定了其精度，即2^23 = 8388608，共7位，所以只能有7位有效数字，其中只能确保6位。同理double有1符号位+12指数位+53尾数位，保证15~16个有效数字。2.Ascii码ASCII 码使用指定的7 位或8 位二进制数组合来表示128 或256 种可能的字符，一般常用的字符只有128种。其中我们要特殊注意的有：ascii码中的48~57对应十进制中的0~9；ascii码中的65~90对应大写字母中的A~Z；ascii码中的97~122对应小写字母中的a~z；3.运算符3.1算术运算符其中要特殊注意的是有：+和*运算符可能造成数据内存溢出；/默认返回int类型的结果；++和--需要注意的是:12i++ =&gt; y = i;i = i + 1;return y;++i =&gt; i = i + 1; return i;相对来说，i++多了一个临时变量，另外它的过程是先引用在自增，一定要注意，一般来说自增操作会在最后执行，具体看后续的运算符优先级。3.2关系运算符3.3逻辑运算符这里要注意的是在使用逻辑运算符时，一定要判断何时为true，何时为false，在c++中，以下情况会被认为false：0、false、NULL、\0、nullptr。3.4位运算符位运算符主要包括与(&amp;)，或(|)，非(~)和异或(^)，之所以叫位运算，是因为其是针对2进制数进行操作的，我们利用位运算可以做很多便捷的事情：对于&amp;，利用这个我们可以用来逐渐将一个数变为0，通过n&amp;(n-1)，每次执行都会将一个1的位置零，因此其有效执行次数就是这个数的2进制形式中的1的个数；奇偶性：a&amp;1为0时候表示其为偶数，否则为奇数；取相反数：~a+1这个过程就是负数的2进制转换操作，先取反，然后+1；取第k位数：a&gt;&gt;k&amp;1，其中k是从0开始计算的；每8位取出数，可用于图像：a&gt;&gt;24&amp;0xFF，a&gt;&gt;16&amp;0xFF，a&gt;&gt;8&amp;0xFF，a&amp;0xFF；判断正负：a&gt;&gt;31为0则正，否则为负。3.5赋值运算符3.6杂项运算符这里面有几个要注意的点有：sizeof返回的是数据的内存占用，单位为字节，而&amp;返回的是数据的地址，*返回的是指针指向的数据内容；,运算符返回的是最后一条语句的值，eg:a=(1,2,3)，返回3；.和-&gt;都可以被用来引用成员，但是-&gt;一般用于指针类型结构体/类；cast并不是真正的运算符，其包括static_cast、dynamic_cast、reinterpret_cast和const_cast等方式，具体不描述。3.7运算符优先级运算符的优先级很重要，比如下面几个例子：1 &lt;&lt; 3 + 2 &amp; 7等价于 (1 &lt;&lt; (3 + 2))&amp;7；a++*2等价于y=a*2; a=a+1; return y；++a*2等价于a=a+1; return a*2；参考资料https://www.runoob.com/cplusplus/cpp-operators.html]]></content>
      <categories>
        <category>C++</category>
        <category>基础结构</category>
      </categories>
      <tags>
        <tag>数据类型</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵求导]]></title>
    <url>%2F2019%2F02%2F15%2F%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[前言无论是机器学习还是最近大热的深度学习算法，很多情况下都需要用到矩阵求导方面的知识，比如拉格朗日乘子法、梯度下降法等等，但是矩阵求导不同于传统的标量求导法则，存在很多冲突。网上以及各种论文中所推导的梯度计算方法，标准都不一样，容易把人搞迷糊。所以我这里理一下矩阵/向量求导相关的原理，并且确定一套属于自己的求导标准，方便自己研究。1.规则说明整体来看，矩阵/向量求导涉及到标量，向量和矩阵三者两两之间的求导，那么这其中就涉及到了如下两种规则，这也是众多论文、网站博客让人迷惑的地方：布局(layout)：求导后的向量/矩阵形状；链式顺序：比如f(g(x))求导问题中f’(g(x))和g’(x)的顺序排列问题；数据优先级：列优先或者行优先，其中列优先时默认为列向量，对于矩阵的向量展开顺序为逐列展开。最有意思的地方在于，我们平时所碰见的机器学习和深度学习相关的指标计算，常常会利用范数进行刻画：$$ \left\{ \begin{array}{l} {\left\| X \right\|_1}{\rm{ = }}{{{\rm{\vec 1}}}^T} \times vec\left( X \right)\\ {\left\| X \right\|_2}{\rm{ = }}tr\left( {{X^T}X} \right)\\ {\left\| X \right\|_\infty }{\rm{ = }}max\left( {\left| X \right|} \right) = {{\vec k}^T} \times vec\left( X \right) \end{array} \right. $$其中的k向量是一个除了最大值位置处为1，其他位置都为0的矩阵的向量展开。从上面可以看到，如果不考虑函数链式法则（即每次都从结果处开始求导)，那么基本上所有的一阶导数问题都能归结为标量对于向量和矩阵的导数问题。但是如果是分段式的求导或者计算二阶导数则需要用到向量和矩阵之间的相互求导。这里我先确定一些统一规则：符号说明：标量用小写字母(a,b,c,…)表示，向量用加粗小写字母(x,y,z,w,…)表示，矩阵用加粗的大写字母(A,B,C…)表示；列优先，即所有向量都是列向量，矩阵的向量化也是逐列展开；分母布局，即求导后的变量形状与分母对齐，至少保证行数一致，这一点是为了方便待求变量的更新，比如：$$ \begin{array}{*{20}{l}} {\frac{{\partial y}}{{\partial {\bf{x}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial y}}{{\partial {x_1}}}}\\ {\frac{{\partial y}}{{\partial {x_2}}}}\\ \vdots \\ {\frac{{\partial y}}{{\partial {x_n}}}} \end{array}} \right].}\\ {\frac{{\partial {\bf{y}}}}{{\partial x}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial {y_1}}}{{\partial x}}}&{\frac{{\partial {y_2}}}{{\partial x}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial x}}} \end{array}} \right].}\\ {\frac{{\partial {\bf{y}}}}{{\partial {\bf{x}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial {y_1}}}{{\partial {x_1}}}}&{\frac{{\partial {y_2}}}{{\partial {x_1}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_1}}}}\\ {\frac{{\partial {y_1}}}{{\partial {x_2}}}}&{\frac{{\partial {y_2}}}{{\partial {x_2}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_2}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {\frac{{\partial {y_1}}}{{\partial {x_n}}}}&{\frac{{\partial {y_2}}}{{\partial {x_n}}}}& \cdots &{\frac{{\partial {y_m}}}{{\partial {x_n}}}} \end{array}} \right].}\\ {\frac{{\partial y}}{{\partial {\bf{X}}}}}&{ = \left[ {\begin{array}{*{20}{c}} {\frac{{\partial y}}{{\partial {x_{11}}}}}&{\frac{{\partial y}}{{\partial {x_{12}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{1q}}}}}\\ {\frac{{\partial y}}{{\partial {x_{21}}}}}&{\frac{{\partial y}}{{\partial {x_{22}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{2q}}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {\frac{{\partial y}}{{\partial {x_{p1}}}}}&{\frac{{\partial y}}{{\partial {x_{p2}}}}}& \cdots &{\frac{{\partial y}}{{\partial {x_{pq}}}}} \end{array}} \right].} \end{array} $$可以看到上面两个向量之间的求导得到的是一个矩阵，这个是符合求导规则的，但是机器学习中的参数更新我们会利用梯度进行更新，这一点会在以后讲解梯度下降法、牛顿法、共轭梯度下降法的时候介绍。既然要更新，则肯定需要保证导数和参数形状一模一样，那么我们根据梯度的累加性得到：$$ \vec x = \vec x - \eta \frac{{\partial \vec y}}{{\partial \vec x}} \times \vec 1 $$这样做还有一个好处，那就是待求参数的转置不影响求导结果，即：$$ \frac{{\partial \left( {Ax} \right)}}{{\partial x}} = \frac{{\partial \left( {{x^T}{A^T}} \right)}}{{\partial x}}={A^T} $$链式法则和运算法则对于分子的组成很敏感，如果详细推导会很麻烦，当分子为标量时，可以用第2章的内容进行推导，当分子为向量时，有如下规律：2.矩阵求导2.1 迹的引入上面所介绍的是标量和向量相关的求导法则，但是涉及到矩阵问题的时候，会出现不一样的地方。由于向量或者矩阵求导的本质是相应元素标量之间的求导，所以对于一个m x n的矩阵A和p x q的矩阵B之间导数，必定是一个mp x nq的大型矩阵，这个操作利用简单Hadamard矩阵乘法是做不到的，我们也无法利用上面的规则。所以对于这类问题可以利用方阵的迹进行求解，其中标量也可以视为是1 x 1大小的方阵，下面所介绍的规则同样适用于标量对向量的求导。首先我们来研究一下一元微积分和多元微积分微分的原始定义：$$ \left\{ \begin{array}{l} df = f'\left( x \right)dx\\ df = \sum\limits_{i = 1}^n {\frac{{\partial f}}{{\partial {x_i}}}d{x_i}} \end{array} \right. \Rightarrow df = {\left( {\frac{{\partial f}}{{\partial x}}} \right)^T}dx $$可以发现可以用导数的转置向量与微分向量的内积来表示导数，所以矩阵的微分也可以定义为：$$ df = \sum\limits_{i = 1}^m {\sum\limits_{j = 1}^n {\frac{{\partial f}}{{\partial {X_{ij}}}}} } d{X_{ij}} = tr\left( {{{\frac{{\partial f}}{{\partial X}}}^T}dX} \right) $$，其中tr表示矩阵的迹，即矩阵对角线元素之和。不同于上面的求导法则，下面我们针对分子为标量或者方阵的矩阵/向量求导设置新的运算法则：微分法则，其中$\odot$表示矩阵点乘，$\sigma$表示元素级函数：$$ \left\{ {\begin{array}{*{20}{l}} {d\left( {X \pm Y} \right) = dX \pm dY}\\ \begin{array}{l} d\left( {XY} \right) = \left( {dX} \right)Y + XdY\\ d\left( {X \odot Y} \right) = dX \odot Y + X \odot dY\\ d\left( {\sigma \left( X \right)} \right) = \sigma '\left( X \right) \odot dX \end{array} \end{array}} \right. $$微分与迹：$$ \left\{ \begin{array}{l} d\left( {tr(X)} \right) = tr\left( {dX} \right)\\ dx = tr\left( {dx} \right)\\ d\left( {{X^T}} \right) = {\left( {dX} \right)^T}\\ tr\left( {{X^T}} \right) = tr\left( X \right) \end{array} \right. $$迹的运算法则：$$ \left\{ \begin{array}{l} tr\left( {A \pm B} \right) = tr\left( A \right) \pm tr\left( B \right)\\ tr\left( {ABC} \right) = tr\left( {CAB} \right) = tr\left( {BCA} \right)\\ tr\left( {{A^T}\left( {B \odot C} \right)} \right) = tr\left( {{{\left( {A \odot B} \right)}^T}C} \right) \end{array} \right. $$其中第二项并不是说矩阵的迹具有交换律，而是轮换不变性，即向左向右移位。特殊运算：$$ \left\{ \begin{array}{l} d\left( {X{X^{ - 1}}} \right) = dX{X^{ - 1}} + Xd{X^{ - 1}} = dI = 0 \Rightarrow d{X^{ - 1}} = - {X^{ - 1}}dX{X^{ - 1}}\\ d\left| X \right| = tr\left( {{X_{adj}}dX} \right)\mathop \to \limits^{inversable} \left| X \right|tr\left( {{X^{ - 1}}dX} \right) \end{array} \right. $$2.2 算例说明由于绝大部分机器学习算法中涉及到的求导都是标量对向量和矩阵的，所以这里用迹来求解非常方便，我们还是保证列优先：eg1: 假设y=AXB是标量，那么其关于X的导数为：$$ \begin{array}{l} dy = tr\left( {dy} \right)\\ = tr\left( {d\left( {AXB} \right)} \right)\\ = tr\left( {dAXB + AdXB + AXdB} \right)\\ = tr\left( {AdXB} \right)\\ = tr\left( {BAdX} \right)\\ = tr\left( {{{\left( {{A^T}{B^T}} \right)}^T}dX} \right)\\ \Rightarrow \frac{{\partial y}}{{\partial X}} = {\left( {{A^T}{B^T}} \right)^T} \end{array} $$eg2:$$ \begin{array}{l} f = tr\left( {{Y^T}MY} \right),Y = \sigma \left( {WX} \right)\\ \Rightarrow df\\ = tr\left( {d{Y^T}MY + {Y^T}MdY} \right)\\ = tr\left( {{Y^T}{M^T}dY + {Y^T}MdY} \right)\\ = tr\left( {{Y^T}\left( {{M^T} + M} \right)dY} \right)\\ \Rightarrow \frac{{\partial f}}{{\partial Y}} = \left( {M + {M^T}} \right)Y\\ \because dY = \sigma '\left( {WX} \right) \odot \left( {WdX} \right)\\ \therefore df = tr\left( {{{\frac{{\partial f}}{{\partial Y}}}^T}dY} \right)\\ = tr\left( {{{\frac{{\partial f}}{{\partial Y}}}^T}\left( {\sigma '\left( {WX} \right) \odot \left( {WdX} \right)} \right)} \right)\\ = tr\left( {\left( {{{\frac{{\partial f}}{{\partial Y}}}^T} \odot \sigma '\left( {WX} \right)} \right)WdX} \right)\\ \Rightarrow \frac{{\partial f}}{{\partial X}} = {W^T}\left( {\frac{{\partial f}}{{\partial Y}} \odot \sigma '{{\left( {WX} \right)}^T}} \right) \end{array} $$eg3: 岭回归模型，X为m x n的矩阵，w为n x 1的向量，y为m x 1的向量$$ \begin{array}{l} l = {\left\| {Xw - y} \right\|^2} + \lambda \left\| w \right\|\\ = {\left( {Xw - y} \right)^T}\left( {Xw - y} \right) + {w^T}w\\ \Rightarrow dl = tr\left( {dl} \right)\\ = tr\left( {d{{\left( {Xw - y} \right)}^T}\left( {Xw - y} \right) + {{\left( {Xw - y} \right)}^T}d\left( {Xw - y} \right) + d{w^T}w + {w^T}dw} \right)\\ \because d\left( {Xw} \right) = dXw + Xdw = Xdw\\ \therefore dl = tr\left( {{{\left( {Xdw} \right)}^T}\left( {Xw - y} \right) + {{\left( {Xw - y} \right)}^T}Xdw + {w^T}dw + {w^T}dw} \right)\\ = tr\left( {2{{\left( {Xw - y} \right)}^T}Xdw + 2{w^T}dw} \right)\\ = tr\left( {2{{\left( {{X^T}\left( {Xw - y} \right)} \right)}^T}dw + 2{w^T}dw} \right)\\ \therefore \frac{{\partial l}}{{\partial w}} = 2\left( {{X^T}\left( {Xw - y} \right) + w} \right) = 0\\ \therefore \left( {{X^T}X + I} \right)w = {X^T}y\\ \therefore w = {\left( {{X^T}X + I} \right)^{ - 1}}{X^T}y \end{array} $$eg4: 神经网络(交叉熵损失函数+softmax激活函数+2层神经网络)，y是一个除一个元素为1之外，其他元素都为0的m x 1向量，W2为m x p矩阵，W1为p x n矩阵，x是n x 1向量：$$ \begin{array}{l} l = - {y^T}\log softmax\left( {{W_2}\sigma \left( {{W_1}x} \right)} \right)\\ \Rightarrow \left\{ \begin{array}{l} l = - {y^T}\log {h_2}\\ {h_2} = softmax\left( {{a_2}} \right) = \frac{{{e^{{a_2}}}}}{{{{\vec 1}^T}{e^{{a_2}}}}}\\ {a_2} = {W_2}{h_1}\\ {h_1} = \sigma \left( {{a_1}} \right) = \frac{1}{{1 + {e^{ - {a_1}}}}}\\ {a_1} = {W_1}x \end{array} \right.\\ \therefore dl = tr\left( {dl} \right) = tr\left( { - {y^T}\left( {\frac{1}{{{h_2}}} \odot d{h_2}} \right)} \right)\\ \left( 1 \right)if\;i = j\\ \frac{{\partial softmax\left( {a_{_2}^i} \right)}}{{\partial a_{_2}^j}} = \left( {\frac{{{e^{a_{_2}^i}}}}{{k + {e^{a_{_2}^i}}}}} \right)'\\ = \left( {1 - \frac{k}{{k + {e^{a_{_2}^i}}}}} \right)'\\ = \frac{{k{e^{a_{_2}^i}}}}{{{{\left( {k + {e^{a_{_2}^i}}} \right)}^2}}}\\ = \frac{{{e^{a_{_2}^i}}}}{{k + {e^{a_{_2}^i}}}}\frac{k}{{k + {e^{a_{_2}^i}}}}\\ = softmax\left( {a_{_2}^i} \right)\left( {1 - softmax\left( {a_{_2}^i} \right)} \right)\\ = h_2^i\left( {1 - h_2^i} \right)\\ \left( 2 \right)if\;i \ne j:\\ \frac{{\partial softmax\left( {a_{_2}^i} \right)}}{{\partial a_{_2}^j}} = \left( {\frac{{{e^{a_{_2}^i}}}}{{n + {e^{a_{_2}^j}}}}} \right)'\\ = \left( { - \frac{{{e^{a_{_2}^i}}{e^{a_{_2}^j}}}}{{n + {e^{a_{_2}^j}}}}} \right)\\ = - \frac{{{e^{a_{_2}^i}}{e^{a_{_2}^j}}}}{{{{\left( {n + {e^{a_{_2}^j}}} \right)}^2}}}\\ = - h_2^ih_2^j\\ \therefore 利用向量对向量求导的定义可得：\\ \therefore d{h_2} = (diag\left( {{h_2}} \right) - {h_2}h_2^T)d{a_2}\\ dl = tr\left( { - {y^T}\left( {\frac{1}{{{h_2}}} \odot \left( {diag\left( {{h_2}} \right) - {h_2}h_2^T} \right)d{a_2}} \right)} \right)\\ = tr\left( { - {{\left( {y \odot \frac{1}{{{h_2}}}} \right)}^T}\left( {diag\left( {{h_2}} \right) - {h_2}h_2^T} \right)d{a_2}} \right)\\ = tr\left( {\left( { - {y^T} + h_2^T} \right)d{a_2}} \right)\\ = tr\left( {{{\left( {{h_2} - y} \right)}^T}d{a_2}} \right)\\ \because d{a_2} = d{W_2}{h_1} = {W_2}d{h_1}\\ \therefore dl = tr\left( {{{\left( {{h_2} - y} \right)}^T}d{W_2}{h_1}} \right) = tr\left( {{h_1}{{\left( {{h_2} - y} \right)}^T}d{W_2}} \right)\\ \Rightarrow \frac{{\partial l}}{{\partial {W_2}}} = \left( {{h_2} - y} \right)h_1^T\\ \because d{h_1} = \sigma '\left( {{a_1}} \right) \odot d{a_1} = {h_1} \odot \left( {1 - {h_1}} \right) \odot d{a_1}\\ d{a_1} = d{W_1}x\\ \therefore dl = tr\left( {{{\left( {{h_2} - y} \right)}^T}{W_2}\left( {{h_1} \odot \left( {1 - {h_1}} \right) \odot d{W_1}x} \right)} \right)\\ = tr\left( {{{\left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right)}^T}d{W_1}x} \right)\\ = tr\left( {x{{\left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right)}^T}d{W_1}} \right)\\ \Rightarrow \frac{{\partial l}}{{\partial {W_1}}} = \left( {{W_2}^T\left( {{h_2} - y} \right) \odot {h_1} \odot \left( {1 - {h_1}} \right)} \right){x^T} \end{array} $$这里主要是为了强调这个过程，实际上log和softmax在一起时，可以将乘除法转化为加减法。2.3 矩阵拓展上面只介绍了向量与标量、向量与向量和矩阵与标量的求导规则，但是对于矩阵与矩阵和矩阵与向量的求导尚未说明，这里只简单说明。我们在之前讨论过，矩阵求导本质上是矩阵对矩阵每一个元素的求导，那么为了更方便整体求导，不妨先将矩阵转化为向量，即将矩阵拉伸为列向量，然后再求导，其同样满足：$$ \begin{array}{l} vec\left( {dF} \right) = {\frac{{\partial F}}{{\partial X}}^T}vec\left( {dX} \right)\\ \left\{ \begin{array}{l} vec\left( {AXB} \right) = \left( {{B^T} \otimes A} \right)vec\left( X \right)\\ vec\left( {{A^T}} \right) = {K_{mn}}vec\left( A \right)\\ vec\left( {A \odot X} \right) = diag\left( A \right)vec\left( X \right) \end{array} \right. \end{array} $$其中$\otimes$表示Kronecker积，${K_{mn}}$表示转换矩阵，其实就是转换前后的仿射矩阵，关于这两个算子有如下规律：$$ \left\{ \begin{array}{l} {\left( {A \otimes B} \right)^T} = {A^T} \otimes {B^T}\\ vec\left( {a{b^T}} \right) = b \otimes a\\ \left( {A \otimes B} \right)\left( {C \otimes D} \right) = \left( {AC} \right) \otimes \left( {BD} \right)\\ {K_{mn}} = K_{nm}^T\\ {K_{mn}}K_{nm}^T = I\\ {K_{pm}}\left( {A \otimes B} \right){K_{nq}} = B \otimes A \end{array} \right. $$如果想利用上面的方式求导，在参数更新的时候，比如AXB，其中A(m x n),X(n x p),B(p x q)，需要对导数(pn x qm)乘以一个qm x 1的全1向量，用来将梯度累加，然后重塑为矩阵n x p。参考资料https://en.wikipedia.org/wiki/Matrix_calculushttps://zhuanlan.zhihu.com/p/24709748https://zhuanlan.zhihu.com/p/24863977]]></content>
      <categories>
        <category>机器学习</category>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>矩阵求导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解YOLO系列]]></title>
    <url>%2F2019%2F02%2F09%2F%E8%AF%A6%E8%A7%A3YOLO%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。其backbone网络darknet也可以替换为很多其他的框架，所以在工程领域也十分受欢迎，下面我将依次介绍YOLO系列的详细发展过程，包括每个版本的原理、特点、缺点，最后还会交代相关的安装、训练与测试方法。1.目标检测简介YOLO(You Only Look Once)的作者非常萌，无论是写作风格、表情包还是Github风格，都表现出他是一个有趣的人。好了，言归正传，众所众知，目标检测算法的核心在于：候选区域/框/角点等的确定。神经网络/深度学习本质是分类，那么对于目标检测问题，我们需要将其转化为分类问题，因此许多研究者发现需要先确定候选位置，然后对候选位置进行分类判断。这里，候选区域的选取从最初的滑窗方式。慢慢演变到以Selective Search(过分割+分层聚类)为主的RCNN算法，为了更高效的生成候选区域，我们又利用卷积和池化过程近似滑窗从而有了Fast RCNN算法。再演变至以anchor box为代表的Faster RCNN、SSD和YOLO等系列算法，其原理在于可以对每一个ROI区域的中心，给定一个假设的长宽比，由此作为候选区域，再在后面利用回归层精修回归框。最后到现在的Corner为代表的CornerNet算法，不断地提升候选框提取效率、候选框有效率、候选框精准度以及与分类框架的融合。判断目标属于什么类别。有了候选区域，那么就可以利用很简单的级联全连接层判定每个候选区域属于前景/背景的概率，以及属于各个目标类别的概率。目标精定位。目标框的描述包括目标的中心/角点位置和宽高，这些仅仅依赖候选区域是不够精确的，那么就需要合理的设计损失函数，利用多个全连接层进行进一步的回归，得到目标框的精修位置。当然CornerNet里面采用骨骼关键点检测里面的Hourglass结构作为backbone，再加上Corner Pooling层预测角点位置，不存在精修。2.Darknet2.1 Darknet网络框架目前来说，无论是在目标检测、目标识别还是目标分割、姿态分析等领域，都会用到各种各样的backbone网络，最常用的就是基于图像分类的backbone网络，因为深度学习本质是分类，而绝大多数分类网络都会在ImageNet竞赛中进行测试，我们从AlexNet，VGGNet到GoogleNet，再到ResNet/DenseNet等，已经见过很多优秀的骨干网络结构了，其中很多优秀的子模块也被用于其他网络结构，如：卷积+池化+BatchNorm+Relu的组合、Inception各个版本结构、残差模块、1x1卷积核等等。而Darknet实际上也是YOLO作者实现的一个backbone网络，其改进对象主要是GoogleNet，将GoogleNet中的Inception结构改成了串行的结构，从而使得网络速度更快，而效果仅仅损失了一点。可以看到下面的网络结构有24个卷积层，外加2个全连接层。2.2 Darknet训练框架除此之外呢，Darknet也是一个深度学习框架，其框架设计与caffe基本一致，只不过是用C语言写的，其整体框架十分简洁，所以编译速度非常快。如果学习过caffe的话，应该很容易上手darknet。其中darknet.h就类似于caffe中的caffe.proto，定义了所有数据结构，而网络的构建是利用了cfg格式文件，即利用key=value方式搭建网络，这种方式的问题在于对于复杂网络的设计非常复杂，很难写。另外，由于darknet是纯C框架，所以要想增加自定义层的话会比较麻烦，主要是因为没有好的设计模式和面向对象设计，导致使用者需要完全读懂整个框架，而且很难实现共享内存和逐层不同学习率。当然，darknet框架的安装也是很简单的，除开显卡驱动和CUDA、cudnn等配置之外，只需要从git上面clone下来源码，然后make即可，这里我们不考虑Windows版本的，github上面有相应的教程。不仅可以利用原始的C接口，还能利用将其编译为动态链接库供C++接口调用，见这里，不过我主要是利用python接口调用，这里呢就存在一个问题，即原始darknet数据结构是image，如果利用ctypes进行C/Python混合编程的话，需要设计到numpy数据结构与image数据结构的交互，即：123456789101112131415161718192021222324252627from ctypes import *import darknet as dnimport numpy as npimport cv2class IMAGE(Structure): _fields_ = [("w", c_int), ("h", c_int), ("c", c_int), ("data", POINTER(c_float))]class DETECTION(Structure): _fields_ = [("bbox", BOX), ("classes", c_int), ("prob", POINTER(c_float)), ("mask", POINTER(c_float)), ("objectness", c_float), ("sort_class", c_int)] def array_to_image(arr): arr = arr.transpose(2,0,1) c = arr.shape[0] h = arr.shape[1] w = arr.shape[2] arr = (arr/255.0).flatten() data = dn.c_array(dn.c_float, arr) im = dn.IMAGE(w,h,c,data) return im上面这种利用图像数据结构转化的方式，会占用很多时间，所以我们可以利用numpy的c接口实现数据结构转换，具体如下：先在src/image.c line 558左右添加：1234567891011121314151617181920212223242526272829#ifdef NUMPYimage ndarray_to_image(unsigned char* src, long* shape, long* strides)&#123; int h = shape[0]; int w = shape[1]; int c = shape[2]; int step_h = strides[0]; int step_w = strides[1]; int step_c = strides[2]; image im = make_image(w, h, c); int i, j, k; int index1, index2 = 0; for(i = 0; i &lt; h; ++i)&#123; for(k= 0; k &lt; c; ++k)&#123; for(j = 0; j &lt; w; ++j)&#123; index1 = k*w*h + i*w + j; index2 = step_h*i + step_w*j + step_c*k; im.data[index1] = src[index2]/255.; &#125; &#125; &#125; rgbgr_image(im); return im;&#125;#endif然后在src/image.h19行左右添加：123#ifdef NUMPYimage ndarray_to_image(unsigned char* src, long* shape, long* strides);#endif再在MakeFile中加入：1234ifeq ($(NUMPY), 1) COMMON+= -DNUMPY -I/usr/include/python2.7/ -I/usr/lib/python2.7/dist-packages/numpy/core/include/numpy/CFLAGS+= -DNUMPYendif并设置Makefile123456GPU=1CUDNN=1OPENCV=1OPENMP=0NUMPY=1DEBUG=0最后python接口为：1234def nparray_to_image(self,img): data = img.ctypes.data_as(POINTER(c_ubyte)) image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides) return image这样的话，数据转换的速度大大提升,下面我附上我写的darknet.py：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146from ctypes import *import mathimport randomimport timeimport cv2import osdef c_array(ctype, values): arr = (ctype*len(values))() arr[:] = values return arrclass BOX(Structure): _fields_ = [("x", c_float), ("y", c_float), ("w", c_float), ("h", c_float)]class DETECTION(Structure): _fields_ = [("bbox", BOX), ("classes", c_int), ("prob", POINTER(c_float)), ("mask", POINTER(c_float)), ("objectness", c_float), ("sort_class", c_int)]class IMAGE(Structure): _fields_ = [("w", c_int), ("h", c_int), ("c", c_int), ("data", POINTER(c_float))]class METADATA(Structure): _fields_ = [("classes", c_int), ("names", POINTER(c_char_p))]class ObjectDetect: def __init__(self,cfg_path = None, wight_path = None, meta_path = None, ctx = None): lib = CDLL(os.path.dirname(os.path.realpath(__file__))+"/libdarknet.so", RTLD_GLOBAL) lib.network_width.argtypes = [c_void_p] lib.network_width.restype = c_int lib.network_height.argtypes = [c_void_p] lib.network_height.restype = c_int predict = lib.network_predict predict.argtypes = [c_void_p, POINTER(c_float)] predict.restype = POINTER(c_float) self.set_gpu = lib.cuda_set_device self.set_gpu.argtypes = [c_int] if ctx is not None: self.set_gpu(ctx) make_image = lib.make_image make_image.argtypes = [c_int, c_int, c_int] make_image.restype = IMAGE self.get_network_boxes = lib.get_network_boxes self.get_network_boxes.argtypes = [c_void_p, c_int, c_int, c_float, c_float, POINTER(c_int), c_int, POINTER(c_int)] self.get_network_boxes.restype = POINTER(DETECTION) make_network_boxes = lib.make_network_boxes make_network_boxes.argtypes = [c_void_p] make_network_boxes.restype = POINTER(DETECTION) self.free_detections = lib.free_detections self.free_detections.argtypes = [POINTER(DETECTION), c_int] free_ptrs = lib.free_ptrs free_ptrs.argtypes = [POINTER(c_void_p), c_int] network_predict = lib.network_predict network_predict.argtypes = [c_void_p, POINTER(c_float)] self.load_net = lib.load_network self.load_net.argtypes = [c_char_p, c_char_p, c_int] self.load_net.restype = c_void_p self.net = self.load_net(cfg_path, weight_path, 0) self.do_nms_obj = lib.do_nms_obj self.do_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float] do_nms_sort = lib.do_nms_sort do_nms_sort.argtypes = [POINTER(DETECTION), c_int, c_int, c_float] self.free_image = lib.free_image self.free_image.argtypes = [IMAGE] letterbox_image = lib.letterbox_image letterbox_image.argtypes = [IMAGE, c_int, c_int] letterbox_image.restype = IMAGE self.load_meta = lib.get_metadata lib.get_metadata.argtypes = [c_char_p] lib.get_metadata.restype = METADATA self.meta = self.load_meta(meta_path) load_image = lib.load_image_color load_image.argtypes = [c_char_p, c_int, c_int] load_image.restype = IMAGE rgbgr_image = lib.rgbgr_image rgbgr_image.argtypes = [IMAGE] self.ndarray_image = lib.ndarray_to_image self.ndarray_image.argtypes = [POINTER(c_ubyte), POINTER(c_long), POINTER(c_long)] self.ndarray_image.restype = IMAGE self.predict_image = lib.network_predict_image self.predict_image.argtypes = [c_void_p, IMAGE] self.predict_image.restype = POINTER(c_float) def nparray_to_image(self,img): data = img.ctypes.data_as(POINTER(c_ubyte)) image = self.ndarray_image(data, img.ctypes.shape, img.ctypes.strides) return image def detect(self,img, thresh=.5, hier_thresh=.5, nms=.45): im = self.nparray_to_image(img) num = c_int(0) pnum = pointer(num) self.predict_image(self.net, im) dets = self.get_network_boxes(self.net, im.w, im.h, thresh, hier_thresh, None, 0, pnum) num = pnum[0] if (nms): self.do_nms_obj(dets, num, self.meta.classes, nms); res = [] for j in range(num): for i in range(meta.classes): if dets[j].prob[i] &gt; 0: b = dets[j].bbox res.append((self.meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h))) res = sorted(res, key=lambda x: -x[1]) self.free_image(im) self.free_detections(dets, num) return resif __name__ == "__main__": model = ObjectDetect("cfg/yolo.cfg", "yolo.weights","cfg/coco.data") cap = cv2.VideoCapture(0) ret, img = cap.read() r = model.detect(img) print(r)3.YOLOv13.1 YOLO网络框架作者为了让backbone网络具有更好的性能，取了上面提到的darknet版本的前20个卷积层，然后利用一个全局池化层和一个全连接层，搭建了一个预训练网络，其中全局池化层是指的将一个通道内的所有元素平均，这一点在YOLO系列版本中都有体现：可以看到YOLOv1才用的darknet版本(Darknet Reference)效果最差，其余的我们后面再说。有了预训练模型之后，我们再以上面提到的Darknet的完整框架（24卷积层+2全连接层）进行训练，其中网络输入大小固定为448x448。然后将最后一层的输出形状改为7x7x30。对于网络的输出，我们可以这样理解，30=20+2+4x2，其中20指的是VOC数据集的类别数，即20个类别的概率，2指的是两个目标框的置信度，然后每个通道预测2个目标框，所以就是两个(x,y,w,h)即8个元素。那么作者在论文中所提到的网格划分是怎么体现的呢，这里要通过最后一个卷积层的输出来看，即7x7x1024，可以看到特征图的尺寸是7x7，那么根据卷积网络的特点，每一层的输出特征图上的每一个像素点都会对应着输入特征图的一个区域，也就是这个像素点的感受野，那么在最后一层卷积层输出特征图上，也是如此，所以我们可以认为是将原图划分成了7X7的网格区域，每个网格预测20个类别的概率，目标框置信度以及两个目标框信息，其中每个目标的中心位置都会转换至网格区域内。另外要说明的是，我在最新的github版本中发现，最后两个全连接层被替换成了：12345678910111213[local]size=3stride=1pad=1filters=256activation=leaky[dropout]probability=.5[connected]output= 1715activation=linear其中的connected不用多说，就是全连接层，只不过节点数变成了1714,即7x7x35，那么这个35则说明每个网格区域会输出：20个类别概率，3个目标框置信度，3个目标框信息包含(x,y,w,h)。对于每个框所包含的物体判别方式则是采用了贝叶斯公式，将上述各个类别的概率作为条件概率。因此每个类别的真实置信度计算方式如下：\left\{ \begin{array}{l} Confidenc{e_{object}} = P\left( {object} \right) \times IOU_{truth}^{pred}\\ Confidenc{e_{clas{s_i}}} = Confidenc{e_{object}} \times P\left( {\left. {clas{s_i}} \right|object} \right) \end{array} \right.也就是说每个目标框所输出的边框置信度，本身就包含了先验概率和IOU的乘积，这一点在YOLOv2论文中有所体现。另外我们还发现YOLOv1中是直接输出目标框的，而不是采用anchor boxes方式。而local是用的Locally Connected Layers结构，这种结构跟1x1卷积方式不同，1x1卷积核是利用很多个1x1大小的卷积核遍历整个特征图，而Locally Connected Layers结构则是一个介于全连接和卷积网络之间的一个结构：可以看到，它也是利用卷积的方式进行计算的，不同的地方在于随着卷积的不断遍历，每个遍历位置的卷积核都不一样，即没有了卷积层所特有的共享内存。其好处在于更多的利用了空间相对区域特征以及整体特征，从而提升了一点效果。3.2 数据准备目前最常用的两个目标检测数据集分别是VOC和COCO，其中VOC数据集中的目标多为大目标,分为20个类别，而COCO数据集中有很多小而密集的目标，更加贴近实际，共80个类别，有几十万幅图像，几百万个目标实例。对于YOLO的训练，我们需要将每个目标的信息进行转化，其中一个文件中包含所有目标信息:1&lt;class_label,center_x,center_y,width,height&gt;其中目标框信息都需要归一化，即除以对应的图像宽高，另一个文件中则是包含对应图像的地址。3.3 数据增强作者在训练中主要采用了 jittering 和 HSV 空间扰动两种数据增强方式，详细的过程比较复杂，我用 matlab 把过程复现了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144clc;clear;close all;%% 参数设置jitter = 0.3;%抖动幅度hue = 0.1;%色调变化幅度saturation = 1.5;%饱和度变化幅度exposure = 1.5;%曝光率变化幅度w = 416;%网络输入的宽h = 416;%网络输入的高filepath = 'E:\201709\val\val\000e4adcf3a3a5a246351fd4a3e18ae9ac4d44a9.jpg';%图片地址%% jittering+resizeI = imread(filepath);%读取图像[oh,ow,~] = size(I);%读取原图宽高[dw,dh] = deal(floor(ow*jitter),floor(oh*jitter));%计算抖动值上限pleft = floor(-dw + 2*rand*dw);%随机化 left 抖动值pright = floor(-dw + 2*rand*dw);%随机化 right 抖动值ptop = floor(-dh + 2*rand*dh);%随机化 top 抖动值pbot = floor(-dh + 2*rand*dh);%随机化 bot 抖动值swidth = ow - pleft - pright;%计算抖动后的图像宽度sheight = oh - ptop - pbot;%计算抖动后的图像高度sx = swidth/ow;%计算 jittering 后图像宽度与原图的比例sy = sheight/oh;%计算 jittering 后图像高度与原图的比例%各向同性 cropcrop_image = uint8(zeros(sheight,swidth,3));for i = 1 : sheight for j = 1 : swidth for k = 1 : 3 r = max(i + ptop,1); r = min(r,oh); c = max(j + pleft,1); c = min(c,ow); crop_image(i,j,k) = I(r,c,k); end endend%图像大小调整 resize,双线性插值w_scale = (swidth-1)/w;%待变换宽尺度h_scale = (sheight-1)/h;%待变换高尺度resized_image = uint8(zeros(w,h,3));part = uint8(zeros(w,sheight,3));for i = 1 : 3 for j = 1 : sheight for k = 1 : w if k == w part(j,k,i) = crop_image(j,swidth,i); else sx = k*w_scale; ix = floor(sx); dx = sx-ix; part(j,k,i) = (1-dx)*crop_image(j,ix,i)+dx*crop_image(j,ix+1,i); end end endendfor i = 1 :3 for j = 1 : h sy = j*h_scale; iy = floor(sy); dy = sy - iy; for k = 1 : w resized_image(j,k,i) = (1-dy)*part(iy,k,i); end if j &lt; h for k = 1 : w resized_image(j,k,i) = resized_image(j,k,i)+dy*part(iy+1,k,i); end end endend%% 方式二 jittering+resizeing(随机)I = imread(filepath);%读取图像[oh,ow,~] = size(I);%读取原图宽高[dw,dh] = deal(ow*jitter,oh*jitter);%计算抖动值上限aspect_ratio = (ow-dw+2*dw*rand)/(oh-dh+2*dh*rand);%计算 jittering 后的长宽比scale = 0.25 + 1.75*rand;%对标准输入大小进行随机放缩，然后保证放缩后长宽比if aspect_ratio &lt; 1 nh = floor(scale*h); nw = floor(nh*aspect_ratio);else nw = floor(scale*w); nh = floor(nw/aspect_ratio);end[Dx,Dy] = deal(floor((w-nw)*rand),floor((h-nh)*rand));resized_image2 = uint8(0.5*ones(h,w,3));for c = 1 : 3 for y = 1 : nh for x = 1 : nw frx = x/nw*ow; fry = y/nh*oh; rx = floor(frx); ry = floor(fry); dx = frx - rx; dy = fry - ry; val = (1-dy)*(1-dx)*get_pixel(I,ry,rx,c)+dy*(1-dx)*get_pixel(I,ry+1,rx,c)+... (1-dy)*dx*get_pixel(I,ry,rx+1,c)+dy*dx*get_pixel(I,ry+1,rx+1,c); if x+Dx&gt;0&amp;&amp;x+Dx&lt;=w&amp;&amp;y+Dy&gt;0&amp;&amp;y+Dy&lt;=h resized_image2(y+Dy,x+Dx,c) = val; end end endendfigure(1)subplot(221);imshow(I);title('原图');subplot(222);imshow(crop_image);title('方式一 jittering');subplot(223);imshow(resized_image);title('方式一各向异性 resize');subplot(224);imshow(resized_image2);title('方式二 jittering+各向同性 resize');%% 翻转if randi([0 1]) J = fliplr(resized_image2);endfigure(2)imshow(J);%% HSV 空间扰动dhue = -hue+2*rand*hue;%随机化色调偏差if randi([0 1]) dsat = 1 + rand*(saturation-1);%随机化饱和度偏差else dsat = 1/(1 + rand*(saturation-1));endif randi([0 1]) dexp = 1 + rand*(exposure-1);%随机化曝光率偏差else dexp = 1/(1 + rand*(exposure-1));endJ = rgb2hsv(J);%将 RGB 空间转换到 HSV 空间temp = J(:,:,1)+dhue;%调整色调temp(temp&gt;1) = temp(temp&gt;1)-1;temp(temp&lt;0) = temp(temp&lt;0)+1;J(:,:,1) = temp;J(:,:,2) = J(:,:,2)*dsat;%调整饱和度J(:,:,3) = J(:,:,3)*dexp;%调整曝光率/亮度J = hsv2rgb(J);%返回 RGB 空间figure(3)imshow(J);title('HSV 空间扰动')function pixel = get_pixel(image,i,j,c)if i &lt; 1||i&gt;size(image,1)||j&lt;1||j&gt;size(image,2) pixel = uint8(0);else pixel = image(i,j,c);endend%%%Matlab 中使用工具箱函数 imresize 会更快：%imresize(I,[w,h],’bilinear’)%双线性插值%imresize(I,[w,h],’bicubic’)%双三次线性插值作者在 YOLO 和 YOLOv2 分别用了两种实现方法，第一种将 jittering 和 resize 分开了，采用双线性插值的方式，第二种则是将二者结合了，先在原图获取一定比例的亚像素值，然后再进行随机双线性插值，效果如下：可以看到，方式一的 jittering 会将边界像素进行复制扩充，并且不会内部像素会进行重排，所以是各向异性 resize。而方式二就好像在保证原始图像比例的前提下，通过填充 0 像素达到规定尺寸，所以是各向同性 resize。然后随机将图像进行左右翻转，最后就是 HSV 空间扰动，具体原理还是直接看代码，由于每次随机的值都不一样，所以下面的图可能与上面的不是一致的：3.4 训练技巧YOLO训练过程中采用了很多技巧，具体如下：采用Leaky Relu激活函数leaky(x) = \left\{ \begin{array}{l} x,\;\;\;\;\;x > 0\\ 0.1x,x \le 0 \end{array} \right.损失函数损失函数整体分为定位误差和分类误差（图中的中心位置x,y部分有错误），其中(1)第一部分表示当区域内存在目标，且也检测到了匹配目标的前提下，计算目标框中心的均方误差，定位权重为5；(2)第二部分就是当区域内存在目标，且也检测到了匹配目标的前提下，计算的目标框宽高的均方误差，定位权重为5，实际上作者训练的时候输出的就是宽高开平方后的结果。这里注意用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大。举个例子，原来 w=10，h=20，预测出来 w=8，h=22，跟原来 w=3，h=5，预测出来其实前者的误差要比后者下，但是如果不加开根号，那么损失都是一样：4+4=8，但是加上根号后，变成 0.15和 0.7；(3)第三部分是分别计算当区域内真实目标和与之匹配的预测目标同时存在和不同时存在的情况下，边框置信度的均方误差，其中${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}\over C} _i}$表示的是真实目标框与预测目标框的IOU值。然而，大部分边界框都没有物体，积少成多，造成loss的不平衡，所以同时存在状态下的分类权重为1，不同时存在状态下的分类权重为0.5；(4)第四部分计算的是当该区域存在目标时，计算目标类别均方误差。对于每个格子而言，作者设计只能包含同种物体。若格子中包含物体，我们希望希望预测正确的类别的概率越接近于1越好，而错误类别的概率越接近于0越好。其中对于预测目标和groundtruth的匹配，作者除了利用IOU进行匹配之外，对于无匹配对象的ground，则是取与其(x,y,w,h)均方误差最小的目标框作为匹配对象非极大值抑制非极大值抑制(NMS)算法是目标检测领域中不可或缺的一个算法，其主要功能在于目标去重。而主要依据在于目标框之间的IOU以及每个目标框的置信度，即保证在IOU较大的目标群中选择置信度最高的目标框作为该目标群唯一的预测输出。YOLO所采用的NMS算法流程如下：Step1 在网络输出结果之后，会得到7x7x2=98个目标框，首先会根据阈值将prob不合格(大概率属于背景)的目标框置信度置为0；Step2 对于每一个类别分开处理，先根据每个目标框在该类别下的prob置信度进行从大到小排序；Step3 对于排序好的第一个置信度不为0的目标框，依次计算与其他置信度不为0的目标框的IOU，如果IOU大于阈值，则将该目标框置信度置为0，且其对应的所有类别prob都置为0；Step4 转移至下一个置信度不为0的目标框，重复Step3，直到下一步没有置信度非0的目标框为止；Step5 重复Step2Step5 输出所有置信度非0的目标框，并根据阈值筛选有效目标。部分C代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647int nms_comparator(const void *pa, const void *pb)&#123; detection a = *(detection *)pa; detection b = *(detection *)pb; float diff = 0; if(b.sort_class &gt;= 0)&#123; diff = a.prob[b.sort_class] - b.prob[b.sort_class]; &#125; else &#123; diff = a.objectness - b.objectness; &#125; if(diff &lt; 0) return 1; else if(diff &gt; 0) return -1; return 0;&#125;void do_nms_sort(detection *dets, int total, int classes, float thresh)&#123; int i, j, k; k = total-1; for(i = 0; i &lt;= k; ++i)&#123; if(dets[i].objectness == 0)&#123; detection swap = dets[i]; dets[i] = dets[k]; dets[k] = swap; --k; --i; &#125; &#125; total = k+1; for(k = 0; k &lt; classes; ++k)&#123; for(i = 0; i &lt; total; ++i)&#123; dets[i].sort_class = k; &#125; qsort(dets, total, sizeof(detection), nms_comparator); for(i = 0; i &lt; total; ++i)&#123; if(dets[i].prob[k] == 0) continue; box a = dets[i].bbox; for(j = i+1; j &lt; total; ++j)&#123; box b = dets[j].bbox; if (box_iou(a, b) &gt; thresh)&#123; dets[j].prob[k] = 0; &#125; &#125; &#125; &#125;&#125;这里要注意的是YOLOv1中并没有用到softmax，所以可能同一个目标框的多个类别的置信度都很高。dropout为了减少过拟合概率，YOLOv1中采用的是dropout方式，即在第一个全连接层/局部连接层后利用dropout随机将特征图中的一部分特征丢失。关于dropout的实现，其主要依据的是drop_probability，即丢失比例/概率，不同框架的实现方式不同，例如caffe：1234567891011121314151617template &lt;typename Dtype&gt;void DropoutLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); unsigned int* mask = rand_vec_.mutable_cpu_data(); const int count = bottom[0]-&gt;count(); if (this-&gt;phase_ == TRAIN) &#123; // Create random numbers caffe_rng_bernoulli(count, 1. - threshold_, mask); for (int i = 0; i &lt; count; ++i) &#123; top_data[i] = bottom_data[i] * mask[i] * scale_; &#125; &#125; else &#123; caffe_copy(bottom[0]-&gt;count(), bottom_data, top_data); &#125;&#125;其在测试环节直接将输入映射到输出，而在训练环节则是利用伯努利分布按照1-drop_probability的比例选取特征图中的特征，得到一个mask，然后乘以一定比例：\left\{ \begin{array}{l} mask \sim Bernoulli\left( {1 - drop\_probability,N} \right)\\ {f_i} = {f_i} \times mas{k_i} \times \frac{1}{{1 - drop\_probability}} \end{array} \right.而在Darknet中，其实现方式是：1234567891011void forward_dropout_layer(dropout_layer l, network net)&#123; int i; if (!net.train) return; for(i = 0; i &lt; l.batch * l.inputs; ++i)&#123; float r = rand_uniform(0, 1); l.rand[i] = r; if(r &lt; l.probability) net.input[i] = 0; else net.input[i] *= l.scale; &#125;&#125;实现原理是一样的，不过其实现方式更加清晰，缺点在于需要重复生成很多无效随机数，以及没有向量加速。学习率YOLOv1采用的是multistep变化方式，即在特定的迭代次数更新学习率，论文中的YOLOv1采用的是学习率逐渐衰减的方式，但有意思的是最新的YOLOv1中学习率变化过程不同于传统的逐渐衰减方式，而是类似于当前新兴的warmup/warmrestart变化方式。我们知道传统的训练模式下：不断衰减的学习率可以稳定收敛，但是现在发现在某些模型训练过程中收敛效果并不好，所以就有研究者提出了“热重启”策略，当然也有类似的“循环”策略：那么在YOLOv1中的学习率设定是：1234learning_rate=0.0005policy=stepssteps=200,400,600,20000,30000scales=2.5,2,2,.1,.1可以看到，初始学习率只有0.0005，先慢慢增大，然后逐渐变小。3.5 测试效果先放一张原论文中的实验效果图：我们可以看到YOLOv1这个典型的one-stage目标检测算法，在速度大幅领先的前提下，只损失了7%的精度，并且值得注意的是YOLO相对于Fast RCNN将目标误识别为背景的概率小很多，所以作者做了一个小尝试，即以YOLO本身预测为主，逐个对比Fast RCNN和YOLO预测的目标框，根据置信度进行选择，也就是下面Fast R-CNN + YOLO的embedding组合。那么对于VOC各个类别的定位精度：其对于稀疏大目标的定位效果还是能接受的：3.6 优缺点YOLO的优点不用多说，其作为当时最快的实时目标检测算法横空出世，SSD是在其后出来的，正式打开了one-stage目标检测算法的大门，虽然精度仍然不如Faster RCNN等，但是其速度很高，适合工程界研究改造。当然其缺点也有很多：1.YOLOv1采用了7x7的网格划分模式，每个网格只能预测两个同类别的目标框，那么就无法预测密集场景下的目标位置，如：拥挤人群；2.YOLOv1的网格划分方式会影响每个目标的边界定位准确度，因为目标一般是跨网格区域的，如果目标只有一小部分在某个网格，那么可能就会被忽略；3.NMS本身漏洞，NMS会将相邻的目标框去重，那么就会出现下面的情况：另外，由于置信度和IOU并不是强相关的，那么对于下面的情况，则不得不选择更差的目标框：4.固定分辨率，由于YOLOv1中存在全连接层，所以输入的分辨率必须固定，那么对于YOLOv1所固定的448x448大小分辨率，很多大分辨率图像中的目标会变得很小，另外许多非正方形分辨率的图像目标会失真。4.YOLOv2YOLOv2也被称作YOLO9000，其相对于YOLOv1提升了很多，正如作者所说：Better、Faster、Stronger。其行文很容易理解，我们直接通过创新点来了解其与YOLOv1的区别。4.1 网络结构改变YOLOv2的网络结构做了较大的改变，主要有:Training for classfication—Darknet19YOLOv2在YOLOv1中的backbone网络基础上借鉴了VGG网络中的卷积方式，利用多个小卷积核替代大卷积核，并且利用1x1卷积核代替全连接层，这样做的好处的特征图每个位置共享参数，然后利用卷积核个数弥补参数组合多样性：123456789101112131415161718192021222324252627layer filters size input output 0 conv 32 3 x 3 / 1 256 x 256 x 3 -&gt; 256 x 256 x 32 0.113 BFLOPs 1 max 2 x 2 / 2 256 x 256 x 32 -&gt; 128 x 128 x 32 2 conv 64 3 x 3 / 1 128 x 128 x 32 -&gt; 128 x 128 x 64 0.604 BFLOPs 3 max 2 x 2 / 2 128 x 128 x 64 -&gt; 64 x 64 x 64 4 conv 128 3 x 3 / 1 64 x 64 x 64 -&gt; 64 x 64 x 128 0.604 BFLOPs 5 conv 64 1 x 1 / 1 64 x 64 x 128 -&gt; 64 x 64 x 64 0.067 BFLOPs 6 conv 128 3 x 3 / 1 64 x 64 x 64 -&gt; 64 x 64 x 128 0.604 BFLOPs 7 max 2 x 2 / 2 64 x 64 x 128 -&gt; 32 x 32 x 128 8 conv 256 3 x 3 / 1 32 x 32 x 128 -&gt; 32 x 32 x 256 0.604 BFLOPs 9 conv 128 1 x 1 / 1 32 x 32 x 256 -&gt; 32 x 32 x 128 0.067 BFLOPs 10 conv 256 3 x 3 / 1 32 x 32 x 128 -&gt; 32 x 32 x 256 0.604 BFLOPs 11 max 2 x 2 / 2 32 x 32 x 256 -&gt; 16 x 16 x 256 12 conv 512 3 x 3 / 1 16 x 16 x 256 -&gt; 16 x 16 x 512 0.604 BFLOPs 13 conv 256 1 x 1 / 1 16 x 16 x 512 -&gt; 16 x 16 x 256 0.067 BFLOPs 14 conv 512 3 x 3 / 1 16 x 16 x 256 -&gt; 16 x 16 x 512 0.604 BFLOPs 15 conv 256 1 x 1 / 1 16 x 16 x 512 -&gt; 16 x 16 x 256 0.067 BFLOPs 16 conv 512 3 x 3 / 1 16 x 16 x 256 -&gt; 16 x 16 x 512 0.604 BFLOPs 17 max 2 x 2 / 2 16 x 16 x 512 -&gt; 8 x 8 x 512 18 conv 1024 3 x 3 / 1 8 x 8 x 512 -&gt; 8 x 8 x1024 0.604 BFLOPs 19 conv 512 1 x 1 / 1 8 x 8 x1024 -&gt; 8 x 8 x 512 0.067 BFLOPs 20 conv 1024 3 x 3 / 1 8 x 8 x 512 -&gt; 8 x 8 x1024 0.604 BFLOPs 21 conv 512 1 x 1 / 1 8 x 8 x1024 -&gt; 8 x 8 x 512 0.067 BFLOPs 22 conv 1024 3 x 3 / 1 8 x 8 x 512 -&gt; 8 x 8 x1024 0.604 BFLOPs 23 conv 1000 1 x 1 / 1 8 x 8 x1024 -&gt; 8 x 8 x1000 0.131 BFLOPs 24 avg 8 x 8 x1000 -&gt; 1000 25 softmax 1000我们可以看到其中有19个卷积层和5个max pooling层，所以称其为Darknet19。作者利用该网络重新再Imagenet上训练了，相对于yolov1中的backbone网络，参数量更少，计算速度更快，效果更好。Training for Detection有了backbone骨干网络之后，作者剔除了Darknet19的最后一个卷积层，然后额外添加了几个卷积层：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[route]layers=-9[convolutional]batch_normalize=1size=1stride=1pad=1filters=64activation=leaky[reorg]stride=2[route]layers=-1,-4[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[convolutional]size=1stride=1pad=1filters=425activation=linear我们可以看到其中出现了两个新层reorg和route，这两个层的意义在于：可以也就是说，route的层的作用就是将选定层按照通道拼接在一起，而reorg层的作用就是将特征图均匀划分为 4 份，从而使得两组特征图可以拼接。大致原理如下：最终的网络结构为：1234567891011121314151617181920212223242526272829303132layer filters size input output 0 conv 32 3 x 3 / 1 416 x 416 x 3 -&gt; 416 x 416 x 32 1 max 2 x 2 / 2 416 x 416 x 32 -&gt; 208 x 208 x 32 2 conv 64 3 x 3 / 1 208 x 208 x 32 -&gt; 208 x 208 x 64 3 max 2 x 2 / 2 208 x 208 x 64 -&gt; 104 x 104 x 64 4 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 5 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 6 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 7 max 2 x 2 / 2 104 x 104 x 128 -&gt; 52 x 52 x 128 8 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 9 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 10 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 11 max 2 x 2 / 2 52 x 52 x 256 -&gt; 26 x 26 x 256 12 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 13 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 14 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 15 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 16 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 17 max 2 x 2 / 2 26 x 26 x 512 -&gt; 13 x 13 x 512 18 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 19 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 20 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 21 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 22 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 23 conv 1024 3 x 3 / 1 13 x 13 x1024 -&gt; 13 x 13 x1024 24 conv 1024 3 x 3 / 1 13 x 13 x1024 -&gt; 13 x 13 x1024 25 route 16 26 reorg / 2 26 x 26 x 512 -&gt; 13 x 13 x2048 27 route 26 24 28 conv 1024 3 x 3 / 1 13 x 13 x3072 -&gt; 13 x 13 x1024 29 conv 425 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 425 30 detection这里输出的425指的是：每个网格输出5个目标框，每个目标框包含COCO的80个类别概率，一个边框置信度，以及(tx,ty,tw,th)，即5x(80+1+4)。全卷积网络综上可知，YOLOv2是一个全卷积网络，与YOLOv1相同，利用感受野的概念，我们可以认为是将原图划分为了多个网格区域，且区域半径为32，所以说输入大小必须是32的倍数。另外全卷积网络的好处在于可以有任意分辨率的输入，因为全连接层参数依赖前后两层的尺寸，而卷积层参数只有卷积核，与前后层尺寸无关，所以更为方便了。4.2 batch normalization相对于YOLOv1，YOLOv2将dropout替换成了效果更好的batch normalization，在每个卷积层计算之前利用batch normalization进行批归一化：\left\{ \begin{array}{l} {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^k} = \frac{{{x^k} - E\left[ {{x^k}} \right]}}{{\sqrt {Var\left[ {{x^k}} \right]} }}\\ {y^k} = {\gamma ^k}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^k} + {\beta ^k} \end{array} \right.4.3 Multi-Scale Training为了让网络能适应不同分辨率的输入，在训练过程中，每个10个batches会随机选择一种分辨率输入，即利用图像插值对图像进行放缩，由于训练速度的要求以及分辨率必须是32倍数，所以训练过程中选择的分辨率分别为：320, 352, …, 608。4.4 Anchor boxesYOLOv2相对于YOLOv1的定位架构最大的改变在于剔除了anchor boxes概念，具体见第一章，从直接预测目标相对网格区域的偏移量到预测anchor box的修正量，有了先验长宽比的约束，可以减少很多不规则的目标定位。\left\{ \begin{array}{l} x = {t_x} \times {w_a} + {x_a}\\ y = {t_y} \times {h_a} + {y_a}\\ w = {t_w} \times img\_width\\ h = {t_h} \times img\_height \end{array} \right. \Rightarrow \left\{ \begin{array}{l} {b_x} = \sigma \left( {{t_x}} \right) + {c_x}\\ {b_y} = \sigma \left( {{t_y}} \right) + {c_y}\\ {b_w} = {p_w}{e^{{t_w}}}\\ {b_h} = {p_h}{e^{{t_h}}}\\ Pr\left( {object} \right) * IOU\left( {b,object} \right) = \sigma \left( {{t_o}} \right) \end{array} \right.其中，p代表的是anchor box的先验值，c表示每个网格区域的左上角顶点，t表示网络输出的目标框的5个参数(tx,ty,tw,th,to),b表示真实预测定位信息。而最后一个关于先验概率的等价关系我们可以知道，与YOLOv1相同，这里目标预测的边框置信度包含了IOU先验值。其中对于部分输出进行了logistic转换：\sigma \left( x \right) = \frac{1}{{1 + x}}对于先验anchor boxes的确定，作者通过 K-means 的方法对 VOC 和 COCO 数据集所有框的标签进行聚类，最后发现anchor box 在仅有 5 种 aspect ratio 的情况下就能达到足够的效果，当然，作者也试着将 K 提升到 9 个，发现效果更好。4.5 其他训练技巧softmaxYOLOv2中对于每个类别的概率输出进行了softmax归一化。学习率YOLOv2的学习率变化方式与YOLOv1类似：123456learning_rate=0.001burn_in=1000max_batches = 500200policy=stepssteps=400000,450000scales=.1,.1只不过多了一个burn_in参数，那么上面的参数设置对应的变化方式为：learning\_rat{e_{iter}} = \left\{ \begin{array}{l} base\_learningrate \times {\left( {iter/burn\_in} \right)^{power}},if\;iter < burn\_in\\ learningrat{e_{iter - 1}} \times scal{e_j},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;elif\;iter = step{s_j}\\ learningrat{e_{iter - 1}},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;otherwise \end{array} \right.损失函数YOLOv2的损失函数类似于YOLOv1，也有所不同，我阅读源码之后总结如下：\begin{array}{l} Loss=\\ {\lambda _{prior}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{iter < 12800}\left[ {{{\left( {\sigma ({t_{{x_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{y_{ij}}}}) - 0.5} \right)}^2} + {{\left( {\sigma ({t_{{w_{ij}}}}) - {p_{{w_k}}}} \right)}^2} + {{\left( {\sigma ({t_{{h_{ij}}}}) - {p_{{h_k}}}} \right)}^2}} \right]} } } \\ + {\lambda _{coord}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {\sum\limits_{k = 0}^A {l_{ij}^{obj}{{\left( {{x_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }_{ijk}}} \right)}^2} + {{\left( {{y_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }_{ijk}}} \right)}^2} + {{\left( {{w_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over w} }_{ijk}}} \right)}^2} + {{\left( {{h_{ijk}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over h} }_{ijk}}} \right)}^2}} } } \\ + {\lambda _{obj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over C} }_{ij}}} \right)^2} + {\lambda _{noobj}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{noobj}} } {\left( {{C_{ij}} - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over C} }_{ij}}} \right)^2}\\ + {\lambda _{class}}\sum\limits_{i = 0}^{{S^2}} {\sum\limits_{j = 0}^B {l_{ij}^{obj}\sum\limits_{c \in classes} {{{\left( {{p_{ij}}\left( c \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over p} }_{ij}}\left( c \right)} \right)}^2}} } } \end{array}可以发现只有定位误差部分的损失函数变化了，其中${\lambda {prior}=0.01},{\lambda {coord}=1},{\lambda {class}=1},{\lambda {obj}=5,{\lambda _{noobj}=1}}$，其中第一部分的意思是当训练的batch数不超过12800个时，尽量让预测目标框靠近每个网格中心，且尺寸与先验anchor box相同。4.6 联合训练分类和检测作者在论文最后提出，可以将分类和检测数据集放在一起训练网络，在遇到分类问题时，就只调用分类部分损失函数，否则调用检测分布损失函数。而对于两类数据集中存在的，类别相互包含的情况，作者则是剔除了Word Tree的概念：其原理实际上就是，预先构建好所有类别的关系树，然后利用联合概率分布和条件概率等，进行组合，相当于YOLO中对于分类概率和目标置信度的关系。4.7 测试效果YOLOv2在VOC和COCO上的测试结果如下：我们可以看到，此时的YOLOv2的效果已经与Faster RCNN以及新出现的one-stage算法SSD持平，不过YOLOv2依旧保持着遥遥领先的速度优势。4.8 优缺点YOLOv2相对来说在每个网格内预测了更多的目标框，并且每个目标框可以不用为同一类，而每个目标都有着属于自己的分类概率，这些使得预测结果更加丰富。另外，由于anchor box的加入，使得YOLOv2的定位精度更加准确。不过，其对于YOLOv1的许多问题依旧没有解决，当然那些也是很多目标检测算法的通病。那么随着anchor box的加入所带来的新问题是：anchor box的个数以及参数都属于超参数，因此会影响训练结果；由于anchor box在每个网格内都需要计算一次损失函数，然而每个正确预测的目标框才能匹配一个比较好的先验anchor，也就是说，对于YOLOv2中的5种anchor box，相当于强行引入了4倍多的负样本，在本来就样本不均衡的情况下，加重了不均衡程度，从而使得训练难度增大；由于IOU和NMS的存在，会出现下面的情况：我们可以看到，当两个人很靠近或重叠时，检测框变成了中间的矩形框，其原因在于对于两个候选框（红，绿），其中红色框可能更加容易受到目标1的影响，而绿色框会同时收到目标1和目标2的影响，从而导致最终定位在中间。然后由于NMS存在，其他的相邻的框则会被剔除。要想避免这种情况，就应该在损失函数中加入相关的判定。5.YOLOv35.1 Darknet-53YOLOv3中又提出了一种新的backbone网络——Darknet-53，其效果还是见第二章的表格图片。其架构如下：可以看到，新增了Residual模块，不同于原本的Resnet中的残差模块：我怎么感觉作者就是为了加深网络，所以才不得不引入残差模块的…可以看到明显的效果变化：5.2 网络多尺度输出YOLOv3增加了top down 的多级预测，解决了yolo颗粒度粗，对小目标无力的问题。可以看到，不仅在不同的感受野范围输出了三种尺度的预测结果，每种预测结果中每个网格包含3个目标框，一共是9个目标框。而且，相邻尺度的网络还存在着级联：DBL: conv+BN+Leaky relu。resn：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。concat：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。upsample：终于把原来的reorg改成了upsample，这里的upsample很暴力，很像克罗内克积，即：\left[ {\begin{array}{*{20}{c}} 1&2\\ 3&4 \end{array}} \right] \Rightarrow \left[ {\begin{array}{*{20}{c}} 1&1&2&2\\ 1&1&2&2\\ 3&3&4&4\\ 3&3&4&4 \end{array}} \right]可以看到每个输出的深度都是255，即3x(80+5)。这种多尺度预测的方式应该是参考的FPN算法。5.3 Anchor Boxes改进YOLOv2中是直接预测了目标框相对网格点左上角的偏移，以及anchor box的修正量，而在YOLOv3中同样是利用K-means聚类得到了9组anchor box，只不过YOLOv2中用的是相对比例，而YOLOv3中用的是绝对大小。那么鉴于我们之前提到的anchor box带来的样本不平衡问题，以及绝对大小可能会出现超出图像边界的情况，作者加入了新的判断条件，即对于每个目标预测结果只选择与groundtruth的IOU最大/超过0.5的anchor，不考虑其他的anchor，从而大大减少了样本不均衡情况。5.4 分类函数YOLOv3中取消了对于分类概率的联合分布softmax，而是采用了logistic函数，因为有一些数据集的中的目标存在多标签，而softmax函数会让各个标签相互抑制。5.5 测试效果YOLOv3的泛化性能更好了：在加入了多尺度预测之后，小尺度目标检测效果更好：与其他算法的对比效果如下：5.6 展望感觉YOLOv3的提升已经很大了，不过一些固有问题还是没有解决，有意思的是，在加入了多尺度预测后，拥挤场景下的目标检测效果更好了。不过基于anchor box的目标检测算法始终都有着瓶颈，寻求更好的出路才是最好的。参考资料http://lanbing510.info/2017/08/28/YOLO-SSD.htmlhttps://pjreddie.com/darknet/https://blog.csdn.net/m0_37192554/article/details/81092514https://blog.csdn.net/leviopku/article/details/82660381https://github.com/pjreddie/darknetRedmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. 2017.Redmon J, Farhadi A. YOLOv3: An Incremental Improvement[J]. 2018.]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剖析KCF]]></title>
    <url>%2F2019%2F02%2F04%2F%E5%89%96%E6%9E%90KCF%2F</url>
    <content type="text"><![CDATA[前言核相关滤波算法是单目标跟踪领域一个举足轻重的算法，而kernelized correlation filters(KCF)是其原始形态，下面我以一个小白的角度慢慢揭开其神秘面纱。1.岭回归理论推导岭回归的理论比较简单，类似于一个单层神经网络加上一个正则项，不同于支撑向量机中的结构风险最小化，岭回归更像是一个逻辑回归，是在保证误差风险最小的情况下尽量使得结构风险小。另外支撑向量机对于高维数据的训练比较快，因为它只取对分类有影响的 support 向量。不过在此处 KCF 的训练样本也不多，所以二者其实都可以，再加上 KCF 中岭回归还引用了对偶空间、傅里叶变换以及核函数，二者的差别就比较小了。​ 岭回归的算法形式如下：\mathop {\min }\limits_w \sum\limits_i {{{\left( {f\left( {{x_i}} \right) - {y_i}} \right)}^2} + \lambda {{\left\| w \right\|}^2}} \Leftrightarrow \mathop {\min }\limits_w \sum\limits_i {{{\left( {Xw - y} \right)}^2} + \lambda {{\left\| w \right\|}^2}}\tag{1-1}其中X 为特征矩阵，w 为权值，y 为样本标签/响应，其中每一项都采用了L2 范数的平方,即矩阵内所有元素的平方和。因此，该优化的关键在于求最优的 w，求解方法则是使用了最直接的拉格朗日乘子法：\begin{array}{l} \because L = {\left( {Xw - y} \right)^T}\left( {Xw - y} \right) + \lambda {w^T}w\\ \;\;\;\;\;\; = \left( {{w^T}{X^T} - {y^T}} \right)\left( {Xw - y} \right) + \lambda {w^T}w\\ \;\;\;\;\;\; = {w^T}{X^T}Xw - {y^T}Xw - {w^T}{X^T}y + {y^T}y + \lambda {w^T}w\\ \therefore \frac{{\partial L}}{{\partial w}} = 2{X^T}Xw - {X^T}y - {X^T}y + 2\lambda w\\ \;\;\;\;\;\;\;\; = 2\left( {{X^T}X + \lambda I} \right)w - 2{X^T}y\\ \;\;\;\;\;\;\;\; = 0\\ \therefore w = {\left( {{X^T}X + \lambda I} \right)^{ - 1}}{X^T}y \end{array}我们假设当前的权重W和输出y都是一维向量，则矩阵的求导公式满足:\begin{array}{l} \frac{{d\left( {XA} \right)}}{{dX}} = A,\frac{{d\left( {AX} \right)}}{{dX}} = {A^T},\frac{{d\left( {{X^T}A} \right)}}{{dX}} = A,\frac{{d\left( {{X^T}AX} \right)}}{{dX}} = \left( {A + {A^T}} \right)X,\\ \frac{{d\left( {{A^T}XB} \right)}}{{dX}} = A{B^T},\frac{{d\left( {{A^T}{X^T}B} \right)}}{{dX}} = B{A^T},\frac{{d\left( {{A^T}{X^T}XA} \right)}}{{dX}} = 2XA{A^T} \end{array}不过，由于后面要引入复频域空间，所以我们这里做一些微调:w = {\left( {{X^H}X + \lambda I} \right)^{ - 1}}{X^H}y\tag{1-2}其中，H 代表共轭转置，即在转置的同时将矩阵内所有元素变为其共轭形式，原因很简单：\left( {a + bi} \right)\left( {a - bi} \right) = {a^2} + {b^2}\tag{1-3}2. 循环矩阵2.1 循环矩阵的引入由于在目标跟踪中定位目标时如果采用循环移位的方式定位其中心，则需要采用循环的方式逐步判断，这样做太耗时，因此作者引入了循环矩阵。这样做的话，我们的待选目标框不用移动，直接将原图像矩阵循环移位。以一维矩阵为例：K = \left[ {\begin{array}{*{20}{c}} 0&1&0& \ldots &0\\ 0&0&1& \ldots &0\\ 0&0& \ldots &1&0\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1&0&0& \ldots &0 \end{array}} \right]\tag{2-1}矩阵的每一行相对上一行都向右移动了一位，这里举这个矩阵例子是有用意的，通过该矩阵的n阶形式，我们可以轻松的实现任意矩阵 X 的右移$XK^T$或者下移$KX$ ，如：X = C\left( x \right) = \left[ {\begin{array}{*{20}{c}} {{x_1}}&{{x_2}}& \ldots &{{x_n}}\\ {{x_n}}&{{x_1}}& \ldots &{{x_{n - 1}}}\\ \vdots & \vdots & \ddots & \vdots \\ {{x_2}}&{{x_3}}& \ldots &{{x_1}} \end{array}} \right]\tag{2-2}2.2 循环矩阵的转换循环矩阵本身是将循环移位的结果整合到了一个矩阵中，虽然可以将循环计算过程优化为矩阵运算，但对于图像这类二维矩阵，则会生成一个很大的循环矩阵，从而耗费内存。这里作者巧妙地引入了离散傅里叶变换(DFT)，将循环矩阵X等价为：X = Fdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){F^H}\tag{2-3}其中 F 与离散傅里叶变换中的矩阵有所差异，${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}\over x} }​$就是原矩阵的傅里叶变换，diag是将矩阵变为对角形式，后面会详细解释。先以一维矩阵为例来证明：Step1 定义循环矩阵 X 的多项式函数为：X = {f_X}\left( K \right) = {x_0}I + {x_1}K + {x_2}{K^2} + ... + {x_{n - 1}}{K^{n - 1}}\tag{2-4}这里先说明一下，单位矩阵 I 其实也是一个循环矩阵，而 $P^n $其实就是将矩阵I所有元素右移 n 个单位。Step2 求矩阵K的特征值和特征向量：\begin{array}{l} \because \left| {\lambda I - K} \right| = \left| {\begin{array}{*{20}{c}} \lambda &{ - 1}& \ldots &0\\ 0&\lambda & \ldots &0\\ \vdots & \vdots & \ddots & \vdots \\ { - 1}&0& \cdots &\lambda \end{array}} \right| = {\lambda ^n} + {\left( { - 1} \right)^{n + 1}} \times {\left( { - 1} \right)^n} = {\lambda ^n} - 1\\ \therefore {\lambda _k} = \cos \frac{{2\pi k}}{n} + i\sin \frac{{2\pi k}}{n} = {e^{\frac{{2\pi k}}{n}}},0 \le k \le n - 1\\ \because \left[ {\begin{array}{*{20}{c}} {{\lambda _k}}&{ - 1}& \ldots &0\\ 0&{{\lambda _k}}& \ldots &0\\ \vdots & \vdots & \ddots & \vdots \\ { - 1}&0& \cdots &{{\lambda _k}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} {{x_1}}\\ {{x_2}}\\ \vdots \\ {{x_n}} \end{array}} \right] = 0\\ \therefore {\lambda _k} = \frac{{{x_1}}}{{{x_0}}} = \frac{{{x_2}}}{{{x_1}}} = ... = \frac{{{x_n}}}{{{x_{n - 1}}}}\\ \therefore {x_n} = \lambda _k^{n - 1}{x_0}\\ \therefore {D_k} = {\left[ {\begin{array}{*{20}{c}} {\lambda _k^0}&{\lambda _k^1}& \cdots &{\lambda _k^{n - 1}} \end{array}} \right]^T} = {\left[ {\begin{array}{*{20}{c}} {{e^0}}&{{e^{\frac{{2\pi k}}{n}}}}& \cdots &{{e^{\frac{{2\pi k\left( {n - 1} \right)}}{n}}}} \end{array}} \right]^T}\\ \therefore 基础解系为： {D} = \left[ {\begin{array}{*{20}{c}} {{e^0}}&{{e^0}}& \cdots &{{e^0}}\\ {{e^0}}&{{e^{\frac{{2\pi }}{n}}}}& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {{e^0}}&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}} \end{array}} \right] \end{array}可以发现矩阵 K 的特征矩阵与 DFT 的变换矩阵 W 一致，再利用多项式矩阵的性质可知,循环矩阵 X 的特征值为${f_X}\left( {diag\left( {{\lambda _k}} \right)} \right)​$, 利用矩阵与其特征值矩阵相似的特点, 可以很容易的证明该性质。Step3 求循环矩阵 X 的特征值和特征向量：\begin{array}{l} \because diag\left( {{\lambda _x}} \right)\\ = {f_X}\left( {diag\left( {{\lambda _k}} \right)} \right)\\ = {x_0} + {x_1}\left[ {\begin{array}{*{20}{c}} {{e^0}}&0& \cdots &0\\ 0&{{e^{\frac{{2\pi }}{n}}}}& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\ 0&0& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}} \end{array}} \right] + ... + {x_{n - 1}}\left[ {\begin{array}{*{20}{c}} {{e^0}}&0& \cdots &0\\ 0&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\ 0&0& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}} \end{array}} \right]\\ = \left[ {\begin{array}{*{20}{c}} {\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi 0}}{n}i}}{x_i}} }&0& \cdots &0\\ 0&{\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi 1}}{n}i}}{x_i}} }& \cdots &0\\ \vdots & \vdots & \ddots & \vdots \\ 0&0& \cdots &{\sum\limits_{i = 0}^{n - 1} {{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}i}}{x_i}} } \end{array}} \right]\\ \therefore {\lambda _x} = \left[ {\begin{array}{*{20}{c}} {{e^0}}&{{e^0}}& \cdots &{{e^0}}\\ {{e^0}}&{{e^{\frac{{2\pi }}{n}}}}& \cdots &{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}\\ \vdots & \vdots & \ddots & \vdots \\ {{e^0}}&{{e^{\frac{{2\pi \left( {n - 1} \right)}}{n}}}}& \cdots &{{e^{\frac{{2\pi {{\left( {n - 1} \right)}^2}}}{n}}}} \end{array}} \right]\left[ {\begin{array}{*{20}{c}} {{x_0}}\\ {{x_1}}\\ \vdots \\ {{x_{n - 1}}} \end{array}} \right] = Dx \end{array}可以发现循环矩阵 X 的特征值就是其原矩阵 x 的离散傅里叶变换，对于循环矩阵的特征向量，推导过程如下:\begin{array}{l} \because {K^{n - 1}}{D_k} = {\lambda _k}{K^{n - 2}}{D_k} = ... = {\lambda _k}^{n - 1}{D_k}\\ \therefore {K^{n - 1}}的特征值是{\lambda _k}^{n - 1}，而特征向量不变\\ \therefore X的特征向量同样是{D_k} \end{array}在这里我们将 $D_k $替换为 DFT 变换矩阵W ，利用矩阵对角化可知：X = Wdiag\left( {{\lambda _x}} \right){W^{ - 1}} = Wdiag\left( {Wx} \right){W^{ - 1}} = Wdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){W^{ - 1}}\tag{2-5}Step4 利用 DFT 变换矩阵 W 的性质修正 X :通过观察可知 W 为对称矩阵，另外也可以轻松证明${W^H}W = W{W^H} = nI​$，在这里呢，我们可以对 W 进行适当地变换：F = \frac{1}{{\sqrt n }}W\tag{2-6}因此${F^H}F = F{F^H} = I​$，则 F 为酉矩阵，同时它也满足 ${F^H} = {F^{-1}}​$，如果我们将之前的 W 替换为 F ，那么：X = \frac{1}{{\sqrt n }}Wdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right)\sqrt n {W^{ - 1}} = X\tag{2-7}所以整体来看， X 保持不变，综上可得公式(2-3)。2.3 二维循环矩阵上述推导都是基于一维矩阵进行的，那么对于二维矩阵的循环矩阵则是要进行两次一维的循环矩阵变换，下面简要介绍一下方法，完整的我不会推导~对于 m×n 的矩阵 x，其循环矩阵是将其当作块矩阵，矩阵每一行的元素都是前一个元素向下平移得来的，每一列都是向右平移得来的。因此其循环矩阵大小为 mn×mn，这主要是为了将循环矩阵 X 变为方阵，这样才能求其特征值和特征向量。当然，如果原矩阵 x 本身就是方阵，那么则不必这样做，可以将行向量改为右移，列向量改为下移，这样就符合观察习惯，这也是论文代码所采用的方式。先将 n×n 大小的矩阵 x 每一行向量单独看作一个 1×n 的块矩阵，按照一维循环矩阵$Kx​$的逻辑去做，不断下移，可得一个 $n^2×n​$的块矩阵。然后再将每一列向量单独看作一个$ n^2×1​$ 的块矩阵,按照 $xK^T​$的方式，不断右移，最后可得一个 ${n^2} × {n^2}​$的块矩阵。其中二维的 DFT 变换方式为 $WXW^H​$ 。3. 循环矩阵与岭回归算法的结合建立了循环矩阵 X 之后，如果判定其第(i, j)处的块矩阵处响应最大，即目标框相对前一个目标框向下偏移 i-1 个单位, 向右偏移 j-1 个单位。那么标签 y的大小也就是 n×n。将其与岭回归算法结合之后可以得到:\begin{array}{l} w = {\left( {{X^H}X + \lambda I} \right)^{ - 1}}{X^H}y\\ \;\;\; = {\left( {Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H} \cdot Fdiag\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} } \right){F^H} + \lambda Fdiag\left( \delta \right){F^H}} \right)^{ - 1}}{X^H}y\\ \;\;\; = Fdiag\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} + \lambda \delta }}} \right){F^H}Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H}y\\ \;\;\; = Fdiag\left( {\frac{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} + \lambda \delta }}} \right){F^H}y \end{array}其中，${{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}$表示 x 经 DFT 变换之后的共轭形式，$\delta $表示全 1 向量，其等同于单位矩阵的特征向量，$\odot $表示矩阵元素点乘。根据DFT时域卷积的性质：x\left( n \right) \otimes y\left( n \right) \leftrightarrow F\left( x \right)F\left( y \right)\tag{3-1}而时域卷积常用的是循环卷积，即将原序列看作一个周期，通过验证可以得到：x\left( n \right) \otimes y\left( n \right) = C{\left( x \right)^T}y\tag{3-2}可以发现${X^H} = C{\left( x \right)^H} = Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}} \right){F^H}​$,因此可得：F\left( {C{{\left( x \right)}^T}y} \right) = \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} \tag{3-3}利用上面的结论可以继续转换w为：Fw = \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over w} = F{F^H}{\left( {\frac{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*}}}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} + \lambda \delta }}} \right)^*} \odot Fy = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} + \lambda \delta }}\tag{3-4}至此，权重矩阵 w 的求解在傅式空间变成了简单的点乘运算，运算复杂度大幅降低。4. 对偶空间的引入对偶空间的具体意义我只有一个模糊的概念，之前在运筹学中学习的时候就感觉对偶空间像是从另外一个角度分析优化问题，比方说岭回归中的权值矩阵w，它的目的是完成 X 到 y 的映射，如果将循环矩阵拉伸为多个行向量，即$n^2$个 n×n 的样本，则更直接一点就是完成从$ n^2$维空间到 1 维空间的维度转换。那么对偶空间呢？对偶空间所要考虑的就是那 $n^2$ 个样本对于问题的影响，而这个影响因子，在优化问题中常常作为约束惩罚项的系数，然后分别权衡约束对于各个样本的影响。4.1 优化角度分析原优化问题为：\mathop {\min }\limits_w {\sum\limits_{i = 1}^n {\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}} } \right)} ^2} + \lambda \sum\limits_{j = 1}^p {w_j^2} \tag{4-1}其等价为：\begin{array}{l} \mathop {\min }\limits_w \sum\limits_{i = 1}^n {{\xi _i}^2} + \lambda \sum\limits_{j = 1}^p {w_j^2} \\ s.t.\;\;{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}} = {\xi _i} \end{array}\tag{4-2}采用惩罚函数的方式是：\mathop {\min }\limits_w \sum\limits_{i = 1}^n {{\xi _i}^2} + \lambda \sum\limits_{j = 1}^p {w_j^2} + \sum\limits_{i = 1}^n {{\alpha _i}\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}{w_j}} - {\xi _i}} \right)} \tag{4-3}可以发现，如果将${\xi _i}$ 看作${w_j}$经过一定线性变换之后在对偶空间的表现形式，则这里将其看作一个单独的变量，利用拉格朗日乘子法可得：\left\{ \begin{array}{l} \frac{{\partial L}}{{\partial {w_j}}} = 2\lambda {w_j} - \sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}} = 0 \Rightarrow {w_j} = \frac{1}{{2\lambda }}\sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}} \\ \frac{{\partial L}}{{\partial {\xi _i}}} = 2{\xi _i} - {\alpha _i} = 0 \Rightarrow {\xi _i} = \frac{1}{2}{\alpha _i} \end{array} \right.\tag{4-4}将其带入原目标函数可得：\begin{array}{l} \mathop {\min }\limits_\alpha \frac{1}{4}\sum\limits_{i = 1}^n {{\alpha _i}^2} + \frac{1}{{4\lambda }}\sum\limits_{j = 1}^p {\left( {\sum\limits_{i = 1}^n {{\alpha _i}{x_{ij}}} } \right)\left( {\sum\limits_{k = 1}^n {{\alpha _k}{x_{kj}}} } \right)} + \sum\limits_{i = 1}^n {{\alpha _i}\left( {{y_i} - \sum\limits_{j = 1}^p {{x_{ij}}\frac{1}{{2\lambda }}\sum\limits_{k = 1}^n {{\alpha _k}{x_{kj}}} - \frac{{{\alpha _i}}}{2}} } \right)} \\ \Leftrightarrow \mathop {\min }\limits_\alpha \sum\limits_{i = 1}^n {{\alpha _i}{y_i}} - \frac{1}{4}\sum\limits_{i = 1}^n {{\alpha _i}^2} - \frac{1}{{4\lambda }}\sum\limits_{i = 1}^n {\sum\limits_{k = 1}^n {{\alpha _i}{\alpha _k}} } \sum\limits_{j = 1}^p {{x_{ij}}{x_{kj}}} \end{array}由此可得原优化问题的对偶问题，将其转换成矩阵形式为：\mathop {\min }\limits_\alpha {\alpha ^T}y - \frac{1}{4}{\alpha ^T}\alpha - \frac{1}{{4\lambda }}{\alpha ^T}G\alpha \tag{4-5}其中 G 表示$\left\langle {{x_i},{x_j}} \right\rangle = {x_{i \cdot }}^T{x_{k \cdot }}​$，利用拉格朗日乘子法可得：\frac{{\partial Q}}{{\partial \alpha }} = y - \frac{1}{2}\alpha - \frac{1}{{2\lambda }}G\alpha \Rightarrow \alpha = 2\lambda {\left( {G + \lambda I} \right)^{ - 1}}y\tag{4-6}再将此对偶空间的最优解带入公式（4-4）可得：w = \frac{1}{{2\lambda }}{X^T}\alpha = {X^T}{\left( {G + \lambda I} \right)^{ - 1}}y\tag{4-7}4.2 矩阵变换角度上面的优化方法更侧重于从理论源头出发，而如果真的要用的话，可以直接用上面的理论，因此呢我们可以直接对矩阵进行变换：\begin{array}{l} \because\left( {{X^T}X + \lambda I} \right)w = {X^T}y\\ \therefore w = {\lambda ^{ - 1}}\left( {{X^T}y - {X^T}Xw} \right) = {\lambda ^{ - 1}}{X^T}\left( {y - Xw} \right) - {X^T}\alpha \\ \therefore \alpha = {\lambda ^{ - 1}}\left( {y - Xw} \right)\\ \therefore \lambda \alpha = y - X{X^T}\alpha \\ \therefore \alpha = {\left( {{X^T}X + \lambda I} \right)^{ - 1}}y = {\left( {G + \lambda I} \right)^{ - 1}}y \end{array}4.3 新样本测试训练好参数之后，当引入新样本时，可以直接利用 wx 的方式求出响应 y,根据公式（4-7）可得：f\left( z \right) = Z{X^T}{\left( {G + \lambda I} \right)^{ - 1}}y = {y^T}{\left( {G + \lambda I} \right)^{ - 1}}X{Z^T} = {y^T}{\left( {{G_{xx}} + \lambda I} \right)^{ - 1}}{G_{xz}}\tag{4-8}5.核函数的高维映射5.1核函数的引入核函数的引入主要是为了减少线性不可分问题的影响，因为岭回归跟神经网络或者深度学习不同，它是单层结构，需要从空间映射来着手。常见的核函数有线性函数、多项式核函数和高斯核函数（RBF），而论文中则是采用了 RBF 核函数，理论上来讲是映射到了无穷维数的空间，可以通过展开其级数得知。如图所示，黑色和蓝色区域明显是一个线性不可分的，而利用一个二次函数却能完美分割，这就是核函数的意义。本文所使用的核函数是高斯核函数：K = {e^{ - \frac{{{{\left( {x - \mu } \right)}^2}}}{{2{\sigma ^2}}}}}\tag{5-1}论文中将核函数引入样本的点积，即：f\left( z \right) = {y^T}{\left( {{K^{xx}} + \lambda I} \right)^{ - 1}}{K^{xz}} = {\alpha ^T}{K^{xz}}\tag{5-2}5.2 核函数与循环矩阵的结合对于任何的循环矩阵 X，还是以一维的原矩阵 x 为例，可知：K_{ij}^{xx} = K\left( {{P^i}x,{P^j}x} \right) = K\left( {{P^{ - i}}{P^i}x,{P^{ - i}}{P^j}x} \right) = K\left( {x,{P^{j - i}}x} \right) = K\left( {x,{P^{\left( {j - i} \right)\;\bmod \;n}}x} \right)\tag{5-3}其中由于 P 是循环矩阵的基础变换矩阵，也就是前面所提到的 K 矩阵，这里是为了与核函数区分开来，在矩阵点积中，两个矩阵的元素对应相乘，因此两个矩阵同时移位$ P^i​$，并不会影响结果。由公式（5-3）可知，只要行号和列号的差值相同，其对应元素的值就相同，所以 $K^{xx}​$是循环矩阵。再利用循环矩阵的特性（2-3）可得：\begin{array}{l} \because \alpha = {\left( {{K^{xx}} + \lambda I} \right)^{ - 1}}y\\ \;\;\;\;\;\; = {\left( {Fdiag\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}}} \right){F^H} + \lambda I} \right)^{ - 1}}y\\ \;\;\;\;\;\; = Fdiag\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }}} \right){F^H}y\\ \therefore F\alpha = F{F^H}{\left( {\frac{1}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }}} \right)^*} \odot Fy\\ \therefore \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{x{x^*}}} + \lambda }}\\ \because k_i^{xx} = K\left( {{x_0},{x_i}} \right),k_{n - i}^{xx} = K\left( {{x_0},{x_{n - i}}} \right) = K\left( {{x_0},{x_i}} \right)\\ \therefore {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{x{x^*}}} = {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}}\\ \therefore \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } = \frac{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over y} }}{{{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} }^{xx}} + \lambda }} \end{array}同理可得：\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over f} \left( z \right) = {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over k} ^{xz}} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } \tag{5-4}虽然原论文关于w的推导错误了，但是代码是根据$\alpha $来实现的，所以正确。5.3 不同核函数的计算从最基础的内积出发，其核函数形式就是：{K^{xz}} = {X^T}z = C\left( x \right){z^T} = {F^{ - 1}}{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)^T}\tag{5-5}对于多项式核，则有：{K^{xz}} = {\left( {{X^T}z + a} \right)^b} = \left[ {{F^{ - 1}}{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)}^T} + a} \right]\tag{5-6}对于 RBF 径向基核，也就是常说的高斯核函数，有：\begin{array}{l} \because k_i^{xz} = h\left( {{{\left\| {{x_i} - z} \right\|}^2}} \right) = h\left( {{{\left\| {{x_i}} \right\|}^2} + {{\left\| z \right\|}^2} - 2x_i^Tz} \right)\\ \therefore {k^{xz}} = h\left( {{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{X^T}z} \right)\\ \;\;\;\;\;\;\;\; = h\left( {{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{F^{ - 1}}\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)} \right)\\ \;\;\;\;\;\;\;\; = {e^{ - \frac{{{{\left\| x \right\|}^2} + {{\left\| z \right\|}^2} - 2{F^{ - 1}}\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^*} \odot \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over z} } \right)}}{{{\sigma ^2}}}}} \end{array}6. 模板图像的获取模板图像是基于第一帧图像目标框所得到的，其具体获取过程如下：Step1 保持初始目标框中心不变，将目标框的宽和高同时扩大相同倍数（论文中取 2.5 倍）；Step2 设定模板图像尺寸为 96，计算扩展框与模板图像尺寸的比例：scal{e_z} = \frac{{max\left( {w,h} \right)}}{{template}}\tag{6-1}Step3 然后将 scale 同时应用于宽和高，获取图像提取区域：ro{i_{w,h}} = \left( {\frac{w}{{scal{e_z}}},\frac{h}{{scal{e_z}}}} \right)\tag{6-2}Step4 由于后面提取 hog 特征时会以 cell 单元的形式提取，另外由于需要将频域直流分量移动到图像中心，因此需保证图像大小为 cell大小的偶数倍，另外，在 hog 特征的降维的过程中是忽略边界 cell 的，所以还要再加上两倍的 cell 大小：ro{i_{w,h}} = {\left[ {\frac{{ro{i_{w,h}}}}{{2cell\_size}}} \right]_{floor}} \cdot 2cell\_size + 2cell\_size\tag{6-3}Step5 由于 roi 区域可能会超出原图像边界，因此超出边界的部分填充为原图像边界的像素；Step6 最后利用线性插值的方式将 roi 区域采样为 template 大小。7.特征提取7.1 f-hog特征hog 特征又叫做方向梯度直方图，顾名思义，它所描述的是图像各像素点的方向梯度它对图像几何的和光学的形变都能保持很好的不变性。而论文中所用的hog 特征与常规的不同，具体的不同和实施细节我会在下文详细介绍：Step1 梯度幅值计算。计算模板图像 RGB 三通道每个像素点的水平梯度 dx 和垂直梯度 dy，并计算各点的梯度幅值，以最大的梯度幅值所在通道为准：a = \mathop {max}\limits_{i \in \left( {r,g,b} \right)} \left( {\sqrt {{{\left( {d{x_i}} \right)}^2} + {{\left( {d{y_i}} \right)}^2}} } \right)\tag{7-1}Step2 梯度方向判定。如果像素点最大幅值所在通道为 m，则利用该通道的水平梯度和垂直梯度计算该点的梯度方向，论文中将[0,180)分为了 9 个方向，同时还将[0,360)分为了18 个方向，具体方向归属则是利用该像素点梯度在模板方向上的投影值确定：\theta = \mathop {max}\limits_\theta \left( {dx\cos \theta + dy\sin \theta } \right)\tag{7-2}分别通过以[0,180 ) 和[0,360 )为周期将个像素点的方向投影至这两个区间，从而每个像素点都有两种方向；Step3 cell的分割。确定 cell 单元尺寸（论文中设为 4），因此，水平方向有 sizeX=24,竖直方向也有 sizeY =24 个 cell，由于在计算像素点梯度时，边界像素点的梯度都是通过镜像幅值边界像素的方式计算的，因此论文中并未考虑边界点的梯度。每个 cell中的方向梯度直方图应该有 9+18 = 27 个方向。Step4 cell 内像素点梯度幅值加权方式。论文代码中关于 cell 的方向梯度直方图的求解很特别。对于每个 cell，根据其尺寸，设计了两个离散序列，因为比较有规律，所以当做两个函数 x 和 y：通过图像可以看到，随着 cell 内像素点横纵坐标的偏移，对应的点的 x,y 值一直在变化，且两个函数关于 0.5 对称，另外 x+y=1，利用这一特性，可以利用x 和 y 进行组合分解：1 = {\left( {x + y} \right)^2} = {x^2} + xy + yx + {y^2}\tag{7-3}利用公式（7-3），论文代码中将每个 cell 等分为 4 部分（左上、右上、左下和右下），每一部分都是由包含该 cell 在内的相邻 4 个 cell 的同一部分加权平均得来的，其权重即为公式（7-3）所示的四个部分。具体组合方式如下（以 cell左上角部分为例）：可以发现，cell 左上部分的加权方式是以自身为左上角，然后取相邻的其他三个 cell 的左上部分组成一个新的虚 拟 cell，再利用上图 所示的权重分布将四个cell 的左上部分进行加权平均，权重正是 $x^2​$,xy,yx,$y^2​$。同理，cell 的其他三个部分也是一样的原理，比如 cell 的右下部分，则是将该 cell 的右下部分作为虚拟 cell的右下角，然后分别取该 cell 左上，正上，正左三个方向相邻的 cell 的右下部分组成新的虚拟 cell，再进行加权平均。当然，对于边界 cell，则只选取不超过边界的部分 cell 进行不完整加权。Step5 方向梯度直方图计算。对于 cell 内每个像素点，将其梯度幅值分别以[0,180 )和[0,360 ) 两种投影区间累加至对应梯度方向直方图中，在按照上一步中提到的加权方式计算完 cell特征之后，每个 cell 保留了 9+18 个方向的梯度。Step6 相对领域归一化及截断。对于每个 cell，分别取包含其在内的相邻四个 cell，如下图所示（好丑-_-||，为了区分四个 cell 的不同，尽力了。。。）因此有四个组合方式，每种组合方式都取该组合方式内四个 cell 的方向梯度直方图的前 9 个方向梯度的 L2 范数 val，然后用该 cell 内 27 个方向的梯度直方图除以 val，即可得到规范化之后的 hog 特征。四个组合可以得到四组 hog 特征，即 9+9+9+9+18+18+18+18=108 个方向。可以发现边界 cell 无法得到这么多方向，因此去掉边界 cell。所以sizeX=24-2=22，sizeY=22。Step7 PCA降维。作者从大量各种分辨率的图片中收集了很多36维特征（按照之前的定义），并且在这些特征上进行了PCA分析，发现了了一个现象：由前11个主特征向量定义的线性子空间基本包含了hog特征的所有信息。并且用降维之后的特征在他们的任务（目标检测）中取得了和用36维特征一样的结果。如果用 $C_{ij}​$表示第 i 组 hog 特征的第 j 个方向，则原作者代码中的降维方式分别如下：\left\{ \begin{array}{l} {f_1}\left( j \right) = \frac{{\sum\limits_{i = 1}^4 {{C_{ij}}} }}{{\sqrt 4 }}\\ {f_2}\left( i \right) = \frac{{\sum\limits_{j = 1}^{18} {{C_{ij}}} }}{{\sqrt {9 \times 2} }} \end{array} \right.\tag{7-4}然后将两种降维方式得到的特征进行组合，得到 27+4=31 组特征。原论文流程示意图如下：7.2 CN/CN2特征该颜色特征将颜色空间划分为了黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄共11种，然后将其投影至10维子空间的标准正交基上，这里作者给出了32768种颜色向量组合。再利用PCA技术，采用奇异值分解方式提取其中主要的两个颜色作为最终的特征。由于CN2特征是单目标跟踪领域中很有效的一种人工特征，这里对其原理做出详细描述：Step1 根据如下计算公式，作者给出了模板矩阵w2c（32768×10）：Index\_img = 1 + \left[ {\frac{R}{8}} \right] + 32 \times \left[ {\frac{G}{8}} \right] + {32^2} \times \left[ {\frac{B}{8}} \right]\tag{7-5}将原图的RGB三通道数据带入其中，图中每个像素位置都能得到一个索引号，范围是1~32768，将此索引号带入w2c中，便可得到一个宽高与原图一致，通道数为10的CN矩阵x_pca，与此同时，将其形状重塑为（W×H）×10的二维矩阵；Step2 逐帧更新外观矩阵：z\_pca = \left( {1 - learning\_rate} \right) \times z\_pca + learning\_rate \times x\_pca\tag{7-6}Step3 开始PCA，先按列对矩阵去中心化，并计算协方差矩阵cov（10×10）：\left\{ \begin{array}{l} dat{a_{ij}} = z\_pc{a_{ij}} - \mathop {mean}\limits_{i \in \left[ {1,WH} \right]} (z\_pc{a_{ij}})\\ cov = \frac{1}{{HWC - 1}} \times dat{a^T} \times data \end{array} \right.\tag{7-7}Step4 进行奇异值分解，由于对于任何矩阵$A \in {R^{m \times n}}$都可以利用奇异值分解为$A = UD{V^H}$, 其中$U \in {R^{m \times m}}$,$D \in {R^{m \times n}}$,$V \in {R^{n \times n}}​$,而对于矩阵cov，U=V,那么AU=UD,所以U的列向量是协方差矩阵的特征向量，D是协方差矩阵的特征值：[U,D,U] = svd\left( {cov} \right)\tag{7-8}Step5 取U的前2列特征向量，逆分解得到新的协方差矩阵：\left\{ \begin{array}{l} old\_cov = \left( {1 - cp\_rate} \right) \times old\_cov + cp\_rate \times cp\_{\mathop{\rm cov}} \\ M = U\left( {:,1:2} \right),N = D\left( {1:2,1:2} \right)\\ cp\_{\mathop{\rm cov}} = M \times N \times {M^T} \end{array} \right.\tag{7-9}Step6 更新协方差矩阵：cov = \left( {1 - cp\_rate} \right) \times cp\_cov + cp\_rate \times {\mathop{\rm cov}} \tag{7-10}Step7 得到CN2特征：CN2 = reshape(x\_pca \times M,(H,W,2))\tag{7-11}下图可以看见原图，灰度图以及CN2特征图的区别，因为CN特征有10个通道，这里我就不放了：8. 算法实现8.1 多通道图像特征矩阵求解利用上述理论推导可以求得每一帧 fhog 特征，如果将其视为多通道的话，那么就是 31 通道的图像特征矩阵，然后分别对各通道使用二维汉宁窗进行滤波，为了降低 FFT 过程带来的频谱泄露，其函数形式如下：f\left( {x,y} \right) = \frac{1}{4}\left( {1 - \cos \left( {2\pi \frac{x}{n}} \right)} \right)\left( {1 - \cos \left( {2\pi \frac{y}{n}} \right)} \right)\tag{8-1}然后分别对各个通道求解其\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x}，对各通道数据进行多通道处理，处理方式如下:\begin{array}{l} {k^{xx'}} = g\left( {C\left( x \right)x'} \right)\\ \;\;\;\;\;\; = g\left( {\sum\limits_{c = 1}^{L = 31} {C\left( {{x^c}} \right)x'} } \right)\\ \;\;\;\;\;\; = g\left( {\sum\limits_{c = 1}^L {{F^{ - 1}}\left( {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} \right)} } \right)\\ \;\;\;\;\;\; = g\left( {{F^{ - 1}}\left( {\sum\limits_{c = 1}^L {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} } \right)} \right)\\ \;\;\;\;\;\; = {e^{ - \frac{{{{\left\| x \right\|}^2} + {{\left\| {x'} \right\|}^2} - 2{F^{ - 1}}\left( {\sum\limits_{c = 1}^L {{{\left( {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}} \right)}^*}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over x} }^c}'} } \right)}}{{{\sigma ^2}}}}} \end{array}8.2 标签制作利用第五章所得结论，可以求得${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}\over \alpha } }$。对于 groundtruth，由于模板函数的中心就是目标框的中心，因此论文中使用高斯分布函数作为标签,其分布函数如下：\left\{ \begin{array}{l} g\left( {x,y} \right) = {e^{ - \frac{{{{\left( {i - cx} \right)}^2} + {{\left( {j - cy} \right)}^2}}}{{2{\sigma ^2}}}}}\\ \sigma = \frac{{\sqrt {sizeX \cdot sizeY} }}{{padding \cdot output\_\sigma }} \end{array} \right.\tag{8-2}其中,(cx,cy)表示图像特征矩阵中心，padding 表示扩展框相对目标框的变化比例 2.5，${output_\sigma }​$表示设定的一个值 0.125。8.3 多尺度检测论文代码中，作者设置了三种尺度，设定尺度步长为 scalestep=1.05，然后分别以 $1.05^{-1}$,$1.05^0$,$1.05^1$三种尺度 scale 进行检测，这里尺度的操作对象是第六章所提到的 roi 矩阵，即roi_{w,h}=scale*roi_{w,h}。首先，为了防止在更新过程中目标框左上角位于图像边界，从而目标框变成了一条线或者一个点，作者将这一类目标框的左上角向远离图像边界的方向移动了一个单位。每一尺度都能求得 f(z)响应矩阵，虽然该响应矩阵对应了循环矩阵每个块矩阵的响应，但是第 i 行第 j 列的块矩阵所对应的响应，正是目标框右移 i-1 个单位，下移 j-1 个单位后的响应，即下一帧图像矩阵 z 的响应。对于该响应矩阵，找出其最大响应值 peakvalue 和最大响应位置 $p{xy}​$。如果最大响应位置不在图像边界，那么分别比较最大响应位置两侧的响应大小，如果右侧比左侧高，或者下侧比上侧高，则分别将最大响应位置向较大的一侧移动一段距离：\left\{ \begin{array}{l} {p_x} = {p_x} + 0.5\frac{{right - left}}{{2peak\_value - right - left}}\\ {p_y} = {p_y} + 0.5\frac{{down - up}}{{2peak\_value - down - up}} \end{array} \right.\tag{8-3}然后计算此位置与图像中心的距离 res。对于不同的尺度，都有着尺度惩罚系数 scale_weight,用此系数乘以该尺度下的最大响应值作为该尺度下的真实最大响应值，取最大响应值对应的尺度为最佳尺度，记为 best_scale。以此来更新目标框参数 T（x,y,w,h）：\left\{ \begin{array}{l} scal{e_z} = scal{e_z} \cdot best\_scale\\ {T_{w,h}} = {T_{w,h}} \cdot best\_scale\\ {T_{x,y}} = {T_{x,y}} - {T_{w,h}}/2 + re{s_{x,y}} \cdot cell\_size \cdot best\_scale \end{array} \right.\tag{8-4}8.4 模板更新首先获取原尺度下当前帧的 fhog 特征矩阵 z，作者代码中这一部分没有用汉宁窗进行滤波，可能是考虑到更新时这一部分的权重不大，其模板和${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}\over \alpha } }​$ 的更新公式如下：\left\{ \begin{array}{l} template = \left( {1 - 0.012} \right) \times template + 0.012z\\ \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } = \left( {1 - 0.012} \right) \times \mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } + 0.012{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over \alpha } }_{xz}} \end{array} \right.\tag{8-5}9.总结整体来讲，本文通过循环矩阵的方式将循环移位的运算复杂度降低了，然后通过引入傅里叶变换使得主要的矩阵乘法变成了点乘，再次降低了运算量。再通过引入对偶空间和核函数，增加了岭回归分类器的性能。整体来说，其算法结构比较简单，也正因如此，其跟踪速度也很快。不过其中也暴露了很多问题:虽然模板和 ${\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}}\over \alpha } }$更新部分对于当前帧的权重比较小，但是其更新是针对整个矩阵进行的，所以如果出现一段遮挡的场景，此模板将一去不复返。这与 siam-fc的模板更新不同，深度网络的跟踪方式通常只是更新目标框的中心位置，以及对目标框大小的微调；论文中使用了三个尺度，对于尺度的应用很微小，所以作用很小，在后面Martin Danelljan 所提出的改进算法 DSST 中算法效果有显著提高，不过用了33 个尺度，其对应的算法速度也就降下来了，所以该算法的核心竞争力降低了；当目标出现形变时，效果会变得很不好，因为 KCF 的核心其实是模板匹配，目标变形时，自然也就难以匹配好；论文中加入的汉宁窗虽然有减少 FFT 频谱泄露的作用，但是由于其分布特性，使得边缘像素cell 的值几乎为 0，因此丢失了大量信息。可能最大响应位置并非最大，甚至有可能过滤掉出现在边缘的目标；参考资料High-Speed Tracking with Kernelized Correlation Filters, Joao F.Henriques, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , 2015.Forsyth D . Object Detection with Discriminatively Trained Part-Based Models[J]. 2014.https://www.jianshu.com/p/69a3e39c51f9https://www.cnblogs.com/torsor/p/8848641.html原作者C++源码：https://github.com/joaofaro/KCFcppQiangWang复现C++源码：https://github.com/foolwood/KCF原作者博士论文：http://www.robots.ox.ac.uk/~joao/publications/henriques_phd.pdf]]></content>
      <categories>
        <category>机器学习</category>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>KCF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的默认参数陷阱]]></title>
    <url>%2F2019%2F01%2F31%2Fpython%E4%B8%AD%E7%9A%84%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E9%99%B7%E9%98%B1%2F</url>
    <content type="text"><![CDATA[前言一直以来，为了函数或者类的方便使用，经常使用默认参数来简化使用，不过最近发现这种方法中存在一个官方陷阱，详见下文。1. 问题描述现有下面一个例子：1234567def add_to(v, target=[]): target.append(v) return targetadd_to(1)add_to(2)r = add_to(3)print(r) # [1, 2, 3]我们可以看到，我们想要的输出应该是[3]，然后实际上就出乎我们的意料，原因在于，python对于函数中的默认参数，会在编译的时候申请一块内存，如果默认参数是可变参数，那么重复调用该函数或者类初始化，会共用这一块内存，从而使得实际输出与理论输出不一致。2.解决方案采用不可变参数作为默认参数不可变参数包含常数、元组、None等等，这些参数作为默认参数后，重复调用会重复申请一块空间，而对于可变参数——·列表`，则是会共用空间。初始化参数每次调用函数或者类构造函数时，都给带有可变参数的默认参数重新赋值，即可重新申请空间。内部判断在函数内部判断，从而根据不同的输入条件，赋予不同的值，这样就避免了默认参数的空间申请。如：123456789def add_to(v, target=[]): if target == []: target = [] target.append(v) return targetadd_to(1)add_to(2)r = add_to(3)print(r) # [3]下面给出网上的示例图：]]></content>
      <categories>
        <category>python</category>
        <category>在bug中写python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>默认参数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的函数传参]]></title>
    <url>%2F2019%2F01%2F31%2Fpython%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BC%A0%E5%8F%82%2F</url>
    <content type="text"><![CDATA[前言为了方便使用，我们常希望函数接口更为灵活，既有简便的默认参数，还有方便扩充的可变参数，关键字参数等等，下面我们具体介绍这些函数传参的使用方法。1. 常规参数python中，函数的传参是一个很简单的过程，如：12def test(A,B,C): return C(A),B(A)上面我给出的例子中，A,B是两个正常的参数，可能是常数，也可能是列表/元组，而C则是一个类或者函数，当然，在python中函数也是一个实例，所以可以作为参数。上面函数的使用方法可以是：123z = test(1,2,func)orx,y = test(1,2,C=func)上面的用法说明，函数的传参过程中，对于没有关键字的参数，会按照函数定义的参数顺序进行读取，有关键字的则会匹配赋值，从而实现函数传参。而返回值则会将return后面的部分以tuple的方式返回，如果return后为空，则返回None，上面的z就是一个tuple，而x和y则分别表示tuple内的两个元素。2. 默认参数有时候函数的参数很多，而在某些场景下，很多参数都是一样的，所以我们希望能够将参数固定，需要改变的时候再传入，因而就有了默认参数，默认参数很好理解：12def test(A = 1, B = 2, C = func1): ...对于这种情况，我们可以有下面几种使用方式：12345test()ortest(A = 3)ortest(1, C = func2)以上方式使得函数传参更为方便了，然而默认参数的设定要注意的是，必须在非默认参数后面，即：12def test(A, B, C=func) √def test(A, B = 2, C) X3. 可变参数除了上面的基本功能之外，python还提供了更加方便的参数工具，当我们不知道我们要使用的参数数量的时候，可以利用python的*进行设计：12def test(A, *args): print(str(A) + ':' + str(args))上面的*args参数允许我们传入无限制的参数，如：1234input: test(1,2,3,4)output: 1:(2,3,4)说明，*会将我们传入的参数打包为tuple传入，但是这就存在一个问题，如果我们想在程序内部使用的话，传入多个参数也是一个问题，这里实际上*还具有一个功能，那就是解包，如：1234x = [1,2,3]y = (1,2,3)z1 = test(1,*x)z2 = test(1,*y)这样的话我们就可利用列表或者元组实现多参数传入~4.关键字参数上面介绍的*可变参数不具有关键字，因此，在有些时候也不能实现我们想要的功能，如：一个数据表中，表的变量数不定，我们需要传入每个变量的参数，还需要知道每个参数对应的变量名，因此就需要用到关键字可变参数**：123def test(**kwargs): for key in kwargs: print str(key) + ':' + str(kwargs[key])用法如下：123456input: test(A = 1, B = 2 ,C = 3)output: A:1 B:2 C:3同样的，我们可以利用tuple解包实现：1234567input: x = &#123;'A':1,'B':2,'C':3&#125; test(**x)output: A:1 B:2 C:3]]></content>
      <categories>
        <category>python</category>
        <category>python中的奇技淫巧</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>函数传参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下的定时任务]]></title>
    <url>%2F2019%2F01%2F29%2FUbuntu%E4%B8%8B%E7%9A%84%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言Ubuntu上我们有时候希望做一些定时任务，以应对不断变化的状态或者消息收发，见下文。1. crontabcrontab是Ubuntu自带的定时任务功能，其命令为：123456&#123;minute&#125; &#123;hour&#125; &#123;day-of-month&#125; &#123;month&#125; &#123;day-of-week&#125; &#123;users&#125; &#123;full-path-to-shell-script or command&#125; o minute: 区间为 0 – 59 o hour: 区间为0 – 23 o day-of-month: 区间为0 – 31 o month: 区间为1 – 12. 1 是1月. 12是12月. o Day-of-week: 区间为0 – 7. 周日可以是0或7.打个比方，我们要设置test.sh程序定时任务，记得利用chmod赋予权限，在/etc/crontab文件中加入：12350 23 * * * root sh path/to/test.sh # 每天23:50启动*/1 * * * * root path/to/test.sh #每隔1分钟启动0 23 * * 1-5 root path/to/test.sh #每周1~5的23:00启动2. 开机自启动可在开机自启动任务中利用sleep、while和if三部分实现死循环式的定时任务，不过sleep的存在，会避免cpu占用率高，记得用&amp;放在后台执行。3.监听启动a.先启动定时任务，处于休眠，然后设置一个启动标志，当检测到某情况发生，则启动任务；b.绑定在一个程序上，当程序执行，再启动定时任务。]]></content>
      <categories>
        <category>Linux探索</category>
        <category>任务执行</category>
      </categories>
      <tags>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下的开机自启动任务]]></title>
    <url>%2F2019%2F01%2F29%2FUbuntu%E4%B8%8B%E7%9A%84%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言为了方便，我们经常希望将一些常用的软件或者程序设为开机启动，这里我以Ubuntu16.04为例进行讲解,介绍几种常见的开机自启动方法。1. startup applicationStep1 给执行文件(自带文件头:#!/bin/sh或者#!/usr/bin/env python)权限：1sudo chmod 777 可执行文件Step2 搜索Startup Application:直接添加任务名称Name、任务执行文件绝对路径或者执行命令Command和备注Comment。2. rc.local我们可以直接在 /etc/rc.local中添加开机启动命令,：1234567891011121314#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will "exit 0" on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.commandexit 0另外，记得赋予待执行的源文件可执行权限。3.init.d下面保持当前路径为/etc/init.d：Step1 在/etc/init.d新建一个开机自启动文件，如：testStep2 在test中写入指令，最好带上文件头:123#!/bin/shcommand &amp;之所以加上&amp;是因为有些命令是一直执行的，可以放在后台执行。Step3 赋权限：1sudo chmod 777 testStep4 更新开机自启动列表：123sudo update-rc.d test defaults 或者sudo update-rc.d test defaults 数字加上数字是为了防止有些开机自启动任务有顺序要求，数字越大越晚执行。删除方式是：1sudo update-rc.d -f test remove4. systemdsystemd是Ubuntu16.04及之后官方的开机自启动管理方式，我们可以在/etc/systemd/system中新建一个服务test.service,权限记得哦~，然后写入：12345678910[Unit]Description=this is test service[Service]Type=simpleExecStart= &lt;shell command&gt; #启动命令ExecStop=&lt;shell command&gt; #停止命令，可缺省[Install]WantedBy=multi-user.target #所有用户组都启动这个任务各模块具体意义可参考这里。然后执行：12sudo systemctl daemon-reloadsudo systemctl enable test.service如果想即刻运行，则:1sudo systemctl start test.service注意：以上可通过systemctl status commandname.service 来查看开机自启动项目是否设立成功，也可以在开机之后利用进程监控。]]></content>
      <categories>
        <category>Linux探索</category>
        <category>任务执行</category>
      </categories>
      <tags>
        <tag>开机自启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下快捷方式的制作]]></title>
    <url>%2F2019%2F01%2F29%2FUbuntu%E4%B8%8B%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%E7%9A%84%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言Ubuntu上为了方便程序的快速执行，我们希望能够像windows那样双击运行程序，下面我会详细介绍。1. 设定程序快捷启动对于可执行文件，Ubuntu上默认的打开方式是gedit，从而可以进行文本编辑，但是，如果我们想直接双击运行的话，则需要简单做以下几件事：Step1 添加文件头，提供文件执行所需解释器,下面给出几个针对shell和python文件的文件头示例：12345#!/bin/sh#!/bin/bash#!/usr/bin/env python#!/usr/bin/python...Step2 给文件可执行权限：1sudo chmod +x 文件Step3 设定双击启动：设定双击启动有两种方法，我主要讲第一种，首先我们打开任意一个目录，并最大化，点击Edit-Perferences-Behavior：可以看到，我们可以设置可执行文件的点击行为：直接执行、文本编辑、询问执行/显示，我的建议是调试模式下选择询问模式，因为询问模式可以选择终端执行、后台执行、显示三种，如果直接执行的话，会变为后台执行。选做：​ 跟这种方式一样的还有一个软件dconf-editor，主要是面向那些找不着顶端选项的人的，首先安装：1sudo apt install dconf-editor然后执行dconf-editor，选择org-&gt;gnome-&gt;natuilus-&gt;preferences-&gt;executable-text-activation:设置一下就好了。2. 绑定快捷方式快捷方式的作用，就相当于在快捷执行的基础上增加了图标以及属性值，绑定快捷方式的过程也是基于第一章中的Step1和Step2，在此基础上，我们可以在/usr/share/applications中新建一个桌面文件文件名.desktop，然后编辑内容，我们以Ubuntu自带蓝牙的桌面文件部分内容为例：12345678[Desktop Entry]Name=Bluetooth # 外显文件名称(（)相当于重命名)Comment=Configure Bluetooth settingsIcon=bluetooth #写入待显示图标的源文件路径(最好是绝对路径)Exec=unity-control-center bluetooth #用来写执行执行文件路径(最好是绝对路径)Terminal=false #是否以终端形式执行Type=Application #类型Categories=GTK;GNOME;Settings;X-GNOME-NetworkSettings;HardwareSettings;X-Unity-Settings-Panel;#分类有了这些，我们就可以在搜索栏搜索其Name，即：单击即可执行，既可以将其拖动到桌面或者侧边栏固定，也可以重命名为想要的名字。]]></content>
      <categories>
        <category>Linux探索</category>
        <category>任务执行</category>
      </categories>
      <tags>
        <tag>快捷方式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下的多任务执行]]></title>
    <url>%2F2019%2F01%2F29%2FUbuntu%E4%B8%8B%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前言Ubuntu上我们有时候需要同时开启多个任务，可能是串行执行，可能是并行执行，也可能是多终端执行等等，下面我将介绍Ubuntu中开启多任务的方式。1. 任务执行的监控我们之前在Ubuntu关闭指定程序进程中介绍过如何查看指定程序的进程。当然啦，除此之外，我们的终端或者UI界面也能够反应程序的执行与否，即通过前端显示来判断程序运行情况。另外，如果我们将程序设为后台执行，可利用nohup命令，如：1nohup python test.py &gt; my_nohup.log 2&gt;&amp;1 &amp;其中my_nohup.log是日志路径。2. 串行任务执行串行任务的执行方式我们见到过很多，如：1sudo apt install cmake build-essential libopencv-dev或者直接用换行方式：123command1command2command3亦或是;1command1;command2;3.并行任务执行3.1 隐式并行隐式并行的方式就是利用&amp;并行执行多任务，即：123command1 &amp;command2 &amp;command3 &amp;这样的话，多任务会并行执行，并且共享一个终端。3.2 显式并行选看：显式并行最好的方式就是开多个终端执行了，皮一下:laughing:,其实还有一种可以在同一窗口执行多终端的方式，即：1sudo apt install screen我们可以依次创建新的子窗口：12screen -S 窗口名commandscreen的用法为：1234screen -ls #显示所有子窗口screen -r screenid #切换至指定窗口ctrl+a+n #切换至下一窗口ctel+a+p #切换至上一窗口不过，上面都是题外话啦~，那么，我现在想并行开多个终端执行并行任务，可行吗？事实是可行的。正文：我们可以利用gnome-terminal命令开启多个终端：12gnome-terminal -x command #开启一个窗口执行一个命令gnome-terminal -e 'command' #开启一个窗口利用上述指令即可完成多终端任务：1234gnome-terminal -e 'command1'gnome-terminal -e 'command2'gnome-terminal -e 'command3'gnome-terminal -e 'command4'上述命令有个问题，就是命令执行完毕会自动关闭窗口，为了避免，可以加上exec bash：1gnome-terminal -e 'bash -c "command;exec bash"']]></content>
      <categories>
        <category>Linux探索</category>
        <category>任务执行</category>
      </categories>
      <tags>
        <tag>多任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下opencv的安装与卸载]]></title>
    <url>%2F2019%2F01%2F28%2FUbuntu%E4%B8%8Bopencv%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[前言Ubuntu上我们一般需要安装opencv来完成图像处理等程序操作，下面我以opencv3.4.0在Ubuntu16.04上的安装为例进行讲解。1. 系统自带opencvUbuntu16.04系统资源库自带opencv2.4.8，如果不需要用到高级功能，这点即可。其下载方法如下：1sudo apt install libopencv-dev判断系统上是否存在opencv，可输入：1pkg-config --modversion opencv卸载方法为:1sudo apt remove libopencv-dev2.官网下载安装opencv2.1 安装opencvStep1 卸载已有opencvStep2 进入Opencv官网选择对应版本source下载。Step3 在下载路径执行：12345678910111213141516171819202122232425262728293031if [ ! -d "opencv-3.4.0/" ];thenunzip opencv-3.4.0.zip -d ./elseecho "opencv package has been unzipped"fisudo apt-get install cmake pkg-config build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev -ycd opencv-3.4.0/sudo mkdir buildcd buildsudo make cleancmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..sudo make -j8sudo make installif [ 0 -eq $(cat /etc/ld.so.conf.d/opencv.conf | grep -c '/usr/local/lib') ] ;then echo '/usr/local/lib' &gt;&gt; /etc/ld.so.conf.d/opencv.conffiif [ 0 -eq $(cat /etc/bash.bashrc | grep -c 'PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig') ] ;then echo 'PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig' &gt;&gt; /etc/bash.bashrcfiif [ 0 -eq $(cat /etc/bash.bashrc | grep -c 'export PKG_CONFIG_PATH') ] ;then echo 'export PKG_CONFIG_PATH' &gt;&gt; /etc/bash.bashrcfisudo ldconfigsudo updatedbsource /etc/bash.bashrcStep4 如果安装过程中卡在ippicv这里，可以去这里下载,然后重命名为4e0352ce96473837b1d671ce87f17359-ippicv_2017u3_lnx_intel64_general_20170822.tgz，放入opencv-3.4.0/.cache/ippicv/下，重新安装。2.2 卸载opencv卸载方法如下，进入下载路径：1234567891011cd opencv-3.4.0/buildsudo make uninstallcd ../..sudo rm -rf opencv-3.4.0/sudo rm -rf /usr/local/include/opencv2 /usr/local/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/local/share/opencv /usr/local/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/local/bin/opencv* /usr/local/lib/libopencv* /usr/local/lib/pkgconfigsudo rm -rf /etc/ld.so.conf.d/opencv.confsudo sed -i '/PKG_CONFIG_PATH/d' /etc/bash.bashrcsudo ldconfigsudo updatedbcd ..source /etc/bash.bashrc3.python版opencvpython版本的opencv可通过pip直接下载安装，pip下载源建议设置为国内源，即在当前用户HOME目录下新建一个.pip/pip.conf文件，写入：1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=pypi.tuna.tsinghua.edu.cn然后下载安装python-opencv:1pip install opencv-python卸载方式：1pip uninstall opencv-python]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下绑定USB设备]]></title>
    <url>%2F2019%2F01%2F28%2FUbuntu%E7%BB%91%E5%AE%9A%E8%AE%BE%E5%A4%87USB%2F</url>
    <content type="text"><![CDATA[前言由于板卡属于外部组件，通过USB和服务器建立联系，而Linux系统对于外部USB接口是根据插入顺序命名的，这一点对于板卡的访问有很大的局限性，因此我根据板卡的内核型号不同对其进行了绑定。Step1 查询主设备号。利用cat /proc/devices 查询ttyUSB的主设备号，每个usb接口都会在/dev目录下产生一个ttyUSB*文件Step2 利用lsusb查询当前的接口情况：​ 可以看到有三个同型号的usb接口，和一个用了转接头的其他型号usb接口。Step3 查询板卡详细信息。以ttyUSB0为例，利用udevadm info -a /dev/ttyUSB0这种方式查询该板卡的详细信息：​ 第一个KERNEL是唯一的，第二KERNELS部分有很多，所以需要先找设备的生产号，即idProduct和idVendor， 与上一步的查询结果对应；Step4 利用板卡信息，建立软连接。1sudo gedit /etc/udev/rules.d/4-device.rules（这里4对应了上面查询的主设备号，-device随意取名，后缀为rules），编辑内容如下：将上面查询的信息填入其中，并填写一个链接名字，如SYMLINK+=“device0”Step5 重新插拔USB端口，重启电脑，之后输入：1ls -l /dev/device*​ ​ 可以发现虽然ttyUSB序号变了，但是device已经自动重定向了。]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>USB</tag>
        <tag>板卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04下GPU环境的搭建]]></title>
    <url>%2F2019%2F01%2F28%2FUbuntu%E4%B8%8BGPU%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言Ubuntu下的GPU环境搭建包括显卡驱动的安装，CUDA的安装以及cudnn的安装配置，下面我将具体介绍N卡的GPU环境配置。1. 显卡驱动的安装1.1 官网下载安装对于N(Nvidia)卡的驱动安装，我们可以在官网搜索最新的适配的驱动，然后手动安装：Step1 进入官网显卡驱动页面搜索合适的显卡驱动并下载：Step2 添加当前显卡驱动进入黑名单：1234567sudo vim /etc/modprobe.d/blacklist.conf blacklist nouveau blacklist lbm-nouveau options nouveau modeset=0 alias nouveau off alias lbm-nouveau offStep3 禁止nouveau:12echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.confsudo update-initramfs -uStep4 重启1sudo rebootStep5 进入字符模式：1ctrl + alt + F1Step6 输入用户名密码之后，关闭x server:12sudo service lightdm stop sudo init 3Step7 切换至下载路径，一般默认/home/用户名/Downloads/，执行：123cd nvidia目录chmod 777 NVIDIA-Linux-x86_64-版本号.runsudo sh NVIDIA-Linux-x86_64-版本号.run --no-opengl-filesStep8 重启电脑1reboot1.2 系统自动更新下载Ubuntu16.04系统自带两个显卡驱动，一个是默认的nouveau驱动，另一个是Nvidia-384显卡驱动，我们可以进入设置-软件与更新-附加驱动可以看到：选择NVIDIA相应驱动，然后点击Revert即可安装，重启可生效。当然，我们还可以通过命令行安装，如：1sudo apt-get install nvidia-384这两种方式本质都是一样的，都是基于已有软件源进行安装的，为了安装更新的驱动版本，可以更新软件源：12sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get update建议在设置中设置默认下载源为中国的软件源。1.3踩坑指南N卡驱动安装生效需要在BIOS设置Secure Boot为false；如果安装失败，且无法进入桌面，可以先进入tty界面(ctrl+alt+F1)，卸载N卡驱动：1sudo apt-get remove --purge nvidia*如果是通过runfile安装的，则在安装目录执行：1sh ./NVIDIA-Linux-x86_64-版本号.run --uninstall输入nvidia-smi或者nvidia-settings，如果有打印信息，则说明安装成功：2.CUDA的安装配置CUDA是为N卡准备的GPU运算加速库，可以方便我们的GPU计算，一般都自带一个比较旧的显卡驱动，可能跟系统不适配，所以最好不选择安装其自带的显卡驱动，另外，CUDA和显卡驱动也有适配关系：2.1 官网下载安装CUDA官网提供了CUDA的下载安装文件，选择我们想要的版本进行安装即可：下载提供的所有文件，然后依次安装即可，同样是在下载路径执行：123sudo chmod 777 cuda*.runsudo sh ./cuda文件1.runsudo sh ./cuda文件2.run安装过程注意：1234567891011121314151617181920在安装cuda过程中首先会遇到一大串协议，直接Ctrl+C停止，并输入accept即可跳过安装过程出现的选项选择方式如下：Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?(y)es/(n)o/(q)uit: n Do you want to install the OpenGL libraries?(y)es/(n)o/(q)uit [ default is yes ]: y Install the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: y Enter Toolkit Location [ default is /usr/local/cuda-9.0 ]: Do you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit: y Install the CUDA 8.0 Samples?(y)es/(n)o/(q)uit: yEnter CUDA Samples Location [ default is /home/dell ]:最后我们需要配置环境，由于比较懒，直接命令行执行，以cuda8.0为例:1234567891011121314151617181920212223if [ 0 -eq $(cat /etc/profile | grep -c 'export PATH=/usr/local/cuda-8.0/bin:/usr/local/cuda/bin:$PATH') ] ;then echo 'export PATH=/usr/local/cuda-8.0/bin:/usr/local/cuda/bin:$PATH' &gt;&gt; /etc/profilefiif [ 0 -eq $(cat /etc/profile | grep -c 'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64') ] ;then echo 'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; /etc/profilefiif [ 0 -eq $(cat /etc/ld.so.conf.d/cuda.conf | grep -c '/usr/local/cuda-8.0/lib64') ] ;then echo '/usr/local/cuda-8.0/lib64' &gt;&gt; /etc/ld.so.conf.d/cuda.conffiif [ 0 -eq $(cat /etc/ld.so.conf.d/cuda.conf | grep -c '/usr/local/cuda/lib64') ] ;then echo '/usr/local/cuda/lib64' &gt;&gt; /etc/ld.so.conf.d/cuda.conffiif [ 0 -eq $(cat ~/.bash_profile | grep -c 'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64') ] ;then echo 'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bash_profilefisudo ldconfigsource /etc/profilesource ~/.bash_profile终端执行nvcc -V，如果显示cuda信息，则环境配置成功，否则重启电脑重试。2.2 Conda安装如果你使用的是anaconda包管理器，可以直接安装下载cuda，可以提前设置下载源：123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes由于conda的环境管理机制，我们在下载安装很多深度框架(tensorflow-gpu)的同时会自动安装cuda和cudnn：2.3 卸载对于cuda8.0直接执行:12345678sudo /usr/local/cuda/bin/uninstall_cuda_8.0.plsudo rm -rf /usr/local/cuda*/sudo rm -rf /etc/ld.so.conf.d/cuda.confsudo sed -i 'cuda-8.0/d' /etc/profilesudo sed -i 'cuda-8.0/d' ~/.bash_profilesudo ldconfigsource /etc/profilesource ~/.bash_profile3.cudnn的安装配置3.1 官网下载安装进入cudnn官网注册登录,选择合适的cudnn版本：下载得到的是一个压缩包，在下载路径执行(我下的对应版本是cuda8.0, cudnn7.1,若使用可自行修改数字)：12345678tar -xzvf cudnn-8.0-linux-x64-v7.1.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo rm -rf /usr/local/cuda/lib64/libcudnn.so.7 /usr/local/cuda/lib64/libcudnn.sosudo cp cuda/lib64/lib* /usr/local/cuda/lib64/sudo rm -rf /usr/local/cuda/lib64/libcudnn.so.7 /usr/local/cuda/lib64/libcudnn.sosudo ln -s /usr/local/cuda/lib64/libcudnn.so.7.1.3 /usr/local/cuda/lib64/libcudnn.so.7sudo ln -s /usr/local/cuda/lib64/libcudnn.so.7 /usr/local/cuda/lib64/libcudnn.sosudo rm -rf cuda/3.2 Conda安装见2.2节3.3卸载12sudo rm -rf /usr/local/cuda/include/cudnn.hsudo rm -rf /usr/local/cuda/lib64/libcudnn*]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>cudnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下关闭指定程序进程]]></title>
    <url>%2F2019%2F01%2F26%2FUbuntu%E5%85%B3%E9%97%AD%E6%8C%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言有时候我们需要通过命令行关闭特定进程，对于指定进程号的关闭很容易，但是如果需要关闭指定的程序或者关闭指定路径下的程序则变得不那么容易，下面我们将会对此进行讨论。另外，很多方式都会有权限限制，所以可以适时采用sudo方式执行。1.进程资源监控1.1 gnome-system-monitorgnome-system-monitor是Ubuntu系统自带的资源管理器，可以让我们在界面中动态查看所有进程资源以及计算内存消耗情况，可直接在命令行中输入gnome-system-monitor：可以看见，以上内容能够很容易帮我们监控系统资源，但是如果我们需要将监控级别设定到指定程序级别，则需要对该界面进行相关配置：左上角显示设置我们可以通过左上角的按钮查询指定程序的进程号，也可以显示出每一个进程的执行命令行信息：右上角筛选设置默认显示的是当前用户所有的进程信息，但如果想知道当前活跃的进程信息，并显示父子进程的依赖关系，我们可以通过右上角按钮选择：1.2 toptop也是Ubuntu系统自带的一个系统资源监控指令，同样是在命令行执行：其中上方的%Cpu的意义如下：1234567us — 用户空间占用CPU的百分比。sy — 内核空间占用CPU的百分比。ni — 改变过优先级的进程占用CPU的百分比id — 空闲CPU百分比wa — IO等待占用CPU的百分比hi — 硬中断（Hardware IRQ）占用CPU的百分比si — 软中断（Software Interrupts）占用CPU的百分比而下方的则是各个进程的详细信息，其中要注意的是：12345678910PID: 进程号USER: 进程所有者PR: 进程优先级VIRT: 进程所占虚拟内存大小，单位：BRES: 进程所占物理内存大小，单位：BSHR: 进程所占共享内存大小，单位：B%CPU: 进程所占CPU使用率%MEM：进程所占内存使用率TIME+: 进程启动后所占总的CPU时间COMMAND: 进程命令名特别的，键盘按下1会显示所有核的信息，按下q会退出当前窗口。1.3 htophtop相对top来说，是一个更加清晰的系统资源监控器，不过需要先安装:1sudo apt install htop同样在命令行执行：2.利用进程号关闭进程指定进程号，如果要关闭其对应进程，只需要执行：12kill 进程号kill -15 进程号以上两种方式会关闭正在执行的进程，不过依然会有部分程序会延时相应，或者不相应，所以我们可以强制关闭：1kill -9 进程号进程号一定要填，不然默认关闭所有进程，包括我们的系统进程。3.利用程序名关闭进程3.1 直接关闭Ubuntu提供了直接关闭进程的指令killall和pkill：12killall 进程名pkill 进程名不过这两个方法有三个缺点：进程名指的是执行命令的名称，并不是命令行，即：1python test.py =&gt;python进程名如果超过15个字符会自动截断：12killall bt_uinfo_memcached xkillall bt_uinfo_memcac √对于图形界面等关闭不了，因为属于tty,不属于command。3.2 先查找再关闭我们在第一章介绍gnome-system-monitor时，提到其自带查找进程号的功能，即search for open files;Ubuntu提供了一种简单完全匹配工具pidof:1pidof 进程名注意这里的进程名必须是完整的名称，如:python。Ubuntu还提供了命令行工具进行查找：12ps -e | grep 进程名ps -ef | grep 进程名我们可以看到，仅仅-e所能匹配的进程不全，但是-ef能匹配更详细的命令行信息，更重要的是这种方式不是完整匹配，而是部分匹配。对于ps -e，Ubuntu提供了更简洁的用法：12pgrep 进程名pgrep -a 进程名pgrep的缺点在于，无法实现ps -ef的命令行匹配功能，其优点在于可以只打印进程号，方便后续关闭。我们如果既想拥有pgrep那样直接打印进程号的简洁，又想拥有ps -ef |grep方式匹配完整命令行的强大，可以利用awk语言编程实现，利用进程号在第二列的特性：1ps -ef | grep 进程信息 |awk '&#123;print $2&#125;'有了这些进程的查找方式，那么关闭进程就很容易了，那么，如果我们想要一次性执行查找和关闭怎么办？可以利用xargs命令，下面为了保证权限问题，都加入了sudo：123pidof python |xargs sudo kill -9pgrep python |xags sudo kill -9ps -ef |grep python |awk '&#123;print $2&#125;'|xargs sudo kill -94.利用python关闭进程4.1 os系统指令python自身可以利用os模块的system功能执行命令行指令，如：1os.system('kill -9 进程号')另外Python也可以获取自身程序的进程号：1os.getpid()4.2 psutil库psutil库能够轻松实现获取系统运行的进程和系统利用率（包括CPU、内存、磁盘、网络等）信息，它主要用来做系统监控，性能分析，进程管理。其安装方式如下：1pip install psutil这里我不介绍其详细功能，只介绍如何利用其关闭特定进程。获取当前系统所有进程：1psutil.pids()返回所有进程号列表。获取每个进程的详细信息：1psutil.Process(进程号)返回的是该进程所对应的类，通过观察其属性可见，有很多信息可以利用：那么我们可以这样进行匹配，假如我们要关闭包含train字样的python进程，那么可以：123456789import psutilimport osfor i in psutil.pids(): try: p = psutil.Process(i) if p.name() == 'python' and 'train' in ''.join(p.cmdline()): os.system('kill -9 '+str(i)) except Exception: pass之所以要加入保护，是因为进程随时都处于启动和关闭状态，有的进程可能在处理过程中关闭了。]]></content>
      <categories>
        <category>Linux探索</category>
        <category>线程/进程管理</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Hexo在多台电脑上提交和更新博客]]></title>
    <url>%2F2019%2F01%2F26%2F%E5%88%A9%E7%94%A8Hexo%E5%9C%A8%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E4%B8%8A%E6%8F%90%E4%BA%A4%E5%92%8C%E6%9B%B4%E6%96%B0%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[前言考虑到Hexo博客每次的生成和部署都需要环境、依赖资源和所有博客文件，所以如果我们要换一台电脑写博客，则需要配置相同的环境、依赖和所有博客文件，最简单的方法就是用U盘或者其他存储方式拷贝，不过我这里我要讲的是利用Github远程仓库进行更新部署。1. 分析依赖1.1环境依赖要想对我们的博客进行正常的编辑和部署，需要安装以下几个部分：Step1下载安装Node.js和Git;Step2 配置本地Git环境，由于我们已经注册了相关信息，所以只需要设置好ssh密钥，方法见我之前的博客Windows平台下Github远程仓库的搭建;Step3 如果需要部署到Coding，那么还可以继续参照我的这篇博客利用Hexo将博客部署到GitPages和CodingPages进行ssh密钥绑定;Step4 下载安装Typora等Markdown编辑器;Step5 下载安装腾讯云对象存储客户端cosbrowser。1.2资源配置依赖我们在制作博客网站的过程中安装了很多npm的资源包，这些资源包很多，我们没必要一一拷贝，具体如下：安装Hexo:1npm install -g hexo-cli利用package.json部署资源：123456789101112131415161718192021222324252627282930313233343536373839404142&#123; "name": "hexo-site", "version": "0.0.0", "private": true, "hexo": &#123; "version": "3.8.0" &#125;, "dependencies": &#123; "eslint": "4.12.1", "gulp": "^4.0.0", "hexo": "^3.7.0", "hexo-deployer-git": "^1.0.0", "hexo-generator-archive": "^0.1.5", "hexo-generator-baidu-sitemap": "^0.1.6", "hexo-generator-category": "^0.1.3", "hexo-generator-cname": "^0.3.0", "hexo-generator-feed": "^1.2.2", "hexo-generator-index": "^0.2.1", "hexo-generator-searchdb": "^1.0.8", "hexo-generator-tag": "^0.2.0", "hexo-helper-live2d": "^3.1.0", "hexo-neat": "^1.0.4", "hexo-pdf": "^1.1.1", "hexo-renderer-ejs": "^0.3.1", "hexo-renderer-markdown-it-plus": "^1.0.4", "hexo-renderer-mathjax": "^0.6.0", "hexo-renderer-stylus": "^0.3.3", "hexo-server": "^0.3.3", "hexo-wordcount": "^6.0.1", "live2d-widget-model-tororo": "^1.0.5", "live2d-widget-model-wanko": "^1.0.5" &#125;, "devDependencies": &#123; "babel-cli": "^6.26.0", "babel-preset-es2015": "^6.24.1", "gulp-babel": "^8.0.0", "gulp-htmlclean": "^2.7.22", "gulp-htmlmin": "^5.0.1", "gulp-minify-css": "^1.2.4", "gulp-uglify": "^3.0.1" &#125;&#125;我们只需要在同一目录下执行即可自动下载安装所有资源1npm install1.3博客内容依赖博客内容依赖包括主题文件和博客文件，因此我们需要的部分就是：12345678910.|—— scaffolds|—— source|—— themes|—— .gitignore|—— _config.yml|—— db.json|—— gulpfile.js|—— package.json|—— package-lock.json2.部署远程仓库2.1同步远程仓库部署远程仓库的目的是将本地的博客源文件同步到远程仓库，当然并不是master分支，具体过程如下：Step1 删除第三方主题下的.git文件夹；Step2 配置.gitignore文件：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/Step3 将项目源文件同步到远程仓库对应的hexo分支：123456git init //初始化本地仓库git add -A //添加本地所有文件到仓库 git commit -m "blog源文件" //添加commitgit branch hexo //添加本地仓库分支hexogit remote add origin 博客的git地址 //添加远程仓库git push origin hexo //将本地仓库的源文件推送到远程仓库hexo分支如果出现报错信息：12ERROR: Repository not found.fatal: Could not read from remote repository.则首先检查ssh配置是否正确，并检查git地址是否一致，利用1git remote -v如果不一致，则通过如下方式改变：1git remote set-url origin git地址2.2 隐藏源文件如果将原文件同步至分支，则会由于GitPages或者CodingPages的特性，自动变为公开目录，如果其中有一些不希望别人看见的信息，则会变得不方便，所以我们可以新建一个私有仓库进行同步。Step1 如果之前已经新建了hexo分支，那么可以选择性的删除该分支；Step2 新建一个私有仓库：Step3 更新远程仓库（如果不行的话，更新ssh）123456git init //初始化本地仓库git remote add origin 博客的git地址 //添加远程仓库git pull origin mastergit add . //添加本地所有文件到仓库 git commit -m "blog源文件" //添加commitgit push -u origin master3.更新远程和本地仓库对于本地电脑，如果是第一次使用，则输入：123git clone -b &lt;分支&gt; &lt;server&gt; &lt;待存放路径&gt; //&lt;server&gt; 是指在线仓库的地址cd &lt;待存放路径&gt;npm install如果不是第一次操作，对于本地仓库的更新，可执行：1git pull]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客优化之Next主题功能强化]]></title>
    <url>%2F2019%2F01%2F25%2FHexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8BNext%E4%B8%BB%E9%A2%98%E5%8A%9F%E8%83%BD%E5%BC%BA%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言这里是Hexo博客优化的最后一篇了，主要讲的是讲的是功能层面的优化，如：网站加速、评论功能、在线聊天功能、一键分享功能等等。1. 添加网站评论功能1.1来必力来必力是一款韩国的评论软件，先进入注册登录，然后安装免费版本：安装之后获取安装代码里面的uid，放入主题配置文件：1livere_uid: 你的uid效果如下：进入管理后台可以管理评论：1.2ValineValine是国内的一款极简风格的评论软件，首先进入LeanCloud注册，然后在控制台随便创建一个项目后，获取密钥：然后修改主题配置文件：123456789101112valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: 你的appid appkey: 你的appkey notify: false # 邮件提醒 verify: false # 验证码 placeholder: ヾﾉ≧∀≦)o 来呀！吐槽一番吧！ # 默认输入信息 avatar: mm # gravatar style guest_info: nick # 用户可输入的信息，支持：昵称nick,邮箱mail和链接link pageSize: 10 # pagination size，每页评论数 visitor: false # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # 评论计数其实在Web设置中可以添加我们的域名，差不多，效果如下：其后台在我们的项目-存储-Comments中：鉴于Valine比较适合我博客主题风格，我还是选择它。2.添加网站分享功能这里我采用的是needmoreshare2,首先在themes/next/下执行：1git clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton然后配置主题文件：12345678910111213141516needmoreshare2: enable: true postbottom: enable: true #是否开启博客分享按钮 options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook float: enable: true #网站分享按钮 options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook效果如下：3.博文压缩我们利用Hexo生成的博客文件中存在大量的空格和空行，从而使得博客资源中有很多不必要的内存消耗，使得网站加载变慢，所以可以利用neat进行博文压缩，首先安装：1npm install hexo-neat --save为了在开启hexo-neat的同时，不要将我们的动态配置压缩了，可在站点配置文件中加入：123456789101112131415161718192021222324# hexo-neat# 博文压缩neat_enable: true# 压缩htmlneat_html: enable: true exclude:# 压缩css neat_css: enable: true exclude: - '**/*.min.css'# 压缩jsneat_js: enable: true mangle: true output: compress: exclude: - '**/*.min.js' - '**/jquery.fancybox.pack.js' - '**/index.js' - '**/clicklove.js' - '**/fireworks.js'当然，除此之外还可以用gulp插件进行压缩，先安装：12npm install gulp -gnpm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save然后在blog主目录下添加gulpfile.js文件：123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']);4.DaoVoice在线联系首先我们在DaoVoice上注册一个账号，然后进入应用设置-安装到网站-仅匿名用户：聊天设置中可以设置聊天窗口样式：相应地，我们还能设置接受消息方式，既能控制台访问，也能微信或者邮件接受消息：5.加速鼠标响应在themes/next/下执行：1git clone https://github.com/theme-next/theme-next-fastclick source/lib/fastclick然后设置主题配置文件：1fastclick = true6.添加cdn加速Next主题官方提供了一些软件的CDN加速，我们可以在主题配置文件中设置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162vendors: # Internal path prefix. Please do not edit it. _internal: lib # Internal version: 2.1.3 # Example: # jquery: //cdn.jsdelivr.net/npm/jquery@2/dist/jquery.min.js # jquery: //cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js jquery: //cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js # Internal version: 2.1.5 # See: https://fancyapps.com/fancybox # Example: # fancybox: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js # fancybox: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.js # fancybox_css: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css # fancybox_css: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.css fancybox: //cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js fancybox_css: //cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css # Internal version: 1.0.6 # See: https://github.com/ftlabs/fastclick # Example: # fastclick: //cdn.jsdelivr.net/npm/fastclick@1/lib/fastclick.min.js # fastclick: //cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js fastclick: //cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js # Internal version: 1.9.7 # See: https://github.com/tuupola/jquery_lazyload # Example: # lazyload: //cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js # lazyload: //cdnjs.cloudflare.com/ajax/libs/jquery_lazyload/1.9.7/jquery.lazyload.min.js lazyload: //cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js # Internal version: 1.2.1 # See: http://velocityjs.org # Example: # velocity: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js # velocity: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js # velocity_ui: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js # velocity_ui: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js velocity: //cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js velocity_ui: //cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js # Internal version: 0.7.9 # See: https://faisalman.github.io/ua-parser-js # Example: # ua_parser: //cdn.jsdelivr.net/npm/ua-parser-js@0/src/ua-parser.min.js # ua_parser: //cdnjs.cloudflare.com/ajax/libs/UAParser.js/0.7.9/ua-parser.min.js ua_parser: //cdn.jsdelivr.net/ua-parser.js/0.7.10/ua-parser.min.js # Internal version: 4.6.2 # See: https://fontawesome.com # Example: # fontawesome: //cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css # fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css fontawesome: //maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css # Internal version: 2.4.1 # See: https://www.algolia.com # Example: # algolia_instant_js: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.js # algolia_instant_css: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.css algolia_instant_js: algolia_instant_css: # Internal version: 1.0.2 # See: https://github.com/HubSpot/pace # Example: # pace: //cdn.jsdelivr.net/npm/pace-js@1/pace.min.js # pace: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js # pace_css: //cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css # pace_css: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css pace: //cdn.bootcss.com/pace/1.0.2/pace.min.js pace_css: //cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.min.css # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-canvas-nest # Example: # canvas_nest: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js # canvas_nest_nomobile: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest-nomobile.min.js canvas_nest: //cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js canvas_nest_nomobile: # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-three # Example: # three: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js # three_waves: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three-waves.min.js # canvas_lines: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_lines.min.js # canvas_sphere: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_sphere.min.js three: three_waves: canvas_lines: canvas_sphere: # Internal version: 1.0.0 # See: https://github.com/zproo/canvas-ribbon # Example: # canvas_ribbon: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js canvas_ribbon: # Internal version: 3.3.0 # See: https://github.com/ethantw/Han # Example: # han: //cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css # han: //cdnjs.cloudflare.com/ajax/libs/Han/3.3.0/han.min.css han: # Internal version: 3.3.0 # See: https://github.com/vinta/pangu.js # Example: # pangu: //cdn.jsdelivr.net/npm/pangu@3/dist/browser/pangu.min.js # pangu: //cdnjs.cloudflare.com/ajax/libs/pangu/3.3.0/pangu.min.js pangu: # Internal version: 1.0.0 # See: https://github.com/revir/need-more-share2 # Example: # needmoreshare2_js: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js # needmoreshare2_css: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css needmoreshare2_js: needmoreshare2_css: # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-bookmark # Example: # bookmark: //cdn.jsdelivr.net/gh/theme-next/theme-next-bookmark@1/bookmark.min.js bookmark: # Internal version: 1.1 # See: https://github.com/theme-next/theme-next-reading-progress # Example: # reading_progress: //cdn.jsdelivr.net/gh/theme-next/theme-next-reading-progress@1/reading_progress.min.js reading_progress: # leancloud-storage # See: https://www.npmjs.com/package/leancloud-storage # Example: # leancloud: //cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js leancloud: # valine # See: https://github.com/xCss/Valine # Example: # valine: //cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js # valine: //cdnjs.cloudflare.com/ajax/libs/valine/1.3.4/Valine.min.js valine: # gitalk # See: https://github.com/gitalk/gitalk # Example: # gitalk_js: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js # gitalk_css: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css gitalk_js: gitalk_css: # js-md5 # See: https://github.com/emn178/js-md5 # Example: # md5: //cdn.jsdelivr.net/npm/js-md5@0/src/md5.min.js md5:7. 添加lazyloadlazylod可以在用户不查看的时候，不加载相关部分，从而提升网站加载速度，设置方法同上：1git clone https://github.com/theme-next/theme-next-jquery-lazyload source/lib/jquery_lazyload然后配置主题文件：1lazyload: true8.网站动态元素延时加载我们的网站添加了许多动态元素之后，加载速度会变慢，所以可以先不加载动态元素，等静态元素加载完之后再加载动态元素，这样就加速了网站的登入。可设置主题文件：123456789101112131415161718# Use velocity to animate everything.motion: enable: false async: false transition: # Transition variants: # fadeIn | fadeOut | flipXIn | flipXOut | flipYIn | flipYOut | flipBounceXIn | flipBounceXOut | flipBounceYIn | flipBounceYOut # swoopIn | swoopOut | whirlIn | whirlOut | shrinkIn | shrinkOut | expandIn | expandOut # bounceIn | bounceOut | bounceUpIn | bounceUpOut | bounceDownIn | bounceDownOut | bounceLeftIn | bounceLeftOut | bounceRightIn | bounceRightOut # slideUpIn | slideUpOut | slideDownIn | slideDownOut | slideLeftIn | slideLeftOut | slideRightIn | slideRightOut # slideUpBigIn | slideUpBigOut | slideDownBigIn | slideDownBigOut | slideLeftBigIn | slideLeftBigOut | slideRightBigIn | slideRightBigOut # perspectiveUpIn | perspectiveUpOut | perspectiveDownIn | perspectiveDownOut | perspectiveLeftIn | perspectiveLeftOut | perspectiveRightIn | perspectiveRightOut post_block: fadeIn post_header: slideDownIn post_body: slideDownIn coll_header: slideLeftIn # Only for Pisces | Gemini. sidebar: slideUpIn9.添加站内搜索Next集成了站内搜索功能，可先安装依赖：1npm install hexo-generator-searchdb --save然后设置主题配置文件：1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 3 # unescape html strings to the readable one unescape: false效果如下：10.添加百度谷歌收录要想让我们的博客被百度、谷歌等搜索引擎索引到，需要提交我们的域名，谷歌很快就能收录，但是百度要一两个月，具体步骤如下：在百度搜搜引擎中查看自己域名是否被收录:site:huangpiao.tech然后点击提交网址，并在百度站长中提交申请，并验证网站：我选择将验证文件放入blog/source中,然后进行部署，为了防止渲染造成的文件失效，需要在这个验证文件上面加入：123---layout: false---验证痛过之后，在我们的博客主目录下载安装：12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save当我们在此加载博客会在public目录生成sitemap.xml和baidusitemap.xml修改博客站点配置文件：12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml修改博客主题配置文件：12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true11.添加公式编辑功能Next6主题集成了mathjax和katex两种公式编辑功能,其中后者渲染速度比前者快很多，只不过支持的功能少一点。mathjax:在博客主目录执行：12npm un hexo-renderer-marked --savenpm i hexo-renderer-kramed --save # 或者 hexo-renderer-pandoc修改node_modules\kramed\lib\rules\inline.js中对应地方：12escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,然后修改主题配置文件：12345678math: enable: true ... engine: mathjax #engine: katexmathjax: cdn: //cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0在博客内容中，添加mathjax = true。katex:在博客主目录执行：12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it-plus --save然后修改主题配置文件：12345math: enable: true ... #engine: mathjax engine: katex具体问题可以查看官方文档，至少我这里没有生效:sob:12.添加流程图支持对于流程图flowchart或者更好的mermaid可以先下载：123npm install --save hexo-filter-flowchartnpm install hexo-filter-mermaid-diagrams npm install --save hexo-filter-sequence然后在站点配置文件中加入：12345678910flowchart: # raphael: # optional, the source url of raphael.js # flowchart: # optional, the source url of flowchart.js options: # options used for `drawSVG`# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: "7.1.2" # default v7.1.2 options: external_link: false #这个已经有了，修改即可对于mermaid，则需要在themes/next/layout/_partial/footer.swig中加入：12345678&#123;% if theme.mermaid.enable %&#125; &lt;script src=&apos;https://unpkg.com/mermaid@&#123;&#123; theme.mermaid.version &#125;&#125;/dist/mermaid.min.js&apos;&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: &apos;forest&apos;&#125;); &#125; &lt;/script&gt;&#123;% endif %&#125;13.增加文章置顶功能修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为：12345678910111213141516171819202122232425262728'use strict';var pagination = require('hexo-pagination');module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, &#123; perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: &#123; __index: true &#125; &#125;);&#125;;在文章中添加 top 值，数值越大文章越靠前，如12345678---title: 解决Charles乱码问题date: 2017-05-22 22:45:48tags: 技巧categories: 技巧copyright: truetop: 100---14.参考链接https://hoxis.github.io/hexo-next-daovoice.htmlhttps://blog.csdn.net/blue_zy/article/details/79071414https://www.jianshu.com/p/61abc6c43220https://blog.csdn.net/blue_zy/article/details/79071414https://blog.csdn.net/lvonve/article/details/80200348]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客优化之Next主题美化]]></title>
    <url>%2F2019%2F01%2F24%2FHexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8BNext%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言有了前面几篇博客的介绍，我们就可以很容易的搭建并编辑我们的博客了，不过既然是属于自己的博客网站，自然也就想让其更加美观，更有意思，所以呢我下面介绍一下Hexo博客的主题美化操作。1. Next主题Hexo博客支持很多主题风格，其中Next主题是Github上Star最多的主题，其一直在更新维护，支持非常多的外部插件和功能选项。我目前使用的是Next6.0版本，下面我会介绍基于Next6主题的界面美化手法。1.1 Next主题的安装配置Next主题的安装方式很简单，只需要在博客主目录下执行：1git clone https://github.com/theme-next/hexo-theme-next themes/next然后设置站点配置文件_config.yml：1theme: next即可将我们的Hexo博客主题替换为Next主题。1.2 主题简单配置Next主题提供很多方便的功能，我们来一一介绍：Next主题风格：Next提供了四中主题风格scheme，可以在主题配置文件blog/themes/next/_config.yml文件中进行选择，分别是Muse、Mist、Pisces、Gemini：这里我选择的是Gemini主题，也就是最后一种样式；Next主题一般配置：12345678910111213141516171819override：false #表示是否将主题置为默认样式cache: enable:true #表示添加缓存功能，这样浏览器后续打开我们的博客网站会更快menu: #设置博客各个页面的相对路径，默认根路径是blog/source home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #日历 #sitemap: /sitemap.xml || sitemap #站点地图，供搜索引擎爬取 #commonweal: /404/ || heartbeat # 腾讯公益404# Enable/Disable menu icons / item badges.menu_settings: icons: true # 是否显示各个页面的图标 badges: true # 是否显示分类/标签/归档页的内容量# Schemesscheme: Gemini以上是Next最常规的配置，而相应的站点配置blog/_config.yml文件的基本配置为：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980title: 见渊の博客subtitle: 记录生活中的点点滴滴description: 直到这一刻微笑着说话为止，我至少留下了一公升眼泪keywords:author: 黄飘language: zh-CN # 主题语言timezone: Asia/Shanghai #中国的时区，不要乱改城市# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://huangpiao.tech #绑定域名root: / #默认根路径，指向实际的sourcepermalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render: README.md # 部署的时候不包含的文件# Writingnew_post_name: :title.md # 默认的新博文名称default_layout: post # 默认布局titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0 #把博客名称改成小写/大写（1,2）render_drafts: false # 是否显示草稿post_asset_folder: false #是否启用资源文件夹（用来存放相对路径图片或文件）relative_link: false # 把链接改为与根目录的相对位址future: truehighlight: enable: true #是否开启代码高亮 line_number: true #是否增加代码行号 auto_detect: true #自动判断代码语言 tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: #首页博客分布 path: '' #博客的默认路径 per_page: 10 #每页博客数量上限 order_by: -date #博客排序# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #博客日期格式time_format: HH:mm:ss #博客时间格式# Pagination## Set per_page to 0 to disable paginationper_page: 10 #同上#归档页的分页设置archive_generator: #归档页的配置 per_page: 30 #归档页每页博客数 yearly: true #按年归档 monthly: true #按月归档#标签页的分页设置tag_generator: per_page: 20 #标签页每页博客数 theme: next6 #选择博客主题，名字为themes中选择的主题文件夹名称# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: #博客部署 type: git repo: github: https://github.com/nightmaredimple/nightmaredimple.github.io.git coding: https://git.coding.net/nightmaredimple/nightmaredimple.git branch: master以上的效果如下：2.添加博客自定义图标我们博客的默认图标是H，不过Next支持修改图标，下面是我的图标：博客网站的图标可以在easyicon、bitbug、iconfont等网站选择和制作，然后选择或者创建相应大小的图标文件，放置在blog/themes/next/sources/images目录下，并在主题配置文件中进行如下配置，只需要设置small和medium两个就可以：12345favicon: small: /images/16x16.png medium: /images/32x32.png apple_touch_icon: /images/128x128.png safari_pinned_tab: /images/logo2.svg3. 鼠标点击特效鼠标的点击红心特效如下：具体步骤如下：在/themes/next/source/js/src下新建文件 clicklove.js ，接着把下面的代码拷贝粘贴到 clicklove.js 文件中：1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document);在\themes\next\layout\_layout.swig文件末尾添加：12&lt;!-- 页面点击小红心 --&gt;&lt;script type="text/javascript" src="/js/src/clicklove.js"&gt;&lt;/script&gt;当然，还有一种特效(只能选一个)：跟那个红心是差不多的，首先在themes/next/source/js/src里面建一个叫fireworks.js的文件，代码如下：1&quot;use strict&quot;;function updateCoords(e)&#123;pointerX=(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY=e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t=anime.random(0,360)*Math.PI/180,a=anime.random(50,180),n=[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=colors[anime.random(0,colors.length-1)],a.radius=anime.random(16,32),a.endPos=setParticuleDirection(a),a.draw=function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle=a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=&quot;#F00&quot;,a.radius=0.1,a.alpha=0.5,a.lineWidth=6,a.draw=function()&#123;ctx.globalAlpha=a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth=a.lineWidth,ctx.strokeStyle=a.color,ctx.stroke(),ctx.globalAlpha=1&#125;,a&#125;function renderParticule(e)&#123;for(var t=0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a=createCircle(e,t),n=[],i=0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n=this,i=arguments;clearTimeout(a),a=setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl=document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx=canvasEl.getContext(&quot;2d&quot;),numberOfParticules=30,pointerX=0,pointerY=0,tap=&quot;mousedown&quot;,colors=[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize=debounce(function()&#123;canvasEl.width=2*window.innerWidth,canvasEl.height=2*window.innerHeight,canvasEl.style.width=window.innerWidth+&quot;px&quot;,canvasEl.style.height=window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render=anime(&#123;duration:1/0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!==e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!==e.target.id&amp;&amp;&quot;A&quot;!==e.target.nodeName&amp;&amp;&quot;IMG&quot;!==e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX=(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY=e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t=anime.random(0,360)*Math.PI/180,a=anime.random(50,180),n=[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=colors[anime.random(0,colors.length-1)],a.radius=anime.random(16,32),a.endPos=setParticuleDirection(a),a.draw=function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle=a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=&quot;#F00&quot;,a.radius=0.1,a.alpha=0.5,a.lineWidth=6,a.draw=function()&#123;ctx.globalAlpha=a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth=a.lineWidth,ctx.strokeStyle=a.color,ctx.stroke(),ctx.globalAlpha=1&#125;,a&#125;function renderParticule(e)&#123;for(var t=0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a=createCircle(e,t),n=[],i=0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n=this,i=arguments;clearTimeout(a),a=setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl=document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx=canvasEl.getContext(&quot;2d&quot;),numberOfParticules=30,pointerX=0,pointerY=0,tap=&quot;mousedown&quot;,colors=[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize=debounce(function()&#123;canvasEl.width=2*window.innerWidth,canvasEl.height=2*window.innerHeight,canvasEl.style.width=window.innerWidth+&quot;px&quot;,canvasEl.style.height=window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render=anime(&#123;duration:1/0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!==e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!==e.target.id&amp;&amp;&quot;A&quot;!==e.target.nodeName&amp;&amp;&quot;IMG&quot;!==e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;打开themes/next/layout/_layout.swig,在&lt;/body&gt;上面写下如下代码：12345&#123;% if theme.fireworks %&#125; &lt;canvas class=&quot;fireworks&quot; style=&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;/canvas&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/animejs/2.2.0/anime.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/fireworks.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125;打开主题配置文件，在里面最后写下：12# Fireworksfireworks: true4.添加动态背景上面这种只是其中一种动态背景，新版的Next主题集成了该功能，只需要在主题配置中设置如下即可，下面每个模块只设置其中一个为true，具体效果如何可自己尝试：1234567891011121314151617181920212223242526272829# Canvas-nest# Dependencies: https://github.com/theme-next/theme-next-canvas-nestcanvas_nest: # 网络背景 enable: true onmobile: true # display on mobile or not color: '0,0,0' # RGB values, use ',' to separate opacity: 0.5 # the opacity of line: 0~1 zIndex: -1 # z-index property of the background count: 150 # the number of lines# JavaScript 3D library.# Dependencies: https://github.com/theme-next/theme-next-three# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false# Canvas-ribbon# Dependencies: https://github.com/theme-next/theme-next-canvas-ribbon# size: The width of the ribbon.# alpha: The transparency of the ribbon.# zIndex: The display level of the ribbon.canvas_ribbon: enable: false size: 300 alpha: 0.6 zIndex: -1另外需要在blog中下载相应资源包，具体见上面的链接，下面我给出canvas_nest的下载方式：1git clone https://github.com/theme-next/theme-next-canvas-nest themes/next/source/lib/canvas-nest5. 修改标签样式博客底部的标签样式默认为#tag，我们可以将其改成：只需要修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;6. 作者头像设置可以设置当鼠标放置在头像上时，头像自动旋转，具体设置如下：1234567891011avatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/author.jpg #将我们的头像图片放置在blog/themes/next/source/images目录下，填写具体地址 # If true, the avatar would be dispalyed in circle. rounded: true #鼠标放在头像上时是否旋转 # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 #头像放缩指数 # If true, the avatar would be rotated with the cursor. rotated: true #头像是否设为圆形，否则为矩形7.文章结束标志在路径 \themes\next\layout\_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt;接着打开\themes\next\layout\_macro\post.swig文件，在post-body 之后(END POST BODY)， post-footer 之前添加如代码：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt;然后打开主题配置文件（_config.yml),在末尾添加：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true8.侧边栏设置设置主题配置文件，其中social表示社交信息，我们可以填入我们相关的链接，格式为链接名:链接地址 || 链接图标，其中链接图标必须是FontAwesome网站中存在的图标名。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Posts / Categories / Tags in sidebar.site_state: true # 是否在侧边栏加入日志、分类、标签等跳转链接# Social Links# Usage: `Key: permalink || icon`# Key is the link label showing to end users.# Value before `||` delimeter is the target permalink.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.social: GitHub: https://github.com/nightmaredimple || github # CSDN: https://blog.csdn.net/nightmare_dimple || crosshairs #E-Mail: mailto:yourname@gmail.com || envelope #Weibo: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skypesocial_icons: enable: true #是否显示社交图标 icons_only: false #是否仅显示社交图标 transition: true # Follow me on GitHub banner in right-top corner.# Usage: `permalink || title`# Value before `||` delimeter is the target permalink.# Value after `||` delimeter is the title and aria-label name.github_banner: https://github.com/nightmaredimple || Follow me on GitHub #添加右上角github绑带# Blog rollslinks_icon: linklinks_title: Linkslinks_layout: block#links_layout: inline#links: #Title: http://example.com# Sidebar Avataravatar: #头像设置 # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/author.jpg # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: true# Table Of Contents in the Sidebartoc: enable: true #是否自动生成目录 # Automatically add list number to toc. number: false #目录是否自动产生编号 # If true, all words will placed on next lines if header width longer then sidebar width. wrap: false #标题过长是否换行 # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` var. max_depth: 6 #最大标题深度sidebar: # Sidebar Position, available values: left | right (only for Pisces | Gemini). position: left #侧边栏位置 #position: right # Manual define the sidebar width. # If commented, will be default for: # Muse | Mist: 320 # Pisces | Gemini: 240 #width: 300 # Sidebar Display, available values (only for Muse | Mist): # - post expand on posts automatically. Default. # - always expand for all pages automatically # - hide expand only when click on the sidebar toggle icon. # - remove Totally remove sidebar including sidebar toggle. display: post #display: always #display: hide #display: remove # Sidebar offset from top menubar in pixels (only for Pisces | Gemini). offset: 12 #侧边栏相对主菜单像素距离 # Back to top in sidebar. b2t: true #是否提供一键置顶 # Scroll percent label in b2t button. scrollpercent: true #是否显示当前阅读进度 # Enable sidebar on narrow view (only for Muse | Mist). onmobile: false #手机上是否显示侧边栏9.文章阴影设置打开\themes\next\source\css\_custom\custom.styl,向里面加入：12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125;这个功能会影响排版，我已经取消了。10. 添加文章版权信息要想开启博客的版权功能，需要设置主题配置文件：1234creative_commons: license: by-nc-sa sidebar: true post: true11.设置博客底部布局这一部分对应主题配置文件中的：123456789101112131415161718192021222324252627282930313233343536373839footer: # Specify the date when the site was setup. # If not defined, current year will be used. since: 2019 #建站时间 # Icon between year and copyright info. icon: # Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons/ # `heart` is recommended with animation in red (#ff0000). name: heart #作者图标（默认是author人像) # If you want to animate the icon, set it to true. animated: true #图标是否闪动 # Change the color of icon, using Hex Code. color: "#808080" #图标颜色 # If not defined, `author` from Hexo main config will be used. copyright: 黄飘 #别填bool型，最后显示的东西是copyright || author，即左边没有设置的话就显示作者 # ------------------------------------------------------------- powered: # Hexo link (Powered by Hexo). enable: false #是否显示 Powered by hexo # Version info of Hexo after Hexo link (vX.X.X). version: false #是否显示Hexo版本 theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: false #是否显示主题信息 # Version info of NexT after scheme info (vX.X.X). version: false #是否显示主题版本 # ------------------------------------------------------------- # Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer. # http://www.miitbeian.gov.cn beian: enable: false #是否显示网站备案信息 icp: # ------------------------------------------------------------- # Any custom text can be defined here. #custom_text: Hosted by &lt;a href="https://pages.coding.me" class="theme-link" rel="noopener" target="_blank"&gt;Coding Pages&lt;/a&gt;12. 添加打赏在主题配置文件中设置如下：123456reward: enable: true comment: 坚持原创技术分享，您的支持将鼓励我继续创作！ wechatpay: /images/wechatpay.jpg alipay: /images/alipay.jpg #bitcoin: /images/bitcoin.jpg自己获取自己的支付收款码，放置在next/source/images中13. 添加页面宠物首先在博客目录下执行：1npm install -save hexo-helper-live2d然后在站点配置文件中加入：123456789101112131415live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false model: use: live2d-widget-model-wanko #选择哪种模型 display: #放置位置和大小 position: right width: 150 height: 300 mobile: show: false #是否在手机端显示上面模型的选择可在lived2d中选择，并下载相应的模型：1npm install live2d-widget-model-wanko14.设置代码块样式代码块的行号显示在上面已经介绍了，位于站点配置文件，对于代码块的主题我么还能设置其背景，增加复制按钮等，可修改主题配置文件如下：12345678910111213# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: nightcodeblock: # Manual define the border radius in codeblock # Leave it empty for the default 1 border_radius: # Add copy button on codeblock copy_button: enable: true # Show text copy result show_result: true15.设置博客摘要显示对于摘要显示，首先我们需要开启摘要功能，修改主题配置文件：123456789101112131415161718# Automatically scroll page to section which is under &lt;!-- more --&gt; mark.scroll_to_more: true #选取博客正文&lt;!--more--&gt;前的内容# Automatically saving scroll position on each post/page in cookies.save_scroll: false# Automatically excerpt description in homepage as preamble text.excerpt_description: true #自动截取摘要# Automatically Excerpt. Not recommend.# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false #自动截取一定程度的摘要 length: 150# Read more button# If true, the read more button would be displayed in excerpt section.read_more_btn: true #显示阅读全文按钮16.设置RSS订阅在博客主目录下执行：1npm install --save hexo-generator-feed在站点配置文件中修改：123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed然后设置主题配置文件：1234# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml17.修改文章链接样式修改文件 themes\next\source\css\_common\components\post\post.styl，在末尾添加如下css样式，：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125;18.增加阅读次数/时长和访客数Next6版本集成了多种相关功能，除了已有的busuanzi，目前又加入了symbols-count-time，二者在主题配置文件中的相关设置方法如下：1234567891011121314151617# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true #分隔符| item_text_post: true #是否统计站点总字数 item_text_total: true #是否同级文章总字数 awl: 2 #平均每个字符的长度 wpm: 300 #words per minute busuanzi_count: enable: true #是否开启不蒜子统计功能 total_visitors: true #是否统计总访客数 total_visitors_icon: user #访客数图标为人像 total_views: true #是否同级总访问数 total_views_icon: eye #访问数图标为眼睛 post_views: true #是否统计文章访问数 post_views_icon: eye #访问数图标为眼睛其中前者还需在站点配置文件中加入：12345symbols_count_time: symbols: true #是否统计字数 time: false #是否统计阅读时长 total_symbols: true #是否统计总字数 total_time: false #是否统计总阅读时长相关依赖如下：1npm install hexo-symbols-count-time --save不过symbols-count-time的数字经常不显示，不知道是不是我配置的问题，不过不担心，因为busuanzi自带了这些功能（除了阅读时长，不过这个意义不大），只需要修改next/layout/_partials/footer.swig文件如下：12345678910111213&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; &#123;% if config.symbols_count_time.total_symbols %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;post-meta-item-icon&quot;&gt; &lt;i class=&quot;fa fa-area-chart&quot;&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.symbols_count_time.item_text_total %&#125; &lt;span class=&quot;post-meta-item-text&quot;&gt;&#123;&#123; __(&apos;symbols_count_time.count_total&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/span&gt; &#123;% endif %&#125; &lt;span title=&quot;&#123;&#123; __(&apos;post.totalcount&apos;) &#125;&#125;&quot;&gt; &#123;&#123; totalcount(site, &apos;0,0.0a&apos;) &#125;&#125;字 &lt;/span&gt; &lt;!--&lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count_total&apos;) &#125;&#125;&quot;&gt;&#123;&#123;symbolsCountTotal(site)&#125;&#125;&lt;/span&gt;--&gt; &#123;% endif %&#125;以及修改next/layout/_macro/post.swig文件中的symbol部分：1234567891011121314151617181920 &#123;% if config.symbols_count_time.symbols or config.symbols_count_time.time %&#125; &lt;div class=&quot;post-symbolscount&quot;&gt; &#123;% if not theme.symbols_count_time.separated_meta %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &#123;% endif %&#125; &#123;% if config.symbols_count_time.symbols %&#125; &lt;span class=&quot;post-meta-item-icon&quot;&gt; &lt;i class=&quot;fa fa-file-word-o&quot;&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.symbols_count_time.item_text_post %&#125; &lt;span class=&quot;post-meta-item-text&quot;&gt;&#123;&#123; __(&apos;symbols_count_time.count&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/span&gt; &#123;% endif %&#125; &lt;!-- &lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count&apos;) &#125;&#125;&quot;&gt;&#123;# #&#125;&#123;&#123; symbolsCount(post.content) &#125;&#125;&#123;# #&#125;&lt;/span&gt; --&gt;&lt;span title=&quot;&#123;&#123; __(&apos;symbols_count_time.count&apos;) &#125;&#125;&quot;&gt; &#123;&#123; wordcount(post.content) &#125;&#125;字 &lt;/span&gt; &#123;% endif %&#125;最终效果如下：19.加入网易云音乐播放器首先在网页搜索网易云音乐，选择音乐，并生成外链：然后得到外链html代码：将代码粘贴到一个合适的位置，建议放在侧边栏，在Blog/themes/next/layout/_macro/sidebar.swig文件下，选择位置复制进去，不同位置效果不同：20. 参考资料http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.htmlhttps://www.jianshu.com/p/1ff2fcbdd155]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客优化之内容编辑]]></title>
    <url>%2F2019%2F01%2F22%2FHexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%B9%8B%E5%86%85%E5%AE%B9%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[前言之前介绍了Hexo博客平台的搭建，这次我们来看看博客内容怎么编辑。Hexo要求博客必须用Github风格的Markdown语言进行编辑，因此我们就需要了解文字、图片、公式等内容的编辑和排版方式。其中，图片的存储需要利用云服务器的对象存储服务或者图床，下面会具体介绍。1. Markdown编辑器既然Markdown是一门语言，那么与之对应的就有很多种编辑器，不仅如此，Markdown的语法在不同的编辑器或者平台（简书、CSDN、Github)还有所不同，既有规则的不同，还有功能的不同，不过大体的语法是相同的。据说Mac上的Markdown编辑器很不错，不过现在我们只介绍Windows上的。由于Hexo需要Github风格的Markdown语法，所以我们最好不用各类博客平台上的在线Markdown编辑器，而是采用支持Github风格Markdown语法的Markdown离线编辑器。1.1 AtomAtom是由Github开发的一款文字与代码编辑器，可以用Git进行版本控制。既然是Github开发的，那么就少不了开源社区众多的支持，所以其支持的语言很丰富，功能也很强大，基本上每个小功能都有一堆的插件供你下载安装，具有极强的定制性,这个比较适合喜欢折腾的人…可以看到，Atom就像一个自带版本库的IDE，支持各种语言，对于所需要的包（主题、语言支持、实时预览等功能）都可以直接在界面中选择安装，非常的方便，不过由于我写博客主要是在Windows上，所以暂时不想这么折腾，以后只用Linux的时候可以考虑用这个，各种包下下来，估计有差不多1个G了，毕竟不是主要面向的Markdown。1.2 TyporaTypora是一款免费的Markdown编辑器,支持Windows,OS X和Linux，单就Markdown来说，不逊色于Atom。另外Typora能够实时将Mardown语法转换为渲染后的画面显示，注意，不是并排显示，而是直接替换原有的Markdown部分。其还支持图片直接拖入，自动生成Markdown语句，word方式生成表格、插入图片、latex公式编辑、代码块高亮、自动显示和插入目录、主题选择等，可谓相当强大了。主题背景很爽，略逊色于Atom的背景，不过相当简洁了，很照顾我这类选择困难症患者。1.3 MarkdownPad2MarkdownPad2是Windows平台曾经最优秀的Markdown编辑器了，其部分免费，如果要使用Github风格Markdown语法或者其他高级功能，则需要收费，当然。。。也有破解码：12Email:Soar360@live.comLicense:GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ==然后在工具-选项中设置自己的偏好：其优点在于，既能够并排实时预览，还能够在网页中查看效果：看上去很不错的样子，不过其自身不具有图片居中、公式编辑、流程图绘制等功能的，要想使用需要结合html语法和添加样式表等方式扩展，不过这种方式实时预览效果会不大好。PS：Windows上使用MardownPad2的时候，实时预览功能一般不能使用，所以需要下载依赖Awesomium 1.6.6 SDK。1.4 HexoEditorHexoEditor，顾名思义，是开源社区贡献者自主开发的一个针对Hexo的博客编辑器，支持自动创建post，也支持图片的云端上传功能，对于公式编辑、表格、流程图等也都支持，不过目前我试用的体验一般，有很多Bug，而且安装方式不适合不懂电脑的用户：暂时不推荐使用，不过待其成熟还是可以试试的。综上呢，我在Windows上使用的Markdown编辑器是Typora~2. Markdown语法Markdown语法差异比较多，不过常用的语法基本上在各个编辑器的快捷指令中都包含了，所以不用担心，下面我介绍一些常用的语法：标题：利用#符号可以设定标题，1个表示一级标题，2个表示二级标题，如此类推；引用：利用&gt;可以创建引用块；代码：利用左上角`将待显示内容包起来即可；代码块：利用3个`描述即可创建代码块(之所以不敲出来是因为这样会让内容变乱)；斜体：利用ctrl+I即可实现斜体；加粗：利用ctrl+B即可实现粗体；线状：水平分割线可以用---或者***实现，下划线可以用ctrl+U实现，删除线可利用~~包裹实现；列表：无序列表用-，有序列表用数字+.。以上可以看上面介绍MarkdownPad2的图效果，对于图片、公式、表格、流程图等特殊元素，我会在下面具体介绍。3. 图片及链接编排图片实际上是一种超链接，所以链接的种类可以分为跳转链接、图片链接两种，具体如下：3.1 跳转链接跳转链接分为两种：站外跳转和站内跳转：站外跳转：这种方式很简单，利用下面的方式即可实现点击跳转：1[说明](链接)站内跳转：这种方式适用于目录跳转或者引用跳转，这个要用html语法，不过我试验了多次都不成功，:sob:3.2 图片编辑与排版图片的超链接创建方式有三种：Markdown语法实现：![](链接)拖入/粘贴实现：将剪切板或者本地图片拖入Typora可以自动生成Mardown语句，链接会自动显示为文件本地位置，也可以在设置里面设置默认存放位置，以及绝对路径or相对路径。利用ctrl+shitf+I或者工具栏-插入图片可以点击插入本地图片。除此之外，Typora的自动创建图片链接的方式可以让图片自动居中，如果我们需要让图片居中或者放缩，则需要用到下面的语法：1&lt;div align=center&gt;&lt;img src = "图片链接" height =xxx width = xxx /&gt;&lt;/div&gt;3.3 图片存储无论是GitPages、CodingPages还是云服务器，其提供的免费存储空间都是有上限的，所以我不建议在写博客的时候使用相对路径存储图片，然后上传，这种方式仅针对文字为主的博主。这种方式一般采用图片链接为相对路径并且在_post目录下新建一个存放图片的目录，并命名为博客名即可。下面我主要讲利用云服务器对象存储功能实现的图床。3.3.1 七牛云七牛云是早期比较方便的一个图床，提供了免费的10G存储空间，而且还提供了免费的对象存储空间域名：除此之外还有图像处理样式，这个是Mardown语法所不具备的，要想使用下面的功能，只需要在图片链接后面加上相应的图像处理样式即可自动处理，可谓极其方便了。不过七牛云不支持文件夹的上传，所以需要利用相应的工具实现：Windows端同步上传客户端（QSunSync)：极简图床以及一些支持七牛云的服务端。以上均需要提供七牛云的密钥，这个可以在七牛云控制台的个人中心-密钥管理看到。但是七牛云有一个很大弊端：其提供的免费域名每一个月更新一次，也就是说你上传的博客中所采用的图片每个月都需要重新上传，如果想要自定义域名，则需要备案，但是我们采用的服务器并不是我们的，所以不能备案。3.3.2 阿里云阿里云对于新用户提供了优惠服务，即免费享用6个月40G的OSS对象存储空间，免费期过后，一年也只需要9元，相当的实惠，并且其提供的域名是永久的，所以不用担心失效。问题在于，图片的云端管理包括：对象存储、下行流量、CDN回源流量、请求次数等服务，仅仅对象存储只能满足我们的存储要求，如果需要供他人或者自己访问，则需要另行购买资源包，很贵！如果要是用的话，记得选择“标准存储”和“公共读私有写”模式，阿里云支持文件夹上传和图片处理服务。3.3.3 腾讯云腾讯云COS对象存储相对阿里云OSS来说，提供了更加便利的免费服务：是不是很爽，虽然我不大相信未来也不收费，我看了下，价格相对阿里云来说便宜很多，而且未来我也有能力支撑这笔费用了。可以看到除了图像处理服务，其他的功能都有，而且腾讯云还提供了客户端上传工具cosbrowser。综上呢，我选择了腾讯云对象存储，我们利用Typora的自动生成图片链接以及自动复制到相应目录的功能，然后将图片文件夹利用cosbrowser上传至云端，再利用Typora的替换功能将本地路径前缀替换为云端路径前缀即可完成图片的管理。4. 公式编排公式编排所利用的是Latex语法，这一点Typora已经集成了，除此之外我们在上传Hexo的时候需要开启mathjax或者katex选项。具体内容如下：4.1 Latex公式编辑Latex公式编辑主要依赖Latex语法，在Markdown中需要利用$$包裹实现行内公式编辑，或者$$$包裹实现行间公式编辑，具体的Latex语法可能对于普通玩家来说比较麻烦，不过有以下几种便捷方式：eqneditor：sciweavershostmath：Mathtype：平时敲公式都使用的Mathtype，所以就像能不能把公式直接转换为Latex语句，事实证明可以：在mathtype-preferences中设置如上的格式，然后复制公式到任意地方，即可自动转为latex语句，只需要将首尾字符替换为$$或者$$$即可，是不是很简单~​4.2 Hexo的Latex渲染支持虽然我们的Markdown编辑器支持了Latex语法，但是Hexo还没支持，所以需要在其中开启相关开关，我是用的是Next6主题，其中自带了相关接口，主题相关内容我后面会单独开博客介绍。5. 表格和流程图编排5.1 表格我们先看下一个例子：12345| Tables | Are | Cool ||----------|:-------------:|------:|| col 1 is | left-aligned | $1600 || col 2 is | centered | $12 || col 3 is | right-aligned | $1 |上例中------表示左对齐，:------：表示居中，-----:表示右对齐，效果如下：不过每次这么敲太麻烦了，还好Typora提供了表格工具：ctrl+T或者段落-表格：这样就跟word一样了，不过缺点在于，我们很难创建复杂表格，虽然excel支持转html的功能，不过bug多多~5.2 流程图流程图我一般都用visio画，因为功能强大，不过呢如果用visio的话，就只能用截图显示了，那就避免不了背景冲突，如果不在意可以用。这里我要说的是在markdown里面写流程图，流程图的创建逻辑无非就是前后的逻辑流，所以可以根据各个对象之间的逻辑进行绘制。Typora本身支持流程图、时序图、甘特图等，可利用创建代码块的方式创建。其中flow支持流程图：而mermaid既支持流程图，又支持时序图和甘特图：是不是很厉害，不过呢，要想在hexo中支持这些功能，还需要下载安装一些插件才行，后续博客我会介绍。6. 内容导航有了以上的内容之后，我们怎么设置内容导航，更方便自己和读者了解文章呢：对此我们就需要设置分类、标签以及目录。6.1 Pages导航在hexo中，首页是默认开启的，而关于，标签，分类，归档这些都是需要自己创建和设置的。对于这些页面，我们需要完成以下步骤：step1 修改scaffolds中的page样板：1234title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;type: &#123;&#123;title&#125;&#125;comments: false其中，title表示默认的标题，即关于，标签，分类，归档，其type也是相应的，date会自动获取当前时间,而comments设置为false的原因是，这些界面不需要评论窗口。step2 创建pages:1234hexo new page categorieshexo new page tagshexo new page abouthexo new page archives注意拼写！Step3 hexo配置：在hexo的主题配置文件中设置menu参数：123456menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive这样我们的博客就有了上述页面Step4 设置博客标签,hexo的配置语言支持yaml格式，所以下面的方式可以设置多个标签：123tags: - Markdown - 对象存储Step5 设置博客分类：123categories: - 软件安装与使用 - Hexo分类与标签有所不同，上面这种方式表示二级分类，并不是并行的！另外一定要注意-后面有空格！最后我们的博客分类和标签页面都能找到相应的博客，各自博客上也会显示其分类和标签~Step6 简介就是about页面，这个页面就是用来写我们的个人简介的，所以可以直接在blog/source/about/index.md中直接按照写博客的方式书写我们的简介。6.2 目录博客的目录有两种生成方式，一种是博客自带的目录，可以利用html语法写，不过Typora自带目录生成，可利用[toc]或者段落-内容目录自动生成目录，并支持跳转，不过这种方式Markdown本身不兼容，所以需要下载安装一些配置,还需要修改一些配置：如果不想折腾的话，hexo的一些主题已经自带了目录功能，以我使用的Next6主题为例：12345678toc: enable: true # Automatically add list number to toc. number: false # If true, all words will placed on next lines if header width longer then sidebar width. wrap: false # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` var. max_depth: 6enable表示是否开启目录生成，如果开启的话，会自动检测我们Markdown中的标题;number表示是否自动加入标题序号，按照1,1.1,1.1.1等进行标注；warp表示如果标题过长的话，是否允许标题换行；max_Depth表示允许的最大标题等级。7. pdf编辑如果我们想在word上编辑，然后放到博客上，可以先利用word另存为pdf，然后利用pdf插件实现。这一点在我所使用的Next6主题中采用了，具体如下：step1 安装pdf插件进入我们博客的主题目录，如blog/themes/next6，执行：1git clone https://github.com/theme-next/theme-next-pdf source/lib/pdf或者在blog/目录下执行：1npm install hexo-pdf --save上面第一种是next官方支持的，后者是最常用的。step2 hexo 配置（如果采用第一种安装方式）123456pdf: enable: true per_page: true height: 500px pdfobject: cdn: //cdnjs.cloudflare.com/ajax/libs/pdfobject/2.1.1/pdfobject.min.jsstep3 存放pdf存放pdf的方式跟图片一模一样，具体我就不说了step4 插入pdf官方支持的方式是：1[](pdf链接)而采用方法二的使用方式是：1&#123;% pdf pdf链接 %&#125;官方的那个我总是编译出问题，所以下面给出方法二的效果：]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>对象存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Hexo将博客部署到GitPages和CodingPages]]></title>
    <url>%2F2019%2F01%2F21%2F%E5%88%A9%E7%94%A8Hexo%E5%B0%86%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2%E5%88%B0GitPages%E5%92%8CCodingPages%2F</url>
    <content type="text"><![CDATA[前言本人有做总结的习惯，以往都是写word文档，然后存放在qq里面，不过这样的话不是很方便管理，于是萌生了写博客的想法。既然写博客，那何不更有仪式感一点，所以就利用Hexo制作了一个属于自己的博客页面，然后托管至GitPages和CodingPages上，前者是国外的，后者是国内的，方便访问加速。最后呢，我在阿里云上注册了一个域名进行域名解析，于是乎，我就有了一个属于自己的博客网站。1. 安装依赖Hexo是一个快速，简单而强大的博客框架，可以使用Markdown（或其他语言）编写文章，Hexo可以在几秒钟内生成具有美丽主题的静态文件。其包管理器是Node.js平台的npm，而我们的博客网站时需压迫依托于一个有效的站点和空间的，这里可以选择国外的GitPages，或者国内的CodingPages，分别属于Github和Coding。其中Markdown部分我会单独有一篇博客介绍。(1) Node.jsNode.js是基于Chrome JavaScript运行时建立的一个平台，npm是node.js的包管理工具.在这里可以下载Node.js,请选择适合自己电脑的最新版本，然后一路next安装，记得选择添加至环境变量，不然后面很麻烦。安装完之后利用cmd或者其他命令行工具输入：12node -vnpm -v如果打印出版本号，那么说明安装成功，否则试着手动配置一下环境变量。(2) GitGit Bash是一款git的命令行工具，支持Mac、Linux和Windows，点击这里可以下载安装。最好选择添加至环境变量，或者手动添加git的bin路径至环境变量，这样就能在Windows命令行或者Git Bash工具中直接使用git命令。git主要用于版本控制，另外这里hexo的操作很多都需要在git bash进行，不然会出现奇怪错误，想要了解更多git知识可以参考以下链接：Git教程—廖雪峰网站。(3) HexoHexo就是用来将我们用Markdown写好的博客生成html页面,并自动部署到Github或者Coding上，其依托于npm包管理器，所以可以直接下载：1npm install -g hexo-cli这条语句的作用是将hexo命令行工具安装在全局，要注意的是npm安装软件时，经常会出现warning，不过不要紧，只要不是error就行。2. 建站Step1 初始化站点首先我们新建一个存放博客的文件目录，例如blog/，然后进入该目录，利用命令行工具进行站点部署hexo init，也可以直接：123mkdir blogcd bloghexo init当然也可以直接执行：1hexo init blog这一步会将远端的hexo文件克隆到博客目录。Step2 安装依赖进入blog目录，然后执行：1npm install自动安装hexo所需要的依赖项，此时的博客目录应该如下：1234567891011.|—— node_modules|—— scaffolds|—— source| |—— _posts|—— themes| |—— landscape|—— .gitignore|—— _config.yml|—— db.json|—— package.json其中node_modules里面放的是利用npm install --save下载安装的软件，scaffolds里面存放的是各种脚手架模板，也就是说可以创建post、page、draft三种页面，会存放在source目录。source目录内的文件就是待发布的文件，themes目录里面存放的是我们所引用的主题模板，默认的是landscape主题。.gitignore存放的是我们部署到github或者coding.me上时需要忽略的文件，_config.yml是站点配置文件，与之相应地，在主题中还有一个主题配置文件，这两个在后面的配置中很重要。Step3 创建页面在blog目录下执行：1hexo clean;hexo g;hexo s即清空站点静态页面缓存（清空public文件夹)，在本地生成静态页面（在public文件夹)，开启本地服务器，这时候，我们可以在网页中输入：1http://localhost:4000就可以看到如下的画面效果：这个页面是hexo初始化的一个简单页面，主题是landscape。3. 托管GitPages上面的整个过程都是基于本地的，只能在本地给自己看，局限性很大，为了将博客部署到网站上，我们可以利用Github提供的GitPages免费站点服务，自带域名和1G免费空间，存放网页绰绰有余（图片不要放进去）。Step1 Windows端搭建Github环境这一部分内容可以见我之前的博客，从而完成Github注册和ssh环境配置。Step2 新建博客repository在github新建一个名为你的用户名.github.io的repository，这样做的好处是避免生成的域名太长,具体可执行：123456echo "# nightmaredimple.github.io" &gt;&gt; README.mdgit initgit add README.mdgit commit -m "first commit"git remote add origin https://github.com/nightmaredimple/nightmaredimple.github.io.gitgit push -u origin master从而在你的github仓库中有了一个属于你博客站点文件存放的位置，一般设为master分支。Step3 Hexo配置首先安装部署依赖：1npm install hexo-deployer-git --save然后在站点配置文件blog/_config.yml中配置deploy为：12345deploy: type: git repo: github: https://github.com/nightmaredimple/nightmaredimple.github.io.git branch: masterStep4 完成部署在blog目录下执行：1hexo clean;hexo g;hexo d执行完毕后，你就可以在你的github相应的博客repository下看到部署后的博客站点文件，通过链接可以看到属于你的博客网页，例如我的：1http://nightmaredimple.github.io也可以通过ping网站的方式观察，二者的ip一致~4. 托管CodingPagesCoding几乎算得上是Github的汉化版，缺点就是资源少，但是因为在国内，所以访问速度快。我在自己电脑上测试发现，ping自己的博客域名的延时是250+ms，很慢，所以我想同时将博客部署在Github和Coding上。既然我说Coding是Github的汉化版，那么也就是说我们之前在Windows端搭建的Github环境依然有效，并且部署过程几乎一样。Step1 Windows端搭建Coding环境首先进入Coding官网注册,然后可以直接新建项目：记得一定要初始化项目，不然无法开启pages服务。 同样地，我们还是保持项目名与用户名一致，这种创建方式可以直接替代上面github的方式，不过我还是贴出：123456echo "# nightmaredimple.coding.me" &gt;&gt; README.mdgit initgit add README.mdgit commit -m "first commit"git remote add origin https://git.dev.tencent.com/nightmaredimple/nightmaredimple.gitgit push -u origin master要注意的是，由于coding和腾讯云合作了，所以我们的coding项目会自动同步到腾讯云，也就是说上面的repository链接还可以写成：1https://git.coding.net/nightmaredimple/nightmaredimple.git**Step2 部署项目公钥** 将我们在部署Github时生成的ssh公钥复制，进入控制台/设置/管理公钥，新建一个公钥复制进去,当然还可以新建一个个人公钥，以面向所有项目，直接点击头像，进入个人账户/SSH公钥即可。Step3 开启CodingPages服务进入控制台/代码/Pages服务,一键开启Coding PagesStep4 Hexo配置这里跟Github一样，我们可以一起安装部署，写入如下：123456deploy: type: git repo: github: https://github.com/nightmaredimple/nightmaredimple.github.io.git coding: https://git.coding.net/nightmaredimple/nightmaredimple.git branch: master据其他人的踩坑记录，貌似需要在blog/source下创建一个名为Staticfile的空文件。Step5 完成部署在blog目录下执行：1hexo clean;hexo g;hexo d执行完毕后，你就可以在你的coding.net或者腾讯云相应的博客repository下看到部署后的博客站点文件，通过链接可以看到属于你的博客网页，例如我的：1http://nightmaredimple.coding.me再ping一下，发现延迟只有40ms了。5. 自定义域名完成以上操作之后，我们就拥有了两个博客域名，但是我还是想申请一个属于自己的域名，所以去阿里云绑定了一个域名，然后将域名解析到了自己的博客域名，具体过程如下：Step1 完成阿里云注册阿里云的域名申请是在万网，我们先根据要求完成注册和实名认证，很快。Step2 申请域名认证成功之后，我们通过查询域名看看那些域名可以用，并且根据合适的名称，合适的价格选择想要的后缀，然后进行域名的购买，不贵。Step3 域名解析申请到我们的域名之后，就需要绑定我们的博客网站了，但是只能绑定其中一个，这里我绑定的coding的，因为访问更快。而域名解析的话，最好解析两个，一个是ip，一个是域名，对于我们博客网站的ip，可以通过ping自己的博客域名获取。添加两条解析记录,其中A对应ip，CNAME对应博客原始域名：Step4 域名绑定首先在blog/source下新建一个CNAME文件，里面填写我们买的域名，然后相应的博客网站进行配置。对于GitPages，我们需要进入博客repository的setting中，添加GitPages的Custom Domain：而对于CodingPages，则是直接在Pages服务中添加新域名：上述均有强制HTTPS访问选项，这个选项开启之后，我们的网站就变成了https://xxxxx，否则就是http。Step5 完成部署首先填写站点配置文件_config.yml中的url选项，填写你的域名。然后选做：1npm install hexo-generator-cname这一项可以帮助你自动添加CNAME,接着在blog目录下执行：1hexo clean;hexo g;hexo d等待十几秒就能看直接通过我们绑定的域名访问博客啦~PS:最近Coding时不时崩溃，所以暂时绑定GitPages。]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitPages</tag>
        <tag>Coding Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows平台下Github远程仓库的搭建]]></title>
    <url>%2F2019%2F01%2F19%2FWindows%E5%B9%B3%E5%8F%B0%E4%B8%8BGithub%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言Github是一个面向开源及私有软件项目的托管平台,拥有超过900万开发者用户，有众多的开源项目供研究者学习。还提供了很多项目管理功能，方便多终端同步管理项目。本文将介绍Windows平台下Github远程仓库的注册，部署和绑定,不介绍Github的使用！1. Github注册进入Github网站进行注册，界面如下：输入用户名、邮箱和密码等相关信息后完成Github的注册和登录，对于修改用户名和密码等相关信息要求可以点击右上角用户图标-选中Settings:设置好相关配置之后就可以试着在右上角点击 ` + `，新建一个repository，设置名称，即可开始上传你的项目文件了。2. Windows本地部署Github2.1. Github for WindowsGithub for Windows是Github在Windows本地的客户端，点击这里可以下载安装。2.2 Git BashGit Bash是一款git的命令行工具，支持Mac、Linux和Windows，点击这里可以下载安装。最好选择添加至环境变量，或者手动添加git的bin路径至环境变量，这样就能在Windows命令行或者Git Bash工具中直接使用git命令。3. Windows本地绑定配置GithubStep1 配置Git账户在Git bash命令行输入：12git config --global user.name "你的GitHub用户名"git config --global user.email "你的GitHub注册邮箱"Step2 生成新的ssh key12ssh-keygen -t rsa -C "你的GitHub注册邮箱"cat C:/Users/用户名/.ssh/id_rsa.pubStep3 Github官网配置ssh key在Github官网个人主页的Settings中新建SSH Key，并将上一步打印的信息复制进其中:在Git Bash命令行输入：1ssh -T git@github.com如果打印You&#39;ve successfully authenticated!，则说明验证成功。4. Windows本地托管项目Step1 新建仓库首先在自己电脑某个地方新建一个文件夹，即新建一个仓库，假设名字为Test,然后在git bash中进入该文件夹目录，执行1git init从而Test目录下会多出一个.git目录Step2 将文件添加到版本库假设在该仓库下有一个待上传的文件夹project，该文件夹中有一些项目文件，同样是进入该文件夹，执行：1git remote add origin 项目git地址从而在你的用户下创建了一个名为project的repository。Step3 将版本库同步到本地如果之前已经创建过了，则需要将Github段的文件同步过来：1git pull 项目git地址与此同时，本地目录上会多出原本在github上的文件 。Step4 更新版本库首先利用 git add 目录（如果是一个点，则提交所有文件，否则指定文件），然后利用git commit –m “版本2” (-m后面跟提示信息，这个提示信息是一定要写的，记录我们提交的过程，写清晰为什么提交或修改了什么是非常有用的。)，最后将其推送至版本库：1git push -u origin master将当前分支推送到版本库master分支]]></content>
      <categories>
        <category>软件安装与使用</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fbaidu_verify_N5kYz6Tb7k.html</url>
    <content type="text"><![CDATA[N5kYz6Tb7k]]></content>
  </entry>
  <entry>
    <title><![CDATA[archives]]></title>
    <url>%2Farchives%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[标签]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Piao Huang]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[I am studying for my master degree in Huazhong University of Science &amp; Technology (HUST). My research interests are in computer vision and deep learning.EducationHuazhong University of Science and Technology (HUST) 2017.12 ~National Key Laboratory of Science and Technology on Multi-spectral Information Processing, School of Artificial Intelligence and Automation.Master of Control Science and Engineering.Huazhong Agricultural University (HZAU) 2013.9 ~ 2017.7Key Laboratory of Agricultural Equipment in the Middle and LowerReaches of the Yangtze River, Ministry of Agriculture, School of EngineeringBachelor of EngineeringPublicationsRefinements in Motion and Appearance for Online Multi-Object TrackingPiao Huang , Shoudong Han, Jun Zhao, Donghaisheng Liu, HongweiWang, En Yu, and Alex ChiChung Kot1st place of MOT Challenge 16&amp;17. 2020.3arxiv codeFemale and Male Identification of Early Chicken Embryo Based on Blood Line Features of Hatching Egg Image and Deep Belief NetworksZhu Zhihui, Tang Yong, Hong Qi, Huang Piao, Wang Qiaohua, Ma MeihuTransactions of the Chinese Society of Agricultural Engineering (Transactions of the CSAE, EI), 2018.paperExperienceFace recognition access control system 2018.07 ~ 2019.08Responsible for the overall framework and all algorithm implementations1:1 and 1:N (N=8000)Security Intelligence System 2018.06 ~ 2018.12Responsible for the Face recognition access control systemm:N(N = 10000)Awards and HonorsNational 1st prize in CUMCM, 2015National 2nd prize in NPMCM, 2015Meritorious Winner in MCM/ICM, 2016National 2nd prize in NPMCM, 2018]]></content>
  </entry>
</search>
