<!-- build time:Tue Apr 07 2020 13:04:50 GMT+0800 (中国标准时间) --><!DOCTYPE html><html class="theme-next gemini" lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.min.css"><link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="stylesheet" href="/css/main.css?v=6.7.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/128x128.png?v=6.7.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/32x32.png?v=6.7.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/16x16.png?v=6.7.0"><link rel="mask-icon" href="/images/logo2.svg?v=6.7.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"6.7.0",sidebar:{position:"left",display:"post",offset:12,b2t:!0,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!0,lazyload:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/8f5c5484.js","daovoice"),daovoice("init",{app_id:"8f5c5484"}),daovoice("update")</script><meta name="description" content="前言前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。"><meta name="keywords" content="深度估计"><meta property="og:type" content="article"><meta property="og:title" content="基于深度学习的单目深度估计综述"><meta property="og:url" content="https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/index.html"><meta property="og:site_name" content="见渊の博客"><meta property="og:description" content="前言前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165941244.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165953514.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170010902.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170123899.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170130969.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170148431.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170229040.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170322182.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172253721.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172519943.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172545276.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172617329.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172710149.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170434918.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171923296.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171722320.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171840905.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308182452471.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173754057.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173806167.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173814812.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173833858.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173857556.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173908649.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173916000.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173932875.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174121034.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174130252.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174828519.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174851298.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174909501.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174943651.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174958809.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175216815.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175228213.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175304525.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175351205.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175401181.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175411913.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175421097.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175431143.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175529893.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175539828.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180333423.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180412887.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180416682.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180432526.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180445762.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180455841.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180520856.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180529831.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180539369.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180614033.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180623440.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180707605.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180715231.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180726951.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180928316.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181145568.png"><meta property="og:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181208321.png"><meta property="og:updated_time" content="2020-03-08T09:30:00.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="基于深度学习的单目深度估计综述"><meta name="twitter:description" content="前言前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。"><meta name="twitter:image" content="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165941244.png"><link rel="alternate" href="/atom.xml" title="见渊の博客" type="application/atom+xml"><link rel="canonical" href="https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>基于深度学习的单目深度估计综述 | 见渊の博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">见渊の博客</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签<span class="badge">41</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类<span class="badge">13</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档<span class="badge">33</span></a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><a href="https://github.com/nightmaredimple" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><div class="reading-progress-bar"></div><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="黄飘"><meta itemprop="description" content="直到这一刻微笑着说话为止，我至少留下了一公升眼泪"><meta itemprop="image" content="/images/author.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="见渊の博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">基于深度学习的单目深度估计综述</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-03-08 17:30:00" itemprop="dateCreated datePublished" datetime="2020-03-08T17:30:00+08:00">2020-03-08</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/3D视觉/" itemprop="url" rel="index"><span itemprop="name">3D视觉</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2020/03/08/基于深度学习的单目深度估计综述/#comments" itemprop="discussionUrl"><span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2020/03/08/基于深度学习的单目深度估计综述/" itemprop="commentCount"></span> </a></span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span><span title="本文字数">9.7k字</span></div></div></header><div class="post-body" itemprop="articleBody"><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>前段时间有思考过结合3D信息来辅助多目标跟踪任务，不过效果没有达到我的预期。一方面是多目标跟踪相关数据集除了KITTI之外缺乏多任务标注信息，另一方面单目深度估计对于密集拥挤人群的效果很差。所以我觉得对于稀疏场景、车辆跟踪或者提供真实3D信息和相机信息的场景任务更有意义。下面的总结主要是我2019年初整理的文献，时效性可能还没跟上。</p></blockquote><a id="more"></a><h2 id="1任务介绍"><a href="#1任务介绍" class="headerlink" title="1任务介绍"></a>1任务介绍</h2><p>深度估计是计算机视觉领域的一个基础性问题，其可以应用在机器人导航、增强现实、三维重建、自动驾驶等领域。而目前大部分深度估计都是基于二维RGB图像到RBG-D图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的Shape from X方法，还有结合SFM(Structure from motion)和SLAM(Simultaneous Localization And Mapping)等方式预测相机位姿的算法。其中虽然有很多设备可以直接获取深度，但是设备造价昂贵。也可以利用双目进行深度估计，但是由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度也较高，尤其是对于低纹理场景的匹配效果不好。而单目深度估计则相对成本更低，更容易普及。</p><p>那么对于单目深度估计，顾名思义，就是利用一张或者唯一视角下的RGB图像，估计图像中每个像素相对拍摄源的距离。对于人眼来说，由于存在大量的先验知识，所以可以从一只眼睛所获取的图像信息中提取出大量深度信息。那么单目深度估计不仅需要从二维图像中学会客观的深度信息，而且需要提取一些经验信息，后者则对于数据集中相机和场景会比较敏感。</p><p>通过阅读文献，可以将基于深度学习的单目深度估计算法大致分为以下几类：</p><ul><li><strong>监督算法</strong></li></ul><p>顾名思义，直接以2维图像作为输入，以深度图为输出进行训练：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165941244.png" alt="image-20200308165941244"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308165953514.png" alt="image-20200308165953514"></p><p>上面给的例子是KITTI数据集中的一组例子，不过深度图可能看的不是很明显，我重新将深度图涂色之后：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170010902.png" alt="image-20200308170010902"></p><ul><li><strong>无监督算法</strong></li></ul><p>由于深度数据的获取难度较高，所以目前有大量算法都是基于无监督模型的。即仅仅使用两个摄像机采集的双目图像数据进行联合训练。其中双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或是将双目图像中各个像素点的对应问题看作是立体匹配问题进行训练。左视图-右视图示例：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170123899.png" alt="image-20200308170123899"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170130969.png" alt="image-20200308170130969"></p><p>视差，以我们人眼为例，两只眼睛看到的图像分别位于不同的坐标系。将手指从较远地方慢慢移动到眼前，会发现，手指在左眼的坐标系中越来越靠右，而在右眼坐标系中越来越靠左，这种差异性就是视差。与此同时，可以说明，视差与深度成反比。除此之外，由于摄像机参数也比较容易获取，所以也可以以相机位姿作为标签进行训练。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170148431.png" alt="image-20200308170148431"></p><ul><li><strong>Structure from motion/基于视频的深度估计</strong></li></ul><p>这一部分中既包含了单帧视频的单目深度估计，也包含了多帧间视频帧的像素的立体匹配，从而近似获取多视角图像，对相机位姿进行估计。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170229040.png" alt="image-20200308170229040"></p><h2 id="2-数据集介绍"><a href="#2-数据集介绍" class="headerlink" title="2 数据集介绍"></a>2 数据集介绍</h2><h3 id="2-1-KITTI"><a href="#2-1-KITTI" class="headerlink" title="2.1 KITTI"></a>2.1 KITTI</h3><p>KITTI是一个多任务属性的数据集，其中原始数据采集平台装配有2个灰度摄像机，2个彩色摄像机，一个Velodyne 64线3D激光雷达，4个光学镜头，以及1个GPS导航系统。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170322182.png" alt="image-20200308170322182"></p><p>其中包含有200+G的原始数据，而有关户外场景的有175G数据。对于这些数据，所标注的任务包含：立体图像匹配、光流、场景流、深度估计（单目或者基于3D点云/激光数据的深度估计）、视觉测距、目标检测（2D/3D物体检测、俯视图物体检测）、目标跟踪、道路/车道线检测、目标分割等。</p><p>链接：<a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">http://www.cvlibs.net/datasets/kitti/eval_object.php</a></p><h3 id="2-2vKITTI"><a href="#2-2vKITTI" class="headerlink" title="2.2vKITTI"></a>2.2vKITTI</h3><p>从名字可以看出这个数据集跟KITTI有关联，其对应KITTI的原始数据和各类任务，创建了全新的虚拟图像，当然，并不是所有原始数据都能对应得上。这里的“虚拟”指的是：左右摄像头15°和30°偏转画面、清晨、晴天、多云、雾天、下雨等基于原图的渲染图像。共计14G原始RGB图像，对应的目标检测、目标跟踪、目标分割标注都存在。这一数据集的意义在于可以缓解深度信息对于光线的敏感问题。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172253721.png" alt="image-20200308172253721"></p><p>链接：<a href="http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds">http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds</a></p><h3 id="2-3Cityscapes"><a href="#2-3Cityscapes" class="headerlink" title="2.3Cityscapes"></a>2.3Cityscapes</h3><p>Cityscapes的数据取自德国的50多个城市的户外场景，其中数据包含有左右视角图像、视差深度图、相机校准、车辆测距、行人标定、目标分割等，同时也包含有类似于vKITTI的虚拟渲染场景图像。其中简单的左视角图像、相机标定、目标分割等数据需要利用学生账号注册获取，其他数据需要联系管理员获取。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172519943.png" alt="image-20200308172519943"></p><p>链接：<a href="https://www.cityscapes-dataset.com/downloads/">https://www.cityscapes-dataset.com/downloads/</a></p><h3 id="2-4NYU-Depth-V2"><a href="#2-4NYU-Depth-V2" class="headerlink" title="2.4NYU Depth V2"></a>2.4NYU Depth V2</h3><p>NYU Depth V2数据集中包含有428G室内场景数据，同时包含有目标分割标注、深度标注。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172545276.png" alt="image-20200308172545276"></p><p>链接：<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</a></p><h3 id="2-5-ScanNet"><a href="#2-5-ScanNet" class="headerlink" title="2.5 ScanNet"></a><strong>2.5 ScanNet</strong></h3><p>ScanNet中包含有约1500个视频序列的RGB-D数据，主要用于三维重建。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172617329.png" alt="image-20200308172617329"></p><p>链接：<a href="http://www.scan-net.org/#code-and-data">http://www.scan-net.org/#code-and-data</a></p><h3 id="2-6Make3D"><a href="#2-6Make3D" class="headerlink" title="2.6Make3D"></a>2.6Make3D</h3><p>Make3d数据集包含约1000张室外场景图片，50张室内场景，7000张合成物体。其中包含有激光-2D图像，立体图像、深度数据等。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308172710149.png" alt="image-20200308172710149"></p><h2 id="3-数据处理"><a href="#3-数据处理" class="headerlink" title="3 数据处理"></a>3 数据处理</h2><h3 id="3-1数据组成"><a href="#3-1数据组成" class="headerlink" title="3.1数据组成"></a>3.1数据组成</h3><p>以KITTI数据集为例，它没有给出深度相关的标注信息。其数据组成包括多个场景下的原始图像数据（gray、color），实例分割、目标跟踪、2d/3d目标检测等任务信息。为了方便我后续使用，我将数据结构解析如下，由于知乎不支持表格，所以我就直接截图了：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308170434918.png" alt="image-20200308170434918"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171923296.png" alt="image-20200308171923296"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171722320.png" alt="image-20200308171722320"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308171840905.png" alt="image-20200308171840905"></p><p>这一部分内容中，对于一个点云数据P:[X,Y,Z,1]^T，其中Z就是深度信息，将其转化为相机左视图的像素点坐标Q:[u,v,1]^T：</p><script type="math/tex;mode=display">ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times P</script><p>对于GPS/imu中的点G:[X,Y,Z]^T，将其转化为相机左视图的像素点坐标Q：</p><script type="math/tex;mode=display">ZQ = P\_rect\_xx \times R\_rect\_00 \times {\left( {\left. R \right|T} \right)_{velo\_to\_cam}} \times {\left( {\left. R \right|T} \right)_{imu\_to\_velo}} \times G</script><p>其中最需要注意的是第一个公式，用于深度的信息的提取，以及P_rect_xx，其前三列数据为修正后的相机内参。</p><h3 id="3-2-数据处理"><a href="#3-2-数据处理" class="headerlink" title="3.2 数据处理"></a>3.2 数据处理</h3><p>有关相机/像素/世界坐标系的知识我就不介绍了，对于相对位姿，如果将每个视频场景的第一帧的位姿视为初始位姿，那么每一帧的相对位姿计算如下：</p><p>$$ \begin{array}{l} imu2cam = \left[ {\begin{array}{*{20}{c}} {R\_rect\_00}&0\\ 0&1 \end{array}} \right] \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{imu2velo}}\\ scale = \cos \left( {latitude} \right)\\ t = \left[ {\begin{array}{*{20}{c}} {{t_x}}\\ {{t_y}}\\ {{t_z}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {scale \times longtitude \times \pi /180}\\ {scale \times er \times \log \left[ {\tan \left( {\frac{{90 + latitude}}{{360}}\pi } \right)} \right]}\\ {altitude} \end{array}} \right]\\ roll,yaw,pitch \Rightarrow R = {R_z}{R_y}{R_x}\\ \Rightarrow T = \left[ {\begin{array}{*{20}{c}} R&t\\ 0&1 \end{array}} \right]\\ \Rightarrow {T_{t \to s}} = imu2ca{m_t} \times T_s^{ - 1} \times {T_t} \times imu2cam_t^{ - 1} \end{array} $$<br>由于深度信息的转换需要用到相机内参，所以对于图像的缩放需要先处理，假如图像大小的放缩尺度为[zoom_x,zoom__y]，那么相机内参的变化如下：</p><p>$$ \begin{array}{l} \left\{ \begin{array}{l} P\_rect\_00[0,:] * = zoom\_x\\ P\_rect\_00[1,:] * = zoom\_ \end{array} \right.y\\ \Rightarrow K = P\_rect\_00[:,:3] \end{array} $$<br>根据世界坐标系的转换：</p><p>$$ P\_velo2im = P\_rect\_00 \times R\_rect\_00 \times {\left[ {\begin{array}{*{20}{c}} R&T\\ 0&1 \end{array}} \right]_{velo2cam}} $$<br>由于要求点云数据的反射强度为1，所以需要先将点云数据的反射强度置为1：</p><p>$$ \begin{array}{l} Z\left[ {\begin{array}{*{20}{c}} x\\ y\\ 1 \end{array}} \right] = {\left( {P\_velo2im \times \left[ {\begin{array}{*{20}{c}} {{X_v}}\\ {{Y_v}}\\ {{Z_v}}\\ 1 \end{array}} \right]} \right)^T} = \left[ {\begin{array}{*{20}{c}} {X'}\\ {Y'}\\ Z \end{array}} \right]\\ \Rightarrow x = X'/Z,y = Y'/ZZ = Z \end{array} $$<br>最后我们只需要保留满足图像边界约束的点的深度信息，如果映射得到的点坐标相同，则只保留深度更小的。</p><p>那么对于网络训练过程中的数据增强，则可以进行多种变换，下面列出几种基础的：</p><ul><li><p>随机水平翻转，所以需要改变相机内参的水平平移量cx=w-cx；</p></li><li><p>随机尺度变换并剪切至固定大小：<br>$$ \left[ {\begin{array}{*{20}{c}} {{f_x}}&{{c_x}}\\ {{f_y}}&{{c_y}} \end{array}} \right] = \left[ {\begin{array}{*{20}{c}} {{f_x} \times scal{e_x}}&{{c_x} \times scal{e_x} + offse{t_x}}\\ {{f_y} \times scal{e_y}}&{{c_y} \times scal{e_y} + offse{t_y}} \end{array}} \right] $$</p></li></ul><h3 id="3-3评价指标"><a href="#3-3评价指标" class="headerlink" title="3.3评价指标"></a>3.3评价指标</h3><p>KITTI数据集在考虑深度估计信息误差时，所以判定的时候只取0.40810811H ~ 0.99189189H，0.03594771W ~ 0.9640522229W部分图像区域，当然也经常会只取50m或者80m范围内的深度信息。为了让预测深度和真实深度的数量级范围一致，一般会用二者深度的中位数作为尺度，对预测深度信息进行尺度放缩。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308182452471.png" alt="image-20200308182452471"></p><h2 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4 相关工作"></a>4 相关工作</h2><h3 id="4-1基于单目视觉的深度估计"><a href="#4-1基于单目视觉的深度估计" class="headerlink" title="4.1基于单目视觉的深度估计"></a>4.1基于单目视觉的深度估计</h3><ul><li><p><strong>传统编解码结构</strong></p><p>深度估计任务是从二维图像到二维深度图像的预测，因此整个过程是一个自编码过程，包含编码和解码，通俗点就是下采样和上采样。这类结构主要有FCN框架和U-net框架，二者的下采样过程都是利用了卷积和池化，而上采样利用了逆卷积/转置卷积(Deconvolution)和upsample。</p></li><li><p><strong>深度回归网络</strong></p><p>早期的单目深度估计网络框架基本上都是直接利用了上面所提到的两个基础框架进行了预测，为了让这类框架更好的应用于深度估计问题，一般都从以下几个方面着手：更好的backbone模型、多尺度、更深的网络。</p><p>以3DV 2016中《Deeper Depth Prediction with Fully Convolutional Residual Networks》一文为例，其提出了FCRN网络框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173754057.png" alt="image-20200308173754057"></p><p>其网络框架主体是基于Resnet-50，在上采样过程中采用了独特的方式，将逆卷积用up-pooing+conv的方式替代了，将Resnet中的project模块进行了一定的改进。</p><p>其中上采样过程中将特征图利用0元素进行填充，然后利用一类特殊的卷积核进行特征提取，这一过程可以先卷积，然后错位相连得到，原理如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173806167.png" alt="image-20200308173806167"></p><p>其中可以发现卷积核并非是正方形，而是矩形，不过过程其实是一样的。而projection部分，即resnet中原图先经过1×1卷积再与特征图相连的部分，变为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173814812.png" alt="image-20200308173814812"></p><p>具体细节我就不在多讲了，其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173833858.png" alt="image-20200308173833858"></p><p>其代码链接为：<a href="https://github.com/iro-cp/FCRN-DepthPrediction，基于Tensorflow和matconvnet。">https://github.com/iro-cp/FCRN-DepthPrediction，基于Tensorflow和matconvnet。</a></p></li><li><p><strong>深度分类网络</strong></p><p>将深度估计问题变为回归问题的缺点在于，太依赖于数据集的场景，并且由于图像中深度往往是分层的，类似于等高线之类的，所以也有学者将深度估计变为一个分类问题，而类别数就是将最远实际距离划分为多份而制作的。</p><p>以此为代表的是CVPR2018中《Deep Ordinal Regression Network for Monocular Depth Estimation》所提出的DORN框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173857556.png" alt="image-20200308173857556"></p><p>该框架2018年在多个数据集上取得了第一的名次，不过现在有个别算法超越了。可以看到，原图在经过密集特征提取之后，增加了一个场景理解模块，这一模块包含5个部分。其中full-image encoder部分与传统的自编码器不同：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173908649.png" alt="image-20200308173908649"></p><p>可以看到其先是利用池化进行下采样，将其拉伸为向量之后，利用全连接层进行编解码，然后还原为原图大小。而ASPP模块是利用了膨胀卷积（dilated convolution）进行特征提取，膨胀倍数分别为6,12,18。五个部分concat得到最终的特征图。再进入有序回归模块，实质上是多分类器。其将深度范围划分为多个区间：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173916000.png" alt="image-20200308173916000"></p><p>最后输出W×H×2K的结果，K代表深度区间，2K是因为每两个相邻的通道值2n表示深度小于n的概率，2n+1表示深度大于n的概率，二者利用softmax归一化。</p><p>其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308173932875.png" alt="image-20200308173932875"></p><p>可以看到，DORN对于深度的细节把握得非常好，其速度约为2fps，代码基于caffe平台，链接为：<a href="https://github.com/hufu6371/DORN">https://github.com/hufu6371/DORN</a></p><p>无论是回归还是分类，都是以深度信息作为标签的监督算法，因此其受限于训练集场景，仅限于刷榜。</p></li></ul><h3 id="4-2结合双目视觉的单目深度估计"><a href="#4-2结合双目视觉的单目深度估计" class="headerlink" title="4.2结合双目视觉的单目深度估计"></a>4.2结合双目视觉的单目深度估计</h3><p>既然监督算法受限于场景，那么近两年则出现了很多无监督算法，其中就包含有利用双目数据进行训练的算法，下面我用几个例子进行说明。</p><p>首先以CVPR2017中《Unsupervised Monocular Depth Estimation with Left-Right Consistency》一文所提出的Monodepth算法为例。这篇论文算是这类算法的一个开端吧，效果并没有非常优异，但是引出这样一条思路。</p><p>其网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174121034.png" alt="image-20200308174121034"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174130252.png" alt="image-20200308174130252"></p><p>该算法将深度估计问题变成了左右视图立体匹配问题，都是先从原始图预测另外一个视图的视差，然后结合输出另外一个视图。整体框架依赖DispNet，而DispNet又是在FlowNet基础上进行的改变，主要改变是在多尺度衔接出增加卷积层，以保证图像尽可能平滑。</p><p>经过自编码器之后，分别利用逆卷积、预测的右视图相对左视图的视差+upsample/双线性插值、预测的左视图相对右视图的视差+upsample/双线性插值、原图。有了这些之后，损失函数部分则同时包含有：</p><ul><li><p>外观匹配损失，即预测的视图和实际视图的区别：<br>$$ C_{ap}^l = \frac{1}{N}\sum\limits_{i,j} {\alpha \frac{{1 - SSIM\left( {I_{ij}^l,\tilde I_{ij}^l} \right)}}{2}} + \left( {1 - \alpha } \right)\left\| {I_{ij}^l - \tilde I_{ij}^l} \right\| $$<br>其中SSIM指的是结构相似性。</p></li><li><p>视差平滑性约束：<br>$$ C_{ds}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {{\partial _x}d_{ij}^l} \right|} {e^{ - \left\| {{\partial _x}I_{ij}^l} \right\|}} + \left| {{\partial _y}d_{ij}^l} \right|{e^{ - \left\| {{\partial _y}I_{ij}^l} \right\|}} $$</p></li><li><p>左右视差一致性损失：<br>$$ C_{lr}^l = \frac{1}{N}\sum\limits_{i,j} {\left| {d_{ij}^l - d_{ij + d_{ij}^l}^r} \right|} $$</p></li></ul><p>由于其主要在KITTI_outdoor和Cityscapes上训练的，所以对于室外场景效果会略好，又因为其算法框架比较简单，所以深度信息中的细节比较模糊，尤其是存在遮挡或者物体相连的情况时。测试效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174828519.png" alt="image-20200308174828519"></p><p>通过其原理和论文中的测试效果来看，其对于室外场景下的深度估计效果还行，不过对于边缘部分的把握不是很好。再加上大多是街景数据，所以对于室内场景的视角具有很大的不适应性。另外，由于立体匹配对于大面积的纯色或者颜色相近的图像块效果很差，所以Monodepth不适用于纹理不清晰的场景，容易将大片颜色类似的图像块视为一个整体。</p><p>代码是基于tensorflow进行开发的：<a href="https://github.com/mrharicot/monodepth，也有pytorch0.4.1的复现版本：https://github.com/ClubAI/MonoDepth-PyTorch。">https://github.com/mrharicot/monodepth，也有pytorch0.4.1的复现版本：https://github.com/ClubAI/MonoDepth-PyTorch。</a></p><p>与此同时，在CVPR2018中，由商汤团队在《Single View Stereo Matching》一文中提出了类似的SVS算法，其相对于Monodepth，在细节和场景适应性方面有了很大的提升。</p><p>SVS网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174851298.png" alt="image-20200308174851298"></p><p>从图中不难看出，得到预测的右视图之后，两个视角的图像进行类似于DispNet的立体匹配，从而获得左视图的视差。而关键在于怎么从左视图预测右视图。可以发现，SVS在利用卷积层对左视图进行特征提取之后，分别将每一通道的特征图和原图进行元素乘法，然后再相加。这一部分实际上是借鉴了Deep3D的框架。Deep3D模型是用来将2D图像转化为3D图像对的，其框架为：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174909501.png" alt="image-20200308174909501"></p><p>假设每个通道分别代表着在左视图中每个像素点相对右视图中的视差偏移量的概率分布。综上，SVS实质上就是Deep3D+Dispnet的合体版，其效果图如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174943651.png" alt="image-20200308174943651"></p><p>同时可以看看基于KITTI数据集训练的SVS模型在其他数据集上的测试效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308174958809.png" alt="image-20200308174958809"></p><p>代码是基于caffe开发的：<a href="https://github.com/lawy623/SVS">https://github.com/lawy623/SVS</a></p><h3 id="4-3基于视频的相机位姿估计和视觉测距"><a href="#4-3基于视频的相机位姿估计和视觉测距" class="headerlink" title="4.3基于视频的相机位姿估计和视觉测距"></a><strong>4.3基于视频的相机位姿估计和视觉测距</strong></h3><p>基于视频的单目深度估计大多都是面向相机位姿估计和视觉测距的，其核心就是利用相邻视频帧所产生的运动，近似多视角图像，并对相机位姿进行估计，从而可以估计出相机的移动路线，进一步完成SLAM工作。</p><p>那么在CVPR2017的一篇《Unsupervised Learning of Depth and Ego-Motion from Video》中则是提出了SFM算法，这篇文章中对于深度估计的求解较为简单，所以效果不是很好，但是提出了基于视频帧序列的相机位姿估计算法。</p><p>其论文中使用了相邻3帧的信息，不过代码却是用的相邻5帧的信息，整体框架比较简单：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175216815.png" alt="image-20200308175216815"></p><p>可以看到，单目深度估计部分仅仅是针对一帧的，直接采用了Dispnet的网络框架，不过我发现实际上是U-net，而相机位姿估计则是将相邻帧的相对相机位姿变化看作一个含有6个元素的向量（可以理解为x,y,z方向的平移量和旋转量）进行预测。有意思的是，SFM并没有使用深度信息作为标签，而是将深度信息作为一个过程变量，将前后帧图像联系起来，从而做到无监督学习，不过相机位姿的训练还是有监督的：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175228213.png" alt="image-20200308175228213"></p><p>利用预测的相机位姿和深度信息，可估计出目标视图相对原视图的像素点位置，由于预测的像素点位置可能不是整数，为了保证其为整数，将采用双线性插值，其中K是相机参数矩阵：</p><p>$$ \left\{ \begin{array}{l} {p_s} \sim K{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over T} }_{t \to s}}{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over D} }_t}\left( {{p_t}} \right){K^{ - 1}}{p_t}\\ {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( {{p_t}} \right) = {I_s}\left( {{p_s}} \right) = \sum\nolimits_{i \in \left\{ {t,b} \right\},j \in \left\{ {l,r} \right\}} {{w^{ij}}{I_s}\left( {p_s^{ij}} \right)} \\ \sum\nolimits_{i,j} {{w^{ij}}} = 1 \end{array} \right. $$<br>可以看到这里的插值方式是对估计像素点位置处的相邻4个位置的像素进行加权平均，然后作为目标像素点位置处的像素值，新合成的视图和目标视图进行一致性约束。</p><p>不过上述这种做法<strong>受限于静态场景，且无遮挡情况</strong>，为了缓解这种问题，作者又加入了一个可解释性网络：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175304525.png" alt="image-20200308175304525"></p><p>该网络的编码部分输出的是相机位姿，解码部分输出的是多个尺度的解释性眼膜，其意义是合成视图中的每个像素点能够被成功建模的概率。而这一部分是没有标签的，所以作者通过设计损失函数将其进行了约束：</p><p>$$ \begin{array}{l} {L_{vs}} = \sum\limits_{\left\langle {{I_1},...,{I_N}} \right\rangle \in S} {\sum\limits_p {{{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} }_s}\left( p \right)\left| {{I_t}\left( p \right) - {{\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over I} }_s}\left( p \right)} \right|} } \\ L = \sum\limits_l {L_{vs}^l} + {\lambda _s}L_{smooth}^l + {\lambda _e}\sum\limits_s {{L_{reg}}\left( {\mathord{\buildrel{\lower3pt\hbox{$\scriptscriptstyle\frown$}} \over E} _s^l} \right)} \end{array} $$<br><em>l</em>指的是尺度，<em>s</em>指的是图片，其中的平滑性约束跟上一节所讲的Monodepth一样，由于解释性掩膜无标签，如果不加约束的话会自动为0，所以利用交叉熵损失函数对其进行了约束，默认为全1矩阵。其效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175351205.png" alt="image-20200308175351205"></p><p>可以看到，深度估计的效果并不是很好，不过整体的设计思路很新颖，也可以看看其对于解释性掩膜的预测效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175401181.png" alt="image-20200308175401181"></p><p>可以发现，对于发生变化的部分，即前景部分，其不可解释性很高，其实这个也能用来估计光流。</p><p>代码是基于tensorflow的：<a href="https://github.com/tinghuiz/SfMLearner，">https://github.com/tinghuiz/SfMLearner，</a></p><p>不过有pytorch的复现版本：<a href="https://github.com/ClementPinard/SfmLearner-Pytorch">https://github.com/ClementPinard/SfmLearner-Pytorch</a></p><p>果不其然，在CVPR2018中商汤又提出了GeoNet，该网络在SFM的基础上增加了光流监督信息：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175411913.png" alt="image-20200308175411913"></p><p>可以看到，前半部分的深度估计和相机位姿估计都跟SFM一样，只是在后面增加了光流的输出，先利用前半部分得到刚性结构的光流，后半部分增加一个非刚性光流预测环节，二者之和就是最终的光流。效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175421097.png" alt="image-20200308175421097"></p><p>可以看到，GeoNet的深度估计效果并没有特别突出，代码是基于Tensorflow的：<a href="https://github.com/yzcjtr/GeoNet">https://github.com/yzcjtr/GeoNet</a></p><p>同样的还有CVPR2018的《Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction》一文中提到的Depth-VO-Feat:</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175431143.png" alt="image-20200308175431143"></p><p>直接从这个网络架构可以看到包含了两个部分的图像重构，一个是左视图和右视图的重构，一个是前后两帧间的重构，重构的意义在于找到对应像素点的联系，并非直接利用左右视图进行误差计算，可以看到图中对于右视图的边缘填充。由于该框架假设场景是Lambertian的，即无论从哪个角度观察，其光照强度是一致的，那么这对于图像的重构就很敏感，因此，作者又添加了特征的重构，框架一致。</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175529893.png" alt="image-20200308175529893"></p><p>对于训练细节，除了图像和特征的L1重构误差之外，也加入了边缘平滑性约束，骨干网络是Resnet50的变种。对于深度估计，其预测的是深度信息的倒数。效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308175539828.png" alt="image-20200308175539828"></p><p>可以看到，深度估计的效果还是中规中矩，不过其可以用来做视频中相机的移动轨迹预测，这一点在多目标跟踪（MOT）中对于手持相机的场景有所帮助。代码是基于caffe的：<a href="https://github.com/Huangying-Zhan/Depth-VO-Feat">https://github.com/Huangying-Zhan/Depth-VO-Feat</a></p><p>相应的，近几年关于无监督单目深度估计的研究越来越多，我抽空了看了下，比如有Google出品的vid2depth和struct2depth算法，二者的代码链接如下：</p><p>vid2depth:<a href="https://github.com/tensorflow/models/tree/master/research/vid2depth">https://github.com/tensorflow/models/tree/master/research/vid2depth</a></p><p>struct2depth: <a href="https://github.com/tensorflow/models/tree/master/research/struct2depth。">https://github.com/tensorflow/models/tree/master/research/struct2depth。</a></p><p>其他的也挺多的，后面章节我会再补充一点，不过肯定不全。</p><h3 id="4-4基于图像风格迁移的单目深度估计"><a href="#4-4基于图像风格迁移的单目深度估计" class="headerlink" title="4.4基于图像风格迁移的单目深度估计"></a>4.4基于图像风格迁移的单目深度估计</h3><p>实质上，深度图像也是一种图像风格，如果我们要将生成学习引入深度估计的话，就需要注意两个地方，一个是原始图像到深度图像的风格转变，这一点可以获取类似于分割的map，另一点就是对像素点的深度进行回归。这里的方式与第一节讲的深度回归模型不一样，因为第一步的风格转变，已经对于场景和相机位姿有了很好的适应性。</p><p>ECCV2018中《T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks》所提出的T2Net尝试性地将图像风格迁移引入单目深度估计领域，虽然效果只是2016年的水平，不过也算是一次很好的尝试了。下面介绍下T2Net的思路，首先给出其网络框架：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180333423.png" alt="image-20200308180333423"></p><p>框架很明显，对于室外场景，其训练集用到了KITTI和VKITTI中的sunset场景，对于室内场景，则使用了NYU Depth v2和SUNCG,没仔细看怎么下载，相关工具在<a href="https://github.com/shurans/SUNCGtoolbox。">https://github.com/shurans/SUNCGtoolbox。</a></p><p>从图中可以看到，作者做了两个模块，一个是图像风格迁移模块，一个是单目深度估计模块。其中图像风格迁移模块中包含有合成图像到真实图像的迁移，真实图像到真实图像的迁移，二者共用一个GAN。其中的Loss包含有：</p><ul><li>由合成图像风格迁移生成的图像与原始图像的GAN Loss，即利用判别器进行判定的误差；</li><li>由真实图像风格迁移生成的图像预原始图像的重构误差，这一部分计算L1 Loss；</li><li>由合成图像风格迁移生成的图像与原始图像的编码特征的GAN Loss。</li></ul><p>然后<strong>仅对合成图像分支进行深度估计</strong>，同样地，也加入了深度图的平滑性约束。从不匹配的图像对可以看出，其基础框架为CycleGAN。</p><p>可以看到风格迁移的效果和深度估计的效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180412887.png" alt="image-20200308180412887"></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180416682.png" alt="image-20200308180416682"></p><p>从结果中我们发现有一个版本的实现效果超过了完整框架，通过查阅发现，是只利用真实数据进行深度估计的效果，也就是说效果比加入图像迁移的效果更好，打自己脸。。。实际上他是在跟只用合成图像进行深度估计训练的效果作比较，确实好了些。</p><p>代码链接：<a href="https://github.com/lyndonzheng/Synthetic2Realistic">https://github.com/lyndonzheng/Synthetic2Realistic</a></p><p>除此之外，在CVPR2018也有一篇类似的算法《Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer》，其效果则是达到了state-of-art,我们暂且称其为MDEDA,网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180432526.png" alt="image-20200308180432526"></p><p>熟悉CycleGAN框架的话，应该很容易看懂其中的关系，其中存在三种图像序列，一种是原始图像，一种是合成的图像，一种是深度图像，不同的是三种图像内容是一致的，而非CycleGAN那样不匹配的。其中原始图像和合成图像之间进行图像风格的循环迁移和重构，合成图像与深度图像进行单向的风格迁移。</p><p>效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180445762.png" alt="image-20200308180445762"></p><p>左侧的是直接对原图进行深度估计的效果，中间是其他图像迁移算法的效果，右侧是采用本文算法后的合成以及深度估计效果，速度大概为44fps。合成图像对于深度估计的效果提升也反映了一个问题，即图像光暗条件对于深度估计有很大影响，所以对于一些出现了阴影，如影子等的场景，深度估计会出现偏差，如：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180455841.png" alt="image-20200308180455841"></p><p>代码只提供了测试模型：<a href="https://github.com/atapour/monocularDepth-Inference">https://github.com/atapour/monocularDepth-Inference</a></p><h3 id="4-5多任务深度估计"><a href="#4-5多任务深度估计" class="headerlink" title="4.5多任务深度估计"></a>4.5多任务深度估计</h3><p>在ICRA2019中《Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations》中基于图像分割算法RefineNet设计了一个多任务框架。其中RefineNets是CVPR2017中提出的算法，其全局框架是基于Resnet的U-net网络框架，可以输出多尺度的分割图：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180520856.png" alt="image-20200308180520856"></p><p>可以看到的是，RefineNet在每一个尺度的上采样部分都增加了一个局部提升的网络，用于多尺度输出的融合：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180529831.png" alt="image-20200308180529831"></p><p>所以其主要创新在于采用skip-connection和 Resnet Block的方式不断融合各种分辨率的特征，用于增加更多的细粒度特征，从而方便生成更高分辨率的预测：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180539369.png" alt="image-20200308180539369"></p><p>那么在BMVC2018中则是提出了一种Light-weighted RefineNet算法，顾名思义，就是RefineNet的轻量级网络，其对于512×512大小的图像，速度从RefineNet的20FPS提升到了55FPS（1080Ti），效果略微下降。代码基于Pytorch: <a href="https://github.com/DrSleep/light-weight-refinenet">https://github.com/DrSleep/light-weight-refinenet</a></p><p>那么回到正题，我们提到的这个同时进行深度估计和目标分割的网络框架，对于1200×350大小的输入，其速度为60FPS。网络框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180614033.png" alt="image-20200308180614033"></p><p>以上的结构通过之前我介绍的深度估计框架以及Light-Weighted RefineNet框架很容易能看懂，之所以比原本的Light-Weighted RefineNet还要快，是因为将其中的部分1×1卷积替换成了MobileNetV2中采用的depthwise卷积方式。</p><p>对于分割和深度估计任务的结合，从网络框架和损失函数的设计来看可以发现，其除了特征是共享的之外，预测任务是独立的。效果如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180623440.png" alt="image-20200308180623440"></p><p>代码仅提供了测试用例：<a href="https://github.com/drsleep/multi-task-refinenet">https://github.com/drsleep/multi-task-refinenet</a></p><p>ECCV2018中《DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency》一文提出了单目深度估计和光流预测的联合任务框架。不同于单独训练两个任务的方式，作者将二者的一致性进行了考虑，从而做到二者的相互促进，可以看到对比效果：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180707605.png" alt="image-20200308180707605"></p><p>其主要思路是利用无监督的方式从视频中预测深度信息和相机位姿变化，这一部分对于刚性流场景比较适用，即静态背景。通过几何一致性的约束监督，可以将3D的场景流映射到2D光流上，由此与光流预测模型的结果进行一致性约束。具体框架如下：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180715231.png" alt="image-20200308180715231"></p><p>乍一看可以发现网络框架的前半部分很眼熟，图中展示的是分别对前后帧做单目深度估计，然后利用前后帧做相机位姿变化预测和光流预测，结合SFM网络中像素点转移的计算公式，可以利用深度信息和相机位姿变化关系求得在t+1时刻对应像素点位置，由此可以计算刚性流场景下的光流。</p><p>​ 对于刚性流场景下的合成光流信息和直接预测到的光流信息，二者都反映了相邻两帧的像素点的对应关系，因此作者对此引入了光照约束（利用对比映射和插值，计算每个像素点的像素值差异）和深度的平滑性约束。</p><p>​ 再来看Forward-Backward模块，由于我们在上面提到了光照一致性约束，但实际上对于重叠区域并不适用，因此加入了前后向一致性的约束。即图中的Valid Mask部分，利用刚性流信息可以检测出一些无效的像素区域，如运动物体、画面边缘等，因为这些都不符合刚性这一条件，那么再在有效区域使用光照一致性假设：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180726951.png" alt="image-20200308180726951"></p><p>感觉这个跟SFM中的Explain Mask一样，然后前后的一致性约束，则是分光流和深度估计两部分，其中深度的一致性跟光照一致性的计算方式一样，而光流的一致性则是真的计算了前向和反向的光流一致性。最后对于深度和光流的共同有效区域，保证二者预测的光流尽可能一致。为了保证更好的训练效果，作者先在SYNTHIA数据集上预训练光流预测，采用的是UnFlownet-C网络，在KITTI和Cityscapes上预训练深度估计和相机位姿预测，采用的是SFM框架，然后进行联合训练。代码基于Tensorflow: <a href="https://github.com/vt-vl-lab/DF-Net">https://github.com/vt-vl-lab/DF-Net</a></p><p>我前段时间还发现一个多任务的集成框架CVPR2019的CCN算法《Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation》，效果目前好像还是SOTA，其融合了单目深度估计、相机位姿估计、光流估计和运动分割多个任务，代码：<a href="https://github.com/anuragranj/cc">https://github.com/anuragranj/cc</a></p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308180928316.png" alt="image-20200308180928316"></p><p>本小节的内容都是基于无监督的单目深度估计算法。</p><h2 id="5总结"><a href="#5总结" class="headerlink" title="5总结"></a><strong>5总结</strong></h2><p>对于单目深度估计模型，目前主要分为基于回归/分类的监督模型，基于双目训练/视频序列的无监督模型，以及基于生成学习的图像风格迁移模型。大概从2017年起，即CVPR2018开始，单目深度估计的效果就已经达到了双目深度估计的效果，主要是监督模型。但是由于现有的数据集主要为KITTI、Cityscapes、NYU DepthV2等，其场景和相机都是固定的，从而导致监督学习下的模型无法适用于其他场景，尤其是多目标跟踪这类细节丰富的场景，可以从论文中看到，基本上每个数据集都会有一个单独的预训练模型。</p><p>对于GAN，其对于图像风格的迁移本身是一个很好的泛化点，既可以用于将场景变为晴天、雾天等情况，也可以用于图像分割场景。但是深度估计问题中，像素点存在相对大小，因此必定涉及到回归，因此其必定是监督学习模型，所以泛化性能也不好，以CVPR2018的那篇GAN模型为例可以对比：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181145568.png" alt="image-20200308181145568"></p><p>左边是KITTI的测试效果，右边是MOT的测试效果，从上到下依次是原图、合成图，以及深度图。可以看到，其泛化性能特别差。而对于无监督模型，从理论上来讲，其泛化性能更好。那么对于无监督模型，我们分两部分进行讨论，第一部分是利用双目视差进行训练的无监督模型，这里的无监督模型中包含有左右视图预测的监督信息，所以存在一定程度的局限性。以Monodepth为例：</p><p><img src="https://blogdata-1258545379.cos.ap-shanghai.myqcloud.com/20200308/image-20200308181208321.png" alt="image-20200308181208321"></p><p>对于无监督的算法，可能场景适应性会更好，但依旧不适用于对行人深度的估计。</p><h2 id="6参考文献"><a href="#6参考文献" class="headerlink" title="6参考文献"></a>6参考文献</h2><p>[1] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.</p><p>[2] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.</p><p>[3] Laina I, Rupprecht C, Belagiannis V, et al. Deeper depth prediction with fully convolutional residual networks[C]//2016 Fourth international conference on 3D vision (3DV). IEEE, 2016: 239-248.</p><p>[4] Fu H, Gong M, Wang C, et al. Deep ordinal regression network for monocular depth estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2002-2011.</p><p>[5] Godard C, Mac Aodha O, Brostow G J. Unsupervised monocular depth estimation with left-right consistency[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 270-279.</p><p>[6] Dosovitskiy A, Fischer P, Ilg E, et al. Flownet: Learning optical flow with convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2758-2766.</p><p>[7] Ilg E, Mayer N, Saikia T, et al. Flownet 2.0: Evolution of optical flow estimation with deep networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2462-2470.</p><p>[8] Mayer N, Ilg E, Hausser P, et al. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4040-4048.</p><p>[9] Xie J, Girshick R, Farhadi A. Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 842-857.</p><p>[10] Luo Y, Ren J, Lin M, et al. Single View Stereo Matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p><p>[11] Zhou T, Brown M, Snavely N, et al. Unsupervised learning of depth and ego-motion from video[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 1851-1858.</p><p>[12] Yin Z, Shi J. Geonet: Unsupervised learning of dense depth, optical flow and camera pose[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1983-1992.</p><p>[13] Zhan H, Garg R, Saroj Weerasekera C, et al. Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 340-349.</p><p>[14] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680.</p><p>[15] Radford A , Metz L , Chintala S . Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks[J]. Computer Science, 2015.</p><p>[16] Arjovsky M, Chintala S, Bottou L. Wasserstein gan[J]. arXiv preprint arXiv:1701.07875, 2017.</p><p>[17] Gulrajani I, Ahmed F, Arjovsky M, et al. Improved training of wasserstein gans[C]//Advances in Neural Information Processing Systems. 2017: 5767-5777.</p><p>[18] Mao X, Li Q, Xie H, et al. Least squares generative adversarial networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2794-2802.</p><p>[19] Mirza M, Osindero S. Conditional generative adversarial nets[J]. arXiv preprint arXiv:1411.1784, 2014.</p><p>[20] Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.</p><p>[21] Wang T C, Liu M Y, Zhu J Y, et al. High-resolution image synthesis and semantic manipulation with conditional gans[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8798-8807.</p><p>[22] Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232.</p><p>[23] Wang T C , Liu M Y , Zhu J Y , et al. Video-to-Video Synthesis[J]. arXiv preprint arXiv:1808.06601,2018.</p><p>[24] Zheng C, Cham T J, Cai J. T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 767-783.</p><p>[25] Atapour-Abarghouei A, Breckon T P. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2800-2810.</p><p>[26] Nekrasov V , Dharmasiri T , Spek A , et al. Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations[J]. arXiv preprint arXiv:1809.04766,2018.</p><p>[27] Nekrasov V , Shen C , Reid I . Light-Weight RefineNet for Real-Time Semantic Segmentation[J]. arXiv preprint arXiv:1810.03272, 2018.</p><p>[28] Lin G , Milan A , Shen C , et al. RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.,2017:1925-1934</p><p>[29] Zou Y , Luo Z , Huang J B . DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018:36-53.</p><p>[30] Ranjan A, Jampani V, Balles L, et al. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 12240-12249.</p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.jpg" alt="黄飘 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="黄飘 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>黄飘</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/" title="基于深度学习的单目深度估计综述">https://huangpiao.tech/2020/03/08/基于深度学习的单目深度估计综述/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度估计/" rel="tag"><i class="fa fa-tag"></i> 深度估计</a></div><div class="post-widgets"><div class="social_share"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/03/06/多目标跟踪中的数据关联代码实践(下)/" rel="next" title="多目标跟踪中的数据关联代码实践(下)"><i class="fa fa-chevron-left"></i> 多目标跟踪中的数据关联代码实践(下)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/03/16/谈谈CNN中的位置和尺度问题/" rel="prev" title="谈谈CNN中的位置和尺度问题">谈谈CNN中的位置和尺度问题 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/author.jpg" alt="黄飘"><p class="site-author-name" itemprop="name">黄飘</p><p class="site-description motion-element" itemprop="description">直到这一刻微笑着说话为止，我至少留下了一公升眼泪</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">33</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">41</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/nightmaredimple" title="GitHub &rarr; https://github.com/nightmaredimple" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/nightmare_dimple" title="CSDN &rarr; https://blog.csdn.net/nightmare_dimple" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="/huangpiao2985@163.com" title="E-Mail &rarr; huangpiao2985@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/huang-piao-72/posts" title="ZhiHu &rarr; https://www.zhihu.com/people/huang-piao-72/posts" rel="noopener" target="_blank"><i class="fa fa-fw fa-zhihu"></i>ZhiHu</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1任务介绍"><span class="nav-text">1任务介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数据集介绍"><span class="nav-text">2 数据集介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-KITTI"><span class="nav-text">2.1 KITTI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2vKITTI"><span class="nav-text">2.2vKITTI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3Cityscapes"><span class="nav-text">2.3Cityscapes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4NYU-Depth-V2"><span class="nav-text">2.4NYU Depth V2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-ScanNet"><span class="nav-text">2.5 ScanNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6Make3D"><span class="nav-text">2.6Make3D</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-数据处理"><span class="nav-text">3 数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1数据组成"><span class="nav-text">3.1数据组成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-数据处理"><span class="nav-text">3.2 数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3评价指标"><span class="nav-text">3.3评价指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-相关工作"><span class="nav-text">4 相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1基于单目视觉的深度估计"><span class="nav-text">4.1基于单目视觉的深度估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2结合双目视觉的单目深度估计"><span class="nav-text">4.2结合双目视觉的单目深度估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3基于视频的相机位姿估计和视觉测距"><span class="nav-text">4.3基于视频的相机位姿估计和视觉测距</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4基于图像风格迁移的单目深度估计"><span class="nav-text">4.4基于图像风格迁移的单目深度估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5多任务深度估计"><span class="nav-text">4.5多任务深度估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5总结"><span class="nav-text">5总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6参考文献"><span class="nav-text">6参考文献</span></a></li></ol></div></div></div><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">黄飘</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="post.totalcount">99.1k字</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量">总访客量: <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量">总访问数: <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script src="/lib/reading_progress/reading_progress.js"></script><script src="/js/src/utils.js?v=6.7.0"></script><script src="/js/src/motion.js?v=6.7.0"></script><script src="/js/src/affix.js?v=6.7.0"></script><script src="/js/src/schemes/pisces.js?v=6.7.0"></script><script src="/js/src/scrollspy.js?v=6.7.0"></script><script src="/js/src/post-details.js?v=6.7.0"></script><script src="/js/src/bootstrap.js?v=6.7.0"></script><script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>var GUEST=["nick","mail","link"],guest="nick";guest=guest.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#comments",verify:!1,notify:!1,appId:"YGYUrzPLBQI94NtQWvTjzrE5-gzGzoHsz",appKey:"aQseSeSwrta45i3gUrcNK2QQ",placeholder:"ヾﾉ≧∀≦)o 来呀！吐槽一番吧！",avatar:"mm",meta:guest,pageSize:"10",visitor:!1})</script><script>function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var e=$("#local-search-input");e.attr("autocapitalize","none"),e.attr("autocorrect","off"),e.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(e){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(e,t,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:e,dataType:isXml?"xml":"json",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():e,r=document.getElementById(t),s=document.getElementById(o),a=function(){var e=r.value.trim().toLowerCase(),t=e.split(/[\s\-]+/);t.length>1&&t.push(e);var o=[];if(e.length>0&&n.forEach(function(n){function r(t,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===e&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(e,t){var o="",n=t.start;return t.hits.forEach(function(t){o+=e.substring(n,t.position);var r=t.position+t.length;o+='<b class="search-keyword">'+e.substring(t.position,r)+"</b>",n=r}),o+=e.substring(n,t.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url).replace(/\/{2,}/g,"/"),d=[],g=[];if(""!=l&&(t.forEach(function(e){function t(e,t,o){var n=e.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(t=t.toLowerCase(),e=e.toLowerCase());(s=t.indexOf(e,r))>-1;)a.push({position:s,word:e}),r=s+n;return a}d=d.concat(t(e,h,!1)),g=g.concat(t(e,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(e){e.sort(function(e,t){return t.position!==e.position?t.position-e.position:e.word.length-t.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;w<0&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hits.length!==t.hits.length?t.hits.length-e.hits.length:e.start-t.start});var T=parseInt("3");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(e){b+="<a href='"+f+'\'><p class="search-result">'+s(p,e)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===t.length&&""===t[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x"></i></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>';else{o.sort(function(e,t){return e.searchTextCount!==t.searchTextCount?t.searchTextCount-e.searchTextCount:e.hitCount!==t.hitCount?t.hitCount-e.hitCount:t.id-e.id});var a='<ul class="search-result-list">';o.forEach(function(e){a+=e.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(e){e.stopPropagation()}),$(document).on("keyup",function(e){var t=27===e.which&&$(".search-popup").is(":visible");t&&onPopupClose()})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0';
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
        extensions: ['[mhchem]/mhchem.js'],
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });</script><script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.MathJax_Display{overflow:auto hidden}</style><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="box",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="box",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><style>.copy-btn{display:inline-block;padding:6px 12px;font-size:13px;font-weight:700;line-height:20px;color:#333;white-space:nowrap;vertical-align:middle;cursor:pointer;background-color:#eee;background-image:linear-gradient(#fcfcfc,#eee);border:1px solid #d5d5d5;border-radius:3px;user-select:none;outline:0}.highlight-wrap .copy-btn{transition:opacity .3s ease-in-out;opacity:0;padding:2px 6px;position:absolute;right:4px;top:8px}.highlight-wrap .copy-btn:focus,.highlight-wrap:hover .copy-btn{opacity:1}.highlight-wrap{position:relative}</style><script>$(".highlight").each(function(e,t){var n=$("<div>").addClass("highlight-wrap");$(t).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(e){var t=$(this).parent().find(".code").find(".line").map(function(e,t){return $(t).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=document.createRange(),a=window.getSelection(),i=window.pageYOffset||document.documentElement.scrollTop;n.style.top=i+"px",n.style.position="absolute",n.style.opacity="0",n.value=t,n.textContent=t,n.contentEditable=!0,n.readOnly=!1,document.body.appendChild(n),o.selectNode(n),a.removeAllRanges(),a.addRange(o),n.setSelectionRange(0,t.length);var d=document.execCommand("copy");d?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(e){var t=$(this).find(".copy-btn");setTimeout(function(){t.text("复制")},300)}).append(t)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html><script type="text/javascript" src="/js/src/clicklove.js"></script><!-- rebuild by neat -->